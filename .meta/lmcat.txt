# Stats
- 37 files
- 8559 (8.6K) lines
- 241003 (241K) chars
- 26798 (27K) `whitespace-split` tokens

# File Tree

```
LatticeVision                           
├── .github                             
│   └── workflows                       
│    └── checks.yml                     [   41L       790C    92T]
├── R_scripts                           
│   ├── cesm_ensemble_sim.R             [  363L     9,446C 1,169T]
│   ├── cnn_datagen.R                   [  252L     7,211C   798T]
│   ├── helper_funcs.R                  [  606L    19,226C 2,450T]
│   ├── i2i_datagen.R                   [  636L    21,360C 2,449T]
│   └── required_packages.R             [   13L       317C    21T]
├── latticevision                       
│   ├── cnn                             
│   │   ├── __init__.py                 [    0L         0C     0T]
│   │   ├── dataset.py                  [  145L     4,185C   490T]
│   │   ├── eval.py                     [  310L     9,335C 1,108T]
│   │   ├── model.py                    [  105L     2,780C   328T]
│   │   └── train.py                    [  301L     9,616C 1,061T]
│   ├── img2img                         
│   │   ├── models                      
│   │   │   ├── __init__.py             [    0L         0C     0T]
│   │   │   ├── stun.py                 [  131L     3,309C   376T]
│   │   │   ├── unet.py                 [   98L     2,265C   209T]
│   │   │   └── vit.py                  [   71L     1,834C   211T]
│   │   ├── __init__.py                 [   16L       281C    29T]
│   │   ├── base.py                     [  503L    15,616C 1,699T]
│   │   ├── dataset.py                  [  183L     5,407C   670T]
│   │   ├── eval.py                     [  290L    10,080C 1,085T]
│   │   └── train.py                    [  350L    11,754C 1,300T]
│   ├── stun                            
│   ├── __init__.py                     [    0L         0C     0T]
│   ├── device.py                       [   54L     1,367C   187T]
│   ├── img_augment.py                  [   99L     2,337C   307T]
│   ├── plotting.py                     [  329L    10,082C 1,141T]
│   └── seed.py                         [   27L       632C    59T]
├── notebooks                           
│   ├── cesm_application.ipynb          [  926L 1,928,325C 2,792T]
│   ├── cnn_demo.ipynb                  [  680L 1,043,535C 1,726T]
│   ├── cnn_trainloop.py                [  339L     8,552C   775T]
│   ├── i2i_demo.ipynb                  [  951L 1,525,218C 2,972T]
│   └── i2i_trainloop.py                [  292L     7,450C   718T]
├── tests                               
│   ├── unit                            
│   │   └── test_integration_pieces.py  [  208L     4,862C   471T]
│   ├── test_cnn_integration.py         [  294L     7,030C   667T]
│   └── test_i2i_integration.py         [  264L     6,030C   546T]
├── README.md                           [  114L     5,956C   765T]
├── makefile                            [1,633L    50,043C 5,988T]
├── pyproject.toml                      [  113L     2,714C   309T]
├── requirements.txt                    [  677L    13,805C 1,492T]
```

# File Contents

``````{ path=".github/workflows/checks.yml"  }
name: Checks

on:
  pull_request:
    branches:
      - main
      - "*"
  push:
    branches:
      - main

jobs:
  
  test:
    name: Test 
    runs-on: ubuntu-latest
    # needs: [lint, check-deps] # for conditionally running this job
    strategy:
      matrix:
        python: ["3.12"] # add 3.11 and 3.13 later 
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with: 
          fetch-depth: 1

      - name: Set up python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}

      - name: set up uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: install
        run: uv sync
      
      - name: tests
        run: uv run python -m pytest tests/
        # TODO: add ruff 
``````{ end_of_file=".github/workflows/checks.yml" }

``````{ path="R_scripts/cesm_ensemble_sim.R"  }
##########             IMPORTANT              ###########
#-------------------------------------------------------#
# Navigate to "Session" in the top left of RStudio and
# click on "Set Working Directory" and then 
# choose "To Source File Location". 
# Now you can run the script. 
#-------------------------------------------------------#

library(LatticeKrig)
library(spam64)
library(tictoc)
library(rhdf5)
library(maps)
library(cmocean)
library(here)
# helper functions
source(here("R_scripts", "helper_funcs.R"))


###########################################
######   Data and Initial Setup    #####
###########################################
rows <- 288
columns <- 192


# load in i2i network clim outputs
file_path_s <- here(
  "results",
  "clim_outputs",
  "modelTransUNet_reps30_posRotaryPosEmbed_clim_output.h5"
)
h5ls(file_path_s)
df_STUN <- h5read(file_path_s, "clim_output")

# extract/make params
kappa2_s <- exp(df_STUN[,,1])
awght_s <- kappa2_s + 4

# need to shift for LK 
theta_s <- pi/2 - df_STUN[,,2]

# for plotting intuitive angles
# theta_s <- -df_STUN[,,2]

rho_s <- df_STUN[,,3]
rhox_s <- sqrt(rho_s)
rhoy_s <- 1/rhox_s

# load in CNN outputs
file_path_c <- here(
  "results",
  "clim_outputs",
  "modelCNN_size25_reps30_clim_output.h5"
)
h5ls(file_path_c)
df_CNN <- h5read(file_path_c, "clim_output")

# extract/make params
kappa2_c <- exp(df_CNN[,,1])
awght_c <- kappa2_c + 4

# need to shift for LK 
theta_c <- pi/2 - df_CNN[,,2]

rho_c <- df_CNN[,,3]
rhox_c <- sqrt(rho_c)
rhoy_c <- 1/rhox_c

# Load in the Climate/temp data 
load(
  here(
  "data",
  "JJAPatternScalingSlope.rda"
  )
)


# normalize all clim fields 
JJASlopeNorm <- JJASlope
for (field in 1:dim(JJASlope)[3]){
  JJASlopeNorm[,,field] <- (JJASlopeNorm[,,field] - JJASlopeMean)/JJASlopeSd
}

JJASlopeNorm <- pacific_centering(JJASlopeNorm)
JJASlope <- pacific_centering(JJASlope)
JJASlopeMean <- pacific_centering(JJASlopeMean)
JJASlopeSd <- pacific_centering(JJASlopeSd)


# Coordinates and data grids 
xcoord = c(1:dim(JJASlope)[1])
ycoord = c(1:dim(JJASlope)[2])
# lon <- seq(-180, 180, length.out = dim(JJASlope)[1])
lon <- seq(0,360, length.out = dim(JJASlope)[1])
lat <- seq(-90, 90, length.out = dim(JJASlope)[2])


gridList<- list( x= seq( 1,rows,length.out= rows),
                 y= seq( 1,columns,length.out= columns) )
sGrid<- make.surface.grid(gridList)


###########################################
######   Initial Visualizations   #####
###########################################


first_field <- JJASlope[,,1] 
first_field_norm <- JJASlopeNorm[,,1]

imagePlot(x = lon, y = lat,
          first_field_norm, main = "first field", 
          col = turbo(256))
map("world2", add = TRUE, 
    col = "grey0", lwd = 1)

# plot stun params with field
plot_params(kappa2_s, theta_s, rho_s, border_lwd = 1)

# plot cnn params with field
plot_params(kappa2_c, theta_c, rho_c, border_lwd = 1)


###########################################
######   Generating Ensembles   #####
###########################################


# get synthetic replicates and associated lkinfos
object_s <- generate_synthetic_reps(
  kappa2 = kappa2_s,
  theta = theta_s,
  rho = rho_s,
  rhox = rhox_s,
  rhoy = rhoy_s,
  n_replicates = 1000,
  random_seed = 6777,
  smooth_choice = FALSE,
  normalize = TRUE
)

object_c <- generate_synthetic_reps(
  kappa2 = kappa2_c,
  theta = theta_c,
  rho = rho_c,
  rhox = rhox_c,
  rhoy = rhoy_c,
  n_replicates = 1000,
  random_seed = 6777,
  smooth_choice = FALSE,
  normalize = TRUE
)

# extract LKinfo objects
LKinfo_s <- object_s$LKinfo
LKinfo_c <- object_c$LKinfo


###########################################
#### Covariance Experiments and Tests   ###
###########################################

set.seed(777)
# number of random points, 50
n_points <- 50

# pre-allocate a data.frame to store locations + RMSEs
results <- data.frame(
  pointx    = integer(n_points),
  pointy    = integer(n_points),
  rmse_stun = numeric(n_points),
  rmse_cnn  = numeric(n_points)
)

# start timer
tic("Total computation")

for (i in seq_len(n_points)) {
  # sample a random location
  px <- sample(seq_len(rows), 1)
  py <- sample(seq_len(columns), 1)
  results$pointx[i] <- px
  results$pointy[i] <- py
  
  # base correlation field
  target_ts <- as.vector(JJASlopeNorm[px, py, ])
  cors      <- apply(JJASlopeNorm, c(1,2), function(x) cor(x, target_ts))
  
  # STUN correlation field
  f_stun          <- array(object_s$f, dim = c(rows, columns, 1000))
  target_ts_stun  <- as.vector(f_stun[px, py, ])
  cors_stun       <- apply(f_stun, c(1,2), function(x) cor(x, target_ts_stun))
  
  # CNN correlation field
  f_cnn           <- array(object_c$f, dim = c(rows, columns, 1000))
  target_ts_cnn   <- as.vector(f_cnn[px, py, ])
  cors_cnn        <- apply(f_cnn, c(1,2), function(x) cor(x, target_ts_cnn))
  
  # compute and store RMSEs
  results$rmse_stun[i] <- sqrt(mean((cors       - cors_stun) ^ 2))
  results$rmse_cnn[i]  <- sqrt(mean((cors       - cors_cnn)  ^ 2))
}

# stop timer and compute elapsed time
tt  <- toc(log = TRUE)
elapsed_time <- tt$toc - tt$tic

# sum‐total of all RMSE values
sum_total_rmse_stun <- sum(results$rmse_stun)
sum_total_rmse_cnn <- sum(results$rmse_cnn)

# print summaries
cat("Sum total RMSE stun empirical: ", sum_total_rmse_stun, "\n")
cat("Sum total RMSE cnn empirical: ", sum_total_rmse_cnn, "\n")
cat("Total time (s): ", elapsed_time, "\n")


# note paired (shouldnt use)
t.test(results$rmse_stun, results$rmse_cnn, 
       paired = FALSE, 
       alternative = "less", 
       conf.level = 0.99)

# paired (should use)
obj <- t.test(results$rmse_stun, results$rmse_cnn, 
              paired = TRUE, 
              alternative = "less", 
              conf.level = 0.99)
print(obj)


###########################################
###   Covariance Visualization (ENSO)   ###
###########################################


tic()
# either randomly sample a point 
pointx <- sample(1:rows,1)
pointy <- sample(1:columns,1)
# or use the Nino3.4 one (one of these pairs must be commented out)
pointx <- 170 # enso choice 
pointy <- 97
print(c(pointx,pointy))

# calculate correlations
target_ts <- as.vector(JJASlopeNorm[pointx, pointy, ])  
cors <- apply(JJASlopeNorm, c(1, 2), function(x) cor(x, target_ts))

f_stun <- array(object_s$f, dim = c(rows, columns, 1000))
target_ts_stun <- as.vector(f_stun[pointx, pointy, ])
cors_stun <- apply(f_stun, c(1, 2), function(x) cor(x, target_ts_stun))

f_cnn <- array(object_c$f, dim = c(rows, columns, 1000))
target_ts_cnn <- as.vector(f_cnn[pointx, pointy, ])
cors_cnn <- apply(f_cnn, c(1, 2), function(x) cor(x, target_ts_cnn))

# take the rmse
sqrt(mean((cors - cors_stun)^2 ))
sqrt(mean((cors - cors_cnn)^2 ))
toc()


#trim poles for visuals
vertchop <- 11:182
latv     <- lat[vertchop]


# compute zlims for covs
zmax <- max(cors, cors_stun, cors_cnn)
zmin <- min(cors, cors_stun, cors_cnn)
zlim = c(-zmax, zmax)

colorchoice = cmocean("balance")(22)
bordercol = "grey10"


# compute common zlims for actual fields
zmaxclim  <- max(JJASlopeNorm[,vertchop,1], f_stun[,vertchop,4], f_cnn[,vertchop,4])
zminclim  <- min(JJASlopeNorm[,vertchop,1], f_stun[,vertchop,4], f_cnn[,vertchop,4])
zlimclim  <- c(zminclim, zmaxclim)



### ACTUAL FIELDS
# png("clim_compare_stun_cnn25.png", width=4000, height=1000, res=600)
par(mfrow=c(1,3), mar=c(1,1,1,1), oma=c(1,1,0,0))

# 1) true field
image(
  x    = lon, y = latv, z = JJASlopeNorm[,vertchop,1],
  col  = turbo(256), zlim = zlimclim,
  xaxt = "n", xlab = "", ylab = ""
)
map("world2", add=TRUE, col=bordercol, lwd=0.5)

# 2) STUN
image(
  x    = lon, y = latv, z = f_stun[,vertchop,4],
  col  = turbo(256), zlim = zlimclim,
  xaxt = "n", yaxt = "n", xlab = "", ylab = ""
)
map("world2", add=TRUE, col=bordercol, lwd=0.5)

# 3) CNN
image(
  x    = lon, y = latv, z = f_cnn[,vertchop,4],
  col  = turbo(256), zlim = zlimclim,
  xaxt = "n", yaxt = "n", xlab = "", ylab = ""
)
map("world2", add=TRUE, col=bordercol, lwd=0.5)
par(mfrow = c(1,1))
# dev.off()


# CORRELATIONS
# png("cov_compare_stun_cnn25.png", width=4000, height=1000, res=600)
par(mfrow=c(1,3), mar=c(1,1,1,1), oma=c(1,1,0,0))


# 1) actual cov
image(x = lon, y = latv,z = cors[,vertchop], col = colorchoice, main = "",
      zlim = zlim, 
      xlab = "", ylab = "")
map("world2", add = TRUE, col = bordercol, lwd = 0.5)
# 2) stun cov
image(x = lon, y = latv,z =cors_stun[,vertchop], col = colorchoice, 
      main = "",
      zlim = zlim, yaxt = "n", 
      xlab = "", ylab = "")
map("world2", add = TRUE, col = bordercol, lwd = 0.5)
# 3) cnn cov
image(x = lon, y = latv,z =cors_cnn[,vertchop], col = colorchoice, 
      main = "",
      zlim = zlim, yaxt = "n", 
      xlab = "", ylab = "")
map("world2", add = TRUE, col = bordercol, lwd = 0.5)
par(mfrow = c(1,1))
# dev.off()


# COLORBAR FOR ACTUAL FIELDS
# png("clim_colorbar_v3_div.png", width = 2333, height = 2000, res = 600)
par( mar=c(1,1,1,1))

# will trim this plot
image.plot(x = lon, y = latv,z =cors_cnn[,vertchop], col = colorchoice, 
           main = "",
           zlim = zlim, yaxt = "n", 
           xlab = "", ylab = "")
# dev.off()

# COLORBAR FOR ACTUAL COVS
# png("cov_colorbar_v3_div.png", width = 2333, height = 2000, res = 600)
par( mar=c(1,1,1,1))

# will trim this plot
image.plot(x = lon, y = latv,z =JJASlopeNorm[,vertchop,1], col = turbo(256), 
           main = "",
           zlim = zlimclim, yaxt = "n", 
           xlab = "", ylab = "")
# dev.off()

``````{ end_of_file="R_scripts/cesm_ensemble_sim.R" }

``````{ path="R_scripts/cnn_datagen.R"  }
##########             IMPORTANT              ###########
#-------------------------------------------------------#
# Navigate to "Session" in the top left of RStudio and
# click on "Set Working Directory" and then 
# choose "To Source File Location". 
# Now you can run the script. 
#-------------------------------------------------------#

library(LatticeKrig)
library(spam64)
library(tictoc)
library(rhdf5)
library(here)
# helper functions
source(here("R_scripts", "helper_funcs.R"))


# Some choices to make: 
# total number of simulated fields 
N_SIMS <- 80000
# sidelength of the field (window size)
sidelen <- 25 
# Other options can be tweaked in the
# "Experimental Setup and Hyperparameters" section below. 


###########################################
####### Full Data Generator #######
###########################################

generate_cnn_data <- function(
    N_SIMS, 
    n_replicates = 30,
    n_buffer = 5,
    random_seed = 777,
    awghts, 
    sidelen = 25, 
    verbose = FALSE, 
    sanity_plotting = FALSE){
  
  dataset <- array(data = NA, dim = c( ( sidelen^2 ), n_replicates + 1, N_SIMS))
  print("Dataset dimensions:") 
  print(dim(dataset))
  
  # incorporate buffer (will chop off later)
  sidelen <- sidelen + (2*n_buffer)
  n <- sidelen^2
  
  #grid for data 
  gridList<- list( x= seq( 1,sidelen,length.out= sidelen),
                   y= seq( 1,sidelen,length.out= sidelen) )
  sGrid<- make.surface.grid(gridList)
  
  script_time <- system.time(
    for (sim in 1:N_SIMS){
      # set.seed(random_seed + sim)
      
      # choose params 
      kappa2 <- sample(awghts, 1) - 4
      theta <- runif(1, -pi/2, pi/2)
      rho <- runif(1, 1, 7)
      
      rhox <- sqrt(rho)
      rhoy <- 1/rhox
      
      if (sim %% 100 == 0){
        print(paste("Loop #", sim))
      }
      
      if (verbose == TRUE){
        print(paste("kappa2:", kappa2, "rho:", rho, "theta:", theta))
      }
      
      # populate H matrix 
      H11 <- ( rhox^2 * (cos(theta))^2) + ( rhoy^2 * (sin(theta))^2 ) 
      H12 <- (rhoy^2 - rhox^2)*(sin(theta)*cos(theta))
      H21 <- H12 
      H22 <- (rhox^2 * (sin(theta))^2) + (rhoy^2 * (cos(theta))^2)
      
      factor <- 0.5
      SAR_stencil <- c( rbind( c(factor*H12, -H22, -factor*H12),
                                  c(-H11, kappa2 + 2*H11 + 2*H22, -H11),
                                  c(-factor*H12, -H22, factor*H12) ) )
      
      awght_obj <- list( values=SAR_stencil)
      class(awght_obj )<- "constantValue"
      
      LKinfo <- LKrigSetup(sGrid, NC =sidelen, 
                           nlevel = 1,
                           a.wghtObject =  awght_obj, 
                           normalize=FALSE, 
                           NC.buffer = 0, overlap = 2.5)
      
      f <- LKrig.sim( sGrid, LKinfo, M = n_replicates, just.coefficients = TRUE)
      mu <- rowMeans(f)
      sd <- apply(f, 1, sd)
      
      #normalization 
      f <- (f - mu)/sd
      
      # trim off buffer 
      f_trim <- array( f, dim = c(sidelen, sidelen, 30))
      f_trim <- f_trim[(n_buffer + 1):(sidelen - n_buffer), 
                   (n_buffer + 1):(sidelen - n_buffer), ]
      f_trim <- array(f_trim, dim = c( (sidelen - (2*n_buffer))*(sidelen - (2*n_buffer)), n_replicates))
      
      
      if (sanity_plotting == TRUE){
        par(mfrow = c(1,2))
        
        plotgridList <- list( x= seq( 1,sidelen - (2*n_buffer),length.out= sidelen - (2*n_buffer)),
                              y= seq( 1,sidelen - (2*n_buffer),length.out= sidelen - (2*n_buffer)) )
        plotGrid<- make.surface.grid(plotgridList)
        
        image.plot( as.surface( sGrid, f[,1]) , col = turbo(256), 
                    main = "First field")
        image.plot( as.surface( plotGrid, f_trim[,1]) , col = turbo(256), 
                    main = "First field (trimmed)")
      }
      
      # add vector with params (the remaining entries are zeroes)
      final_sim <- cbind(f_trim, rep(0, times = nrow(f_trim)))
      
      final_sim[1,(n_replicates + 1)] <- kappa2
      final_sim[2,(n_replicates + 1)] <- theta
      final_sim[3,(n_replicates + 1)] <- rho
      
      if (verbose == TRUE){
        print("Sim + param dims:")
        print(dim(final_sim))
      }
      
      dataset[,,sim] <- final_sim
      
    } # simulation for loop
  ) # timing 
  
  print(paste("Simulation took ", script_time[[3]]/60," minutes to run."))
  return(dataset)
  
} # function 


############################################
## Experimental Setup and Hyperparameters ##
############################################

n_awghts_log <- 600
n_awght_unif <- 400
# awghts <- 4 + seq(sqrt(0.001), sqrt(2), length.out=n_awghts)^2
# awghts <- 4 + seq(0.0001, 2, length.out=n_awghts)
awghts_log <- 4 + exp(seq(log(0.0001),
                      log(2),
                      length.out=n_awghts_log))
awghts_unif <- 4 + seq(0.0001, 2, length.out=n_awght_unif)

awghts <- c(awghts_log, awghts_unif)

#sanity checks
summary(awghts)

# sanity plotting
par(mfrow = c(1,2))
hist(log(awghts-4), main = "log kappas", col = "lightgreen")
hist(awghts, main = "awghts", col = "gold")
par(mfrow = c(1,1))


n_replicates <- 30 
n_buffer <- 5
verbose <- FALSE
sanity_plotting <- FALSE
random_seed <- 777


###########################################
########## Simulation and Saving ##########
###########################################

# Simulation
dataset <- generate_cnn_data(
  N_SIMS = N_SIMS, 
  n_replicates = n_replicates,
  n_buffer = n_buffer,
  random_seed = random_seed,
  awghts = awghts, 
  sidelen = sidelen, 
  verbose = verbose, 
  sanity_plotting = sanity_plotting
)

# Saving the dataset 

file_name <- here("data", "CNN_data.h5")
#file_name <- here("data", "CNN_sample_data.h5")
dataset_name <- "fields"

nx <- sidelen^2
ny <- n_replicates + 1
nz <- N_SIMS

h5createFile(file_name)

h5createDataset(
  file = file_name, 
  dataset = dataset_name,
  dims = c(nx,ny,nz),
  maxdims = c(nx,ny, H5Sunlimited()),
  level = 9, 
  shuffle = FALSE
)

# time how long it takes to write 
write_time <- system.time(
  h5write(
    obj = dataset,
    file = file_name, 
    name = dataset_name, 
    index = list(1:nx, 1:ny, 1:nz)
  )
)
print(paste("Writing data to h5 took ", write_time[[3]]/60," minutes."))
# remember to close it
h5closeAll()


############################################
#### Loading in the Data (Sanity) ####
############################################

# file_name <- here("data", "CNN_data.h5")
# #file_name <- here("data", "CNN_sample_data.h5")
# dataset_name <- "fields"
# 
# print(h5ls(file_name))
# 
# dataset <- h5read(file_name, dataset_name)
# print(dim(dataset))
# 
# k <- 1
# replicate_num <- 7
# 
# gridList<- list( x= seq( 1,sidelen,length.out= sidelen),
#                  y= seq( 1,sidelen,length.out= sidelen) )
# sGrid<- make.surface.grid(gridList)
# 
# image.plot( as.surface( sGrid, dataset[,replicate_num,k]) , col = turbo(256),
#             main = "Sample Field",
#             xlab = paste("Kappa2:", round(dataset[1, n_replicates+1, k], 4),
#                          "Theta:", round(dataset[2, n_replicates+1, k], 3),
#                          "Rho:", round(dataset[3, n_replicates+1, k], 3))
#             )

``````{ end_of_file="R_scripts/cnn_datagen.R" }

``````{ path="R_scripts/helper_funcs.R"  }
library(LatticeKrig)
library(spam64)
library(tictoc)
library(rhdf5)
library(maps)
library(cmocean)
library(here)
rm(list=ls())


###########################################
######     Helper Functions          #####
###########################################

# Simulation function for generating realizations of the SAR
LKrig.sim <- function(x1, LKinfo, M = 1, just.coefficients = FALSE) {
  Q <- LKrig.precision(LKinfo)
  Qc <- chol(Q, memory = LKinfo$choleskyMemory)
  m <- LKinfo$latticeInfo$m
  E <- matrix(rnorm(M * m), nrow = m, ncol = M)
  randomC <- backsolve(Qc, E)
  if (just.coefficients) {
    return(randomC)
  } 
  else {
    PHI1 <- LKrig.basis(x1, LKinfo)
    return(PHI1 %*% randomC)
  }
}	


# LKrig function that creates the precision matrix
# which requires the creation of the SAR matrix 
LKrig.precision <- function(LKinfo, return.B = FALSE,
                            verbose=FALSE) { 
  L <- LKinfo$nlevel
  offset <- LKinfo$latticeInfo$offset
  # some checks on arguments
  LKinfoCheck(LKinfo)
  # ind holds non-zero indices and ra holds the values
  ind <- NULL
  ra <- NULL
  da <- rep(0, 2)
  # loop over levels
  for (j in 1:L) {
    # evaluate the SAR matrix at level j.
    tempB<- LKrigSAR( LKinfo, Level=j)
    if( verbose){
      cat("dim indices in spind of B:",dim( tempB$ind) , fill=TRUE)            	
    }
    # accumulate the new block
    # for the indices that are not zero
    ra <- c(ra,tempB$ra )
    ind <- rbind(ind, tempB$ind + offset[j])
    # increment the dimensions
    da[1] <- da[1] + tempB$da[1]
    da[2] <- da[2] + tempB$da[2]
  }
  # dimensions of the full matrix
  # should be da after loop
  # check this against indices in LKinfo
  #
  if ((da[1] != offset[L + 1]) | (da[2] != offset[L + 
                                                  1])) {
    stop("Mismatch of dimension with size in LKinfo")
  }
  # convert to spind format:
  # tempBtest <- list(ind = ind, ra = ra, da = da) 
  # tempB<- spind2spam( tempBtest)
  if( verbose){
    cat("dim of ind (fullB):", dim( ind), fill=TRUE)
  }
  tempB <- spam( list( ind=ind, ra), nrow=da[1], ncol=da[2])
  if( verbose){
    cat("dim after spind to spam in precision:", dim( tempB), fill=TRUE)
  }
  if (return.B) {
    return(tempB)
  }
  else {
    # find precision matrix Q = t(B)%*%B and return
    return(t(tempB) %*% (tempB))
  }
}


# returns sparse SAR matrix which is used in the 
# precision function, which is then called in the 
# simulation function. here we ensure the B matrix
# has periodic properties which allows us to 
# respect the Mercator projection

LKrigSAR <- function( object, Level, ...){ 
  
  mx1<-              object$latticeInfo$mx[Level,1]
  mx2<-              object$latticeInfo$mx[Level,2]
  m<- mx1*mx2
  #
  a.wght<- (object$a.wght)[[Level]]
  
  stationary <-     (attr( object$a.wght, "stationary"))[Level]
  first.order<-     attr( object$a.wght, "first.order")[Level]
  isotropic  <-     attr(object$a.wght, "isotropic")[Level]
  distance.type <-  object$distance.type
  if( all(stationary & isotropic) ) {
    if( any(unlist(a.wght) < 4) ){
      stop("a.wght less than 4")
    }
  }
  
  #  either  a.wght is a  matrix (rows index lattice locations)
  #  or fill out matrix of this size with stationary values
  dim.a.wght <- dim(a.wght)
  
  # figure out if just a single a.wght or matrix is passed
  # OLD see above first.order <-  (( length(a.wght) == 1)|( length(dim.a.wght) == 2)) 
  
  # order of neighbors and center
  index <- c(5, 4, 6, 2, 8, 3, 9, 1, 7)
  # dimensions of precision matrix
  da <- as.integer(c(m, m))
  # contents of sparse matrix organized as a 2-dimensional array
  # with the second dimension indexing the weights for center and four nearest neighbors.
  if (first.order) {
    ra <- array(NA, c(mx1*mx2, 5))
    ra[,  1] <- a.wght
    ra[,  2:5] <- -1
  }
  else {
    ra <- array(NA, c(mx1 * mx2, 9))
    for (kk in 1:9) {
      # Note that correct filling happens both as a scalar or as an mx1 X mx2 matrix
      if (stationary) {
        ra[ , kk] <- a.wght[index[kk]]
      }
      else {
        ra[ ,  kk] <- a.wght[ , index[kk]]
      }
    }
  }
  #
  #  Order for 5 nonzero indices is: center, top, bottom, left right
  #  a superset of indices is used to make the arrays regular.
  #  and NAs are inserted for positions beyond lattice. e.g. top neighbor
  #  for a lattice point on the top edge. The NA pattern is also
  #  consistent with how the weight matrix is filled.
  #
  Bi <- rep(1:m, 5)
  i.c <- matrix(1:m, nrow = mx1, ncol = mx2)
  # indices for center, top, bottom, left, right or ... N, S, E, W
  # NOTE that these are just shifts of the original matrix
  Bj <- c(i.c,
          LKrig.shift.matrix(i.c, 0, -1), # top 
          LKrig.shift.matrix(i.c, 0,  1), #bottom
          LKrig.shift.matrix(i.c, 1,  0, periodic = TRUE), #left (periodic here)
          LKrig.shift.matrix(i.c, -1, 0, periodic = TRUE)  #right (periodic here)
  )
  # indices for NW, SW, SE, SW
  if (!first.order) {
    Bi <- c(Bi, rep(1:m, 4))
    Bj <- c(Bj,
            LKrig.shift.matrix(i.c,  1,  1, periodic = c(TRUE,TRUE)), # bottom left (periodic here)
            LKrig.shift.matrix(i.c, -1,  1, periodic = c(TRUE,TRUE)), # bottom right (periodic here)
            LKrig.shift.matrix(i.c,  1, -1, periodic = c(TRUE,TRUE)), # top left (periodic here)
            LKrig.shift.matrix(i.c, -1, -1, periodic = c(TRUE,TRUE)) # top right (periodic here)
            # LKrig.shift.matrix(i.c,  1,  1), # bottom left 
            # LKrig.shift.matrix(i.c, -1,  1), # bottom right 
            # LKrig.shift.matrix(i.c,  1, -1), # top left 
            # LKrig.shift.matrix(i.c, -1, -1) # top right 
    )
  }
  
  # find all cases that are actually in lattice
  good <- !is.na(Bj)
  # remove cases that are beyond the lattice and coerce to integer
  # also reshape ra as a vector stacking the 9 columns
  #
  Bi <- as.integer(Bi[good])
  Bj <- as.integer(Bj[good])
  ra <- c(ra)[good]
  # return spind format because this is easier to accumulate
  # matrices at different multiresolution levels
  # see calling function LKrig.precision
  #
  # below is an add on hook to normalize the values to sum to 4 at boundaries
  if(!is.null(object$setupArgs$BCHook)){
    M<- da[1]
    for( i in 1: M){
      rowI<- which(Bi== i)
      rowNN<- rowI[-1]
      ra[rowNN]<-  4*ra[rowNN] / length(rowNN )
    }
    
  }
  return(list(ind = cbind(Bi, Bj), ra = ra, da = da))
}



# for making the SAR quickly 
LKrigSARGaussian<- function(LKinfo,M,
                            asList=FALSE ){
  coefSAR<- NULL
  nLevel<- LKinfo$nlevel
  for( K in 1:nLevel){
    B <- LKrigSAR(LKinfo,Level=K)
    B<- spind2spam(B) # convert to spam format 
    mLevel<- LKinfo$latticeInfo$mLevel[K]
    # here using Gaussian but can change to other distribution 
    U<- runif(mLevel*M )
    E<-  qnorm(U)
    E<- matrix(E, mLevel, M)
    #
    y<- as.matrix(solve(B, E)) # uses sparse methods
    if(!asList){
      coefSAR<- rbind(coefSAR, y)
    }
    else{ coefSAR<- c( coefSAR, list(y))}
  }
  return( coefSAR)
}


# function for defining fun coastline-like surfaces
coastline <- function(x, coast_coef, coast_bump_scale, coast_freq) {
  return(coast_coef * x + coast_bump_scale * sin(2 * pi * coast_freq * x) 
         + 0.01 * rnorm(length(x)))
}

# read current dims of h5 data
get_dims <- function() {
  info  <- h5ls(file_name, all=TRUE)
  row   <- subset(info, name==dataset_name & otype=="H5I_DATASET")
  dims  <- as.integer(strsplit(row$dim, " x ")[[1]])
  names(dims) <- c("nx","ny","nz")
  dims
}

# predict constant values
predict.constantValue<- function(object,x){
  n<- length(object$values)
  m<- nrow( x)
  return( matrix( object$values, nrow=m, ncol=n, byrow=TRUE ) )
}

# predict functions for param grids 
predict.surfaceGrid<- function(object,x){
  interp.surface( object, x)
}

# predict function for non-stationary, anisotropic awght
predict.multivariateSurfaceGrid<- function(object,x){
  dimZ<- dim( object$z)
  L<- dimZ[3]
  out<- matrix( NA, nrow= nrow(x), ncol=L)
  for (  l in 1:L){
    out[,l]<- interp.surface( 
      list( x=object$x,y=object$y, z=object$z[,,l]) , x)
  }
  return( out)
}


# param field generator for pattern figs
generate_param_field <- function(
    paramtype, 
    config, 
    sGrid, 
    rows, 
    columns,
    n){
  
  if (paramtype == "awght"){
    # sampling from predetermined awghts 
    # (this can be changed, just a design choice)
    lower_bound <- min(awghts)
    upper_bound <- max(awghts)
    midpoint <- (lower_bound + upper_bound)/2 # 5
    
    param_constant <- sample(awghts, 1)
    param_low <- sample(low_awghts, 1)
    param_high <- sample(high_awghts, 1)
    
    # these variables are dependent on the domain of the param (awght)
    gauss_amps <- runif(2, 0.1, 0.5) * ifelse(param_constant <= midpoint, 1, -1)
    mult_factor <- runif(1, 0.001, 0.1998)
    
  } else if (paramtype == "rho"){
    # rho is constructed simply based on bounds
    lower_bound <- 1
    upper_bound <- 7
    midpoint <- (lower_bound + upper_bound) / 2  # 4
    
    param_constant <- runif(1, lower_bound, upper_bound)
    param_low <- runif(1, lower_bound, param_constant)
    param_high <- runif(1, param_constant, upper_bound)
    
    # dependent on rho domain
    gauss_amps <- runif(2, 0.1, 1.5) * ifelse(param_constant <= midpoint, 1, -1) 
    mult_factor <- runif(1, 0.001, 0.7498)  
    
  } else if (paramtype == "theta"){
    # making sure to obtain all possible ellipses
    lower_bound <- 0
    upper_bound <- 3
    midpoint <- (lower_bound + upper_bound) / 2  # 1.5 (roughly pi/2)
    
    param_constant <- runif(1, lower_bound, upper_bound)
    param_low <- runif(1, lower_bound, param_constant)
    param_high <- runif(1, param_constant, upper_bound)
    
    # dependent on theta domain
    gauss_amps <- runif(2, 0.1, pi/4) * ifelse(param_constant < midpoint, 1, -1) 
    mult_factor <- runif(1, 0.001, 0.9998) 
    
  } else {
    stop("Unknown paramtype. Please use either awght, rho, or theta.")
  }
  
  # these quantities dont depend on which param field we are making
  taper_sd <- runif(1, 0.05, 1)
  
  gauss_slopes <- runif(2, 0.2, 0.5)
  # gauss locations need to be within [-1,1],[-1,1] domain
  gauss_locs <- runif(4, sGrid[1], tail(sGrid,1)[1]) 
  
  coast_sharpnesses <- runif(2, 3, 50)
  coast_bump_scales <- runif(2, 0.1, 0.5)
  coast_freqs <- runif(2, 0.4, 3)
  coast_coefs <- runif(2, -2 , 2)
  # make sure that adding coastlines doesnt go out of bounds
  coast_amp1 <- runif(1,0.1 , 0.9)
  coast_amp2 <- runif(1, 0.1, 1-coast_amp1)
  
  # make sure sin amplitude doesnt go out of bounds
  if (param_constant > midpoint){
    sin_amp <- runif(1, 0, upper_bound - param_constant)
  }
  else{
    sin_amp <- runif(1, 0, param_constant - lower_bound)
  }
  sin_freq <- runif(1, 1.5, 5)
  sin_orientation <- sample(c("horiz","vert"),1)
  
  #number of basis for generating a gp for the param field. 
  # a gp will then be generated from this gp. 
  #Just like inception...
  num_basis_param <- sample(c(6:32), 1) 
  
  if (config == "constant"){
    param_func <- rep(param_constant, n)
  }
  
  else if (config == "taper"){
    taper<- pnorm( sGrid[,1] + sGrid[,1],
                   mean = 0, sd = taper_sd)
    param_func<- param_low*taper +  param_high*(1-taper)
  }
  
  else if (config == "Gaussian"){
    param_func <- param_constant + 
      gauss_amps[1] * exp(-((sGrid[,1]^2 + sGrid[,2]^2) / gauss_slopes[1])) 
  }
  
  else if (config == "coastline"){
    param_func <- param_low + (param_high - param_low) / 
      (1 + exp(-coast_sharpnesses[1] * (sGrid[,2] - coastline(sGrid[,1], 
                                                              coast_coefs[1],
                                                              coast_bump_scales[1],
                                                              coast_freqs[1]))))
  }
  
  else if (config == "sinwave"){
    if (sin_orientation == "vert"){
      param_func <- param_constant + sin_amp * sin( pi * sGrid[,1] * sin_freq)
    }
    else{
      param_func <- param_constant + sin_amp * cos( pi * sGrid[,2] * sin_freq)
    }
  }
  
  else if (config == "double_Gaussian"){
    
    peak1 <- gauss_amps[1] * exp(-((sGrid[,1] - gauss_locs[1])^2 + 
                                     (sGrid[,2] - gauss_locs[2])^2) / gauss_slopes[1])
    peak2 <- gauss_amps[2] * exp(-((sGrid[,1] + gauss_locs[3])^2 + 
                                     (sGrid[,2] + gauss_locs[4])^2) / gauss_slopes[2])
    param_func <- param_constant + peak1 + peak2
  }
  
  else if (config == "double_coastline"){
    coastline1 <- (param_high - param_low)/
      (1 + exp(-coast_sharpnesses[1] * (sGrid[,2] - 
                                          coastline(sGrid[,1], 
                                                    coast_coefs[1], 
                                                    coast_bump_scales[1],
                                                    coast_freqs[1]))))
    coastline2 <- (param_high - param_low)/
      (1 + exp(-coast_sharpnesses[2] * (sGrid[,2] - 
                                          coastline(sGrid[,1], 
                                                    coast_coefs[2], 
                                                    coast_bump_scales[2],
                                                    coast_freqs[2]))))
    param_func <- param_low + (coast_amp1 * coastline1) + (coast_amp2 * coastline2)
  }
  
  else if (config == "GP_gp") {
    # pick scaling strategy half the time
    scale_choice <- sample(c("minmax","perturb"), size = 1, prob = c(0.5,0.5))
    
    # simulate the little GP
    LKinfo_param <- LKrigSetup(
      sGrid,
      NC        = num_basis_param,
      nlevel    = 1,
      a.wght    = sample(awghts, 1),
      nu        = 1,
      normalize = FALSE
    )
    f_param <- LKrig.basis(sGrid, LKinfo_param) %*% 
      LKrigSARGaussian(LKinfo_param, M = 1)
    
    if (scale_choice == "minmax") {
      # full min/max rescale to [lower_bound, upper_bound]
      f_min    <- min(f_param)
      f_max    <- max(f_param)
      f_norm   <- (f_param - f_min) / (f_max - f_min)
      param_func <- f_norm * (upper_bound - lower_bound) + lower_bound
      
    } else {
      # old small perturbation around param_constant technique 
      f_param <- (f_param - min(f_param)) / (max(f_param) - min(f_param))
      if (param_constant < midpoint) {
        param_func <- ((f_param * mult_factor) + 1) * param_constant
      } else {
        param_func <- (1 - (f_param * mult_factor)) * param_constant
      }
    }
  }
  
  # finally, reshape back to a matrix and return
  param_field <- matrix(param_func, nrow = rows, ncol = columns)
  return(param_field)
}


# function to plot model estimated params
plot_params <- function(
    kappa2,
    theta, 
    rho, 
    border_lwd = 1
){
  awght <- kappa2 + 4
  # sanity plotting for input and output params
  par(mfrow = c(2,2), mar =   c(3.1, 4.1, 2.1, 2.1))
  imagePlot(x = lon, y = lat, 
            first_field_norm, main = "First Clim Field", col = turbo(256)) #, 
  #horizontal = TRUE)
  map("world2", add = TRUE, 
      col = "grey0", lwd = border_lwd)
  
  imagePlot(x = lon, y = lat,
            awght, main = "awght", col = viridis(256))
  map("world2", add = TRUE, 
      col = "grey90", lwd = border_lwd)
  
  imagePlot(x = lon, y = lat,
            theta, main = "Theta", col = viridis(256))
  map("world2", add = TRUE, 
      col = "grey90", lwd = border_lwd)
  
  imagePlot(x = lon, y = lat,
            rho, main = "Rho", col = viridis(256))
  map("world2", add = TRUE, 
      col = "grey90", lwd = border_lwd)
  par(mfrow = c(1,1))
}

# function for plotting covariance surface with contours
plot_cov_surface <- function(
    LKinfo, 
    real_field,
    loc_x, 
    loc_y, 
    plotting = TRUE
){
  cov_surface <- LKrig.cov(sGrid, rbind(c(loc_x,loc_y)), LKinfo)
  cov_surface <- matrix(cov_surface, nrow = rows, ncol = columns)
  
  if (plotting == TRUE){
    par(mfrow = c(1,2), mar=  c(5.1, 4.1, 4.1, 2.1))
    image.plot(as.surface(sGrid, cov_surface), col = viridis(256), 
               main = "Correlation")
    contour( as.surface( sGrid, cov_surface), col = "white", add = TRUE)
    
    imagePlot(as.surface(sGrid, real_field), 
              main = "First Clim Field", col = turbo(256))
    contour( as.surface( sGrid, cov_surface), col = "grey30", add = TRUE)
    par(mfrow = c(1,1))
  }
  
  return (cov_surface)
}


# plots the real and simulated fields side by side 
plot_real_sim <- function(
    real_field, 
    simulated_field,
    border_lwd = 1
){
  # plot the real field and simulated field
  par(mfrow = c(1,2), mar=  c(5.1, 4.1, 4.1, 2.1))
  imagePlot(x = lon, y = lat,
            real_field, main = "", col = turbo(256)) #Clim Field
  map("world2", add = TRUE, 
      col = "grey30", lwd = border_lwd)
  
  image.plot(x = lon, y = lat,
             simulated_field, col = turbo(256), main = "") #Simulated field
  map("world2", add = TRUE, 
      col = "grey30", lwd = border_lwd)
  par(mfrow = c(1,1))
}

# function for centering plots in the pacific
pacific_centering <- function(fields){
  if (length(dim(fields)) == 3){
    fields <- fields[c((rows/2+1):rows,1:(rows/2)),,]
  }
  else if (length(dim(fields)) == 2){
    fields <- fields[c((rows/2+1):rows,1:(rows/2)),]
  }
  else {
    stop("Input must be a 2D or 3D array")
  }
}

# MAIN GENERATION FUNCTION
# takes in the estimated parameters and creates synthetic fields
generate_synthetic_reps <- function(
    kappa2, 
    theta, 
    rho,
    rhox,
    rhoy,
    n_replicates = 30,
    random_seed = 777,
    smooth_choice = FALSE,
    normalize = TRUE
){
  
  # create H tensor out of params
  H11 <- ( rhox^2 * (cos(theta))^2) + ( rhoy^2 * (sin(theta))^2 ) 
  H12 <- (rhoy^2 - rhox^2)*(sin(theta)*cos(theta))
  H21 <- H12 
  H22 <- (rhox^2 * (sin(theta))^2) + (rhoy^2 * (cos(theta))^2)
  
  # fill the high dimensional stencil (9 fields)
  stencil_tensor <- array( NA, c( rows,columns,9))
  stencil_tensor[,,1] <- 0.5 * H12
  stencil_tensor[,,2] <- -H22
  stencil_tensor[,,3] <- -0.5 * H12
  stencil_tensor[,,4] <- -H11
  stencil_tensor[,,5] <- kappa2 + 2 * H11 + 2 * H22
  stencil_tensor[,,6] <- -H11
  stencil_tensor[,,7] <- -0.5 * H12
  stencil_tensor[,,8] <- -H22
  stencil_tensor[,,9] <- 0.5 * H12
  
  # put everything into awght obj of a particular class
  awght_obj <- list( x= gridList$x,  y= gridList$y, z=stencil_tensor )
  class( awght_obj)<- "multivariateSurfaceGrid"
  
  #setup an LKinfo object for generating fields 
  set.seed(random_seed)
  LKinfo <- LKrigSetup(sGrid, NC =rows,
                       nlevel = 1, 
                       a.wghtObject =  awght_obj, 
                       normalize=FALSE, 
                       NC.buffer = 0, overlap = 2.5, nu = 1) 
  
  
  # generate fields
  gen_time <- system.time(
    f <- LKrig.sim( sGrid, LKinfo, M = n_replicates, just.coefficients = smooth_choice)
  )
  
  # normalize the fields 
  if (normalize == TRUE){
    mu <- rowMeans(f)
    sd <- apply(f, 1, sd) 
    f <- (f - mu)/sd
  }
  
  # SAVE 
  object <- list()
  object$gen_time <- gen_time
  object$LKinfo <- LKinfo
  object$f <- f
  object$mu <- mu
  object$sd <- sd
  
  return (object)
}

``````{ end_of_file="R_scripts/helper_funcs.R" }

``````{ path="R_scripts/i2i_datagen.R"  }
##########             IMPORTANT              ###########
#-------------------------------------------------------#
# Navigate to "Session" in the top left of RStudio and
# click on "Set Working Directory" and then 
# choose "To Source File Location". 
# Now you can run the script. 
#-------------------------------------------------------#

library(LatticeKrig)
library(spam64)
library(tictoc)
library(rhdf5)
library(here)
# helper functions
source(here("R_scripts", "helper_funcs.R"))


# Some choices to make: 
# total number of simulated fields 
total_sims <- 8000
# chunks the data will be created in (RAM constraints), 1000 is about 16 GB
chunk_size <- 1000
# Other options can be tweaked in the
# "Experimental Setup and Hyperparameters" section below. 


###########################################
######     Param Field Generator     ######
###########################################

generate_param_field <- function(
    paramtype,
    config,
    sGrid,
    rows,
    columns,
    n){

  if (paramtype == "awght"){
    # sampling from predetermined awghts
    # (this can be changed, just a design choice)
    lower_bound <- min(awghts)
    upper_bound <- max(awghts)
    midpoint <- (lower_bound + upper_bound)/2 # 5

    param_constant <- sample(awghts, 1)
    param_low <- sample(low_awghts, 1)
    param_high <- sample(high_awghts, 1)

    # these variables are dependent on the domain of the param (awght)
    gauss_amps <- runif(2, 0.1, 0.5) * ifelse(param_constant <= midpoint, 1, -1)
    mult_factor <- runif(1, 0.001, 0.1998)

  } else if (paramtype == "rho"){
    # rho is constructed simply based on bounds
    lower_bound <- 1
    upper_bound <- 7
    midpoint <- (lower_bound + upper_bound) / 2  # 4

    param_constant <- runif(1, lower_bound, upper_bound)
    param_low <- runif(1, lower_bound, param_constant)
    param_high <- runif(1, param_constant, upper_bound)

    # dependent on rho domain
    gauss_amps <- runif(2, 0.1, 1.5) * ifelse(param_constant <= midpoint, 1, -1)
    mult_factor <- runif(1, 0.001, 0.7498)

  } else if (paramtype == "theta"){
    # making sure to obtain all possible ellipses
    lower_bound <- 0
    upper_bound <- 3
    midpoint <- (lower_bound + upper_bound) / 2  # 1.5 (roughly pi/2)

    param_constant <- runif(1, lower_bound, upper_bound)
    param_low <- runif(1, lower_bound, param_constant)
    param_high <- runif(1, param_constant, upper_bound)

    # dependent on theta domain
    gauss_amps <- runif(2, 0.1, pi/4) * ifelse(param_constant < midpoint, 1, -1)
    mult_factor <- runif(1, 0.001, 0.9998)

  } else {
    stop("Unknown paramtype. Please use either awght, rho, or theta.")
  }

  # these quantities dont depend on which param field we are making
  taper_sd <- runif(1, 0.05, 1)

  gauss_slopes <- runif(2, 0.2, 0.5)
  # gauss locations need to be within [-1,1],[-1,1] domain
  gauss_locs <- runif(4, sGrid[1], tail(sGrid,1)[1])

  coast_sharpnesses <- runif(2, 3, 50)
  coast_bump_scales <- runif(2, 0.1, 0.5)
  coast_freqs <- runif(2, 0.4, 3)
  coast_coefs <- runif(2, -2 , 2)
  # make sure that adding coastlines doesnt go out of bounds
  coast_amp1 <- runif(1,0.1 , 0.9)
  coast_amp2 <- runif(1, 0.1, 1-coast_amp1)

  # make sure sin amplitude doesnt go out of bounds
  if (param_constant > midpoint){
    sin_amp <- runif(1, 0, upper_bound - param_constant)
  }
  else{
    sin_amp <- runif(1, 0, param_constant - lower_bound)
  }
  sin_freq <- runif(1, 1.5, 5)
  sin_orientation <- sample(c("horiz","vert"),1)

  #number of basis for generating a gp for the param field.
  # a gp will then be generated from this gp.
  #Just like inception...
  num_basis_param <- sample(c(6:32), 1)

  if (config == "constant"){
    param_func <- rep(param_constant, n)
  }

  else if (config == "taper"){
    taper<- pnorm( sGrid[,1] + sGrid[,1],
                   mean = 0, sd = taper_sd)
    param_func<- param_low*taper +  param_high*(1-taper)
  }

  else if (config == "Gaussian"){
    param_func <- param_constant +
      gauss_amps[1] * exp(-((sGrid[,1]^2 + sGrid[,2]^2) / gauss_slopes[1]))
  }

  else if (config == "coastline"){
    param_func <- param_low + (param_high - param_low) /
      (1 + exp(-coast_sharpnesses[1] * (sGrid[,2] - coastline(sGrid[,1],
                                                              coast_coefs[1],
                                                              coast_bump_scales[1],
                                                              coast_freqs[1]))))
  }

  else if (config == "sinwave"){
    if (sin_orientation == "vert"){
      param_func <- param_constant + sin_amp * sin( pi * sGrid[,1] * sin_freq)
    }
    else{
      param_func <- param_constant + sin_amp * cos( pi * sGrid[,2] * sin_freq)
    }
  }

  else if (config == "double_Gaussian"){

    peak1 <- gauss_amps[1] * exp(-((sGrid[,1] - gauss_locs[1])^2 +
                                     (sGrid[,2] - gauss_locs[2])^2) / gauss_slopes[1])
    peak2 <- gauss_amps[2] * exp(-((sGrid[,1] + gauss_locs[3])^2 +
                                     (sGrid[,2] + gauss_locs[4])^2) / gauss_slopes[2])
    param_func <- param_constant + peak1 + peak2
  }

  else if (config == "double_coastline"){
    coastline1 <- (param_high - param_low)/
      (1 + exp(-coast_sharpnesses[1] * (sGrid[,2] -
                                          coastline(sGrid[,1],
                                                    coast_coefs[1],
                                                    coast_bump_scales[1],
                                                    coast_freqs[1]))))
    coastline2 <- (param_high - param_low)/
      (1 + exp(-coast_sharpnesses[2] * (sGrid[,2] -
                                          coastline(sGrid[,1],
                                                    coast_coefs[2],
                                                    coast_bump_scales[2],
                                                    coast_freqs[2]))))
    param_func <- param_low + (coast_amp1 * coastline1) + (coast_amp2 * coastline2)
  }

  else if (config == "GP_gp") {
    # pick scaling strategy half the time
    scale_choice <- sample(c("minmax","perturb"), size = 1, prob = c(0.5,0.5))

    # simulate the little GP
    LKinfo_param <- LKrigSetup(
      sGrid,
      NC        = num_basis_param,
      nlevel    = 1,
      a.wght    = sample(awghts, 1),
      nu        = 1,
      normalize = FALSE
    )
    f_param <- LKrig.basis(sGrid, LKinfo_param) %*%
      LKrigSARGaussian(LKinfo_param, M = 1)

    if (scale_choice == "minmax") {
      # full min/max rescale to [lower_bound, upper_bound]
      f_min    <- min(f_param)
      f_max    <- max(f_param)
      f_norm   <- (f_param - f_min) / (f_max - f_min)
      param_func <- f_norm * (upper_bound - lower_bound) + lower_bound

    } else {
      # old small perturbation around param_constant technique
      f_param <- (f_param - min(f_param)) / (max(f_param) - min(f_param))
      if (param_constant < midpoint) {
        param_func <- ((f_param * mult_factor) + 1) * param_constant
      } else {
        param_func <- (1 - (f_param * mult_factor)) * param_constant
      }
    }
  }

  # finally, reshape back to a matrix and return
  param_field <- matrix(param_func, nrow = rows, ncol = columns)
  return(param_field)
}

###########################################
######      Full Data Generator      ######
###########################################

generate_i2i_data <- function(
    N_SIMS, 
    n_replicates = 30, 
    n_buffer = 10,
    random_seed = 777, 
    configs,
    awghts, 
    low_awghts, 
    high_awghts,
    rows = 288,
    columns = 192,
    verbose = FALSE, 
    sanity_plotting = FALSE){
  
  dataset <- array(data = NA, dim = c( ( rows * columns), n_replicates + 3, N_SIMS))
  print("Dataset dimensions:") 
  print(dim(dataset))
  
  rows <- rows + (2*n_buffer)
  columns <- columns + (2*n_buffer)
  n <- rows * columns
  
  #grid for data 
  gridList<- list( x= seq( 1,rows,length.out= rows),
                   y= seq( 1,columns,length.out= columns) )
  sGrid<- make.surface.grid(gridList)
  
  #grid for param fields 
  gridList_param<- list( x= seq( -1,1,length.out= rows),
                         y= seq( -1,1,length.out= columns) )
  sGrid_param<- make.surface.grid(gridList_param)
  
  script_time <- system.time(
    for (sim in 1:N_SIMS){
      # set.seed(random_seed + sim)
      
      # choose configs for params
      config_k <- sample(configs, 1)
      config_th <- sample(configs, 1)
      config_r <- sample(configs, 1)
      
      # make kappa field (with occasional stacking)
      stack_choice_k <- sample(c("yes", "no"), 1)
      if (stack_choice_k == "no") {
        kappa2 <- generate_param_field(
          paramtype = "awght",
          config    = config_k,
          sGrid     = sGrid_param,
          rows      = rows,
          columns   = columns,
          n         = n
        ) - 4
        config_k_2 <- NA
      } else {
        config_k_2   <- sample(configs, 1)
        kappa2_pt1   <- generate_param_field("awght", config_k,   sGrid_param, rows, columns, n) - 4
        kappa2_pt2   <- generate_param_field("awght", config_k_2, sGrid_param, rows, columns, n) - 4
        wght_k       <- runif(1, 0.1, 1)
        kappa2       <- (wght_k * kappa2_pt1) + ((1 - wght_k) * kappa2_pt2)
        rm(kappa2_pt1, kappa2_pt2)
      }
      
      # theta field (with stacking option)
      stack_choice_th <- sample(c("yes", "no"), 1)
      if (stack_choice_th == "no") {
        theta      <- generate_param_field(
          paramtype = "theta",
          config    = config_th,
          sGrid     = sGrid_param,
          rows      = rows,
          columns   = columns,
          n         = n
        )
        config_th_2 <- NA
      } else {
        config_th_2 <- sample(configs, 1)
        theta_pt1   <- generate_param_field("theta", config_th,   sGrid_param, rows, columns, n)
        theta_pt2   <- generate_param_field("theta", config_th_2, sGrid_param, rows, columns, n)
        wght_th     <- runif(1, 0.1, 1)
        theta       <- (wght_th * theta_pt1) + ((1 - wght_th) * theta_pt2)
        rm(theta_pt1, theta_pt2)
      }
      
      # rho field (with stacking option)
      stack_choice_r <- sample(c("yes", "no"), 1)
      if (stack_choice_r == "no") {
        rho        <- generate_param_field(
          paramtype = "rho",
          config    = config_r,
          sGrid     = sGrid_param,
          rows      = rows,
          columns   = columns,
          n         = n
        )
        config_r_2  <- NA
      } else {
        config_r_2 <- sample(configs, 1)
        rho_pt1    <- generate_param_field("rho", config_r,   sGrid_param, rows, columns, n)
        rho_pt2    <- generate_param_field("rho", config_r_2, sGrid_param, rows, columns, n)
        wght_r     <- runif(1, 0.1, 1)
        rho        <- (wght_r * rho_pt1) + ((1 - wght_r) * rho_pt2)
        rm(rho_pt1, rho_pt2)
      }
      
      # derive rhox and rhoy 
      rhox <- sqrt(rho)
      rhoy <- 1 / rhox
      
      if (sanity_plotting == TRUE){
        par(mfrow = c(2,2))
        image.plot(kappa2, col = viridis(256), main = "Kappa2 field")
        image.plot(theta, col = viridis(256), main = "Theta field")
        image.plot(rhox, col = viridis(256), main = "Rhox field")
        image.plot(rhoy, col = viridis(256), main = "Rhoy field")
        par(mfrow = c(1,1))
      }
      
      # populate the H tensor (4 fields)
      # in the case of a constant stencil, this acts as a 2x2 dispersion matrix
      H11 <- ( rhox^2 * (cos(theta))^2) + ( rhoy^2 * (sin(theta))^2 ) 
      H12 <- (rhoy^2 - rhox^2)*(sin(theta)*cos(theta))
      H21 <- H12 
      H22 <- (rhox^2 * (sin(theta))^2) + (rhoy^2 * (cos(theta))^2)
      
      # fill the high dimensional stencil (9 fields)
      stencil_tensor <- array( NA, c( rows,columns,9))
      stencil_tensor[,,1] <- 0.5 * H12
      stencil_tensor[,,2] <- -H22
      stencil_tensor[,,3] <- -0.5 * H12
      stencil_tensor[,,4] <- -H11
      stencil_tensor[,,5] <- kappa2 + 2 * H11 + 2 * H22
      stencil_tensor[,,6] <- -H11
      stencil_tensor[,,7] <- -0.5 * H12
      stencil_tensor[,,8] <- -H22
      stencil_tensor[,,9] <- 0.5 * H12
      
      # put everything into awght obj of a particular class
      awght_obj <- list( x= gridList$x,  y= gridList$y, z=stencil_tensor )
      class( awght_obj)<- "multivariateSurfaceGrid"
      
      # remove from memory for space
      rm(H11, H12, H21, H22, stencil_tensor)
      
      if (sanity_plotting == TRUE){
        par(mfrow = c(3,3))
        labels = c("top left corner", "top middle", "top right corner",
                   "middle left", "middle (awght)", "middle right",
                   "bottom left corner", "bottom middle", "bottom right corner")
        for (i in 1:9){
          image.plot(awght_obj$z[,,i], main = labels[i], col = viridis(256))
        }
        par(mfrow = c(1,1))
      }
      
      # setup LKrig object 
      # we make our own buffer here so the package buffer is set to 0
      LKinfo <- LKrigSetup(sGrid, NC =rows,
                           nlevel = 1, 
                           a.wghtObject =  awght_obj, 
                           normalize=FALSE, 
                           NC.buffer = 0, overlap = 2.5) 
      #NOTE: 
      # overlap can be set to anything between 0.001 and 1, it will be same (SAR)
      # once u get above 1 (like 1.001) you start to get basis function smoothing 
      # Although this does not matter for us here, as we will not use basis functions
      # we choose to operate with only the coefficients, which is why we select the 
      # just.coefficients = TRUE option below. This is equivalent to just a SAR!
      
      if (verbose) {
        cat(paste0(
          "Loop #", sim, "\n",
          "  kappa2 stack?     ", stack_choice_k,       "\n",
          "    config1:        ", config_k,             "\n",
          "    config2:        ", config_k_2,           "\n",
          "  theta stack?      ", stack_choice_th,      "\n",
          "    config1:        ", config_th,            "\n",
          "    config2:        ", config_th_2,          "\n",
          "  rho   stack?      ", stack_choice_r,       "\n",
          "    config1:        ", config_r,             "\n",
          "    config2:        ", config_r_2,           "\n\n"
        ))
      }
      
      print(paste0("Loop #", sim))
      
      # simulate the fields
      f <- LKrig.sim( sGrid, LKinfo, M = n_replicates, just.coefficients = TRUE)
      mu <- rowMeans(f)
      sd <- apply(f, 1, sd) 
      # normalize all of the fields
      f <- (f - mu)/sd
      
      if (sanity_plotting == TRUE){
        par(mfrow = c(1,2))
        image.plot(as.surface(sGrid, f[,1]), col = turbo(256), main = "First field")
        
        cov_surface <- LKrig.cov(sGrid, rbind(c(rows/2,columns/2)), LKinfo)
        image.plot( as.surface( sGrid, cov_surface), col = magma(256), 
                    main = paste("Covariance at", rows/2, ",", columns/2))
        contour( as.surface( sGrid, cov_surface), col = "white", add = TRUE)
        
        par(mfrow = c(1,1))
      }
      
      # do some reshaping to trim off the buffer (crust of thickness n_buffer)
      f <- array(f, dim = c(rows, columns, n_replicates))
      f <- f[(n_buffer + 1):(rows - n_buffer), 
             (n_buffer + 1):(columns - n_buffer), ]
      f <- array(f, dim = c( (rows - (2*n_buffer))*(columns - (2*n_buffer)), n_replicates) )
      
      # do the same for kappa2, theta, rhox
      kappa2 <- kappa2[(n_buffer + 1):(rows - n_buffer), 
                       (n_buffer + 1):(columns - n_buffer)]
      theta <- theta[(n_buffer + 1):(rows - n_buffer), 
                     (n_buffer + 1):(columns - n_buffer)]
      rhox <- rhox[(n_buffer + 1):(rows - n_buffer), 
                   (n_buffer + 1):(columns - n_buffer)]
      
      if (sanity_plotting == TRUE){
        par(mfrow = c(2,2))
        # first field
        plotgridList <- list( x= seq( 1,rows - (2*n_buffer),length.out= rows - (2*n_buffer)),
                              y= seq( 1,columns - (2*n_buffer),length.out= columns - (2*n_buffer)) )
        plotGrid<- make.surface.grid(plotgridList)
        image.plot(as.surface(plotGrid, f[,1]), col = turbo(256), main = "First field (trimmed)")
        
        # params
        image.plot(kappa2, col = viridis(256), main = "Kappa2 field (trimmed)")
        image.plot(theta, col = viridis(256), main = "Theta field (trimmed)")
        image.plot(rhox^2, col = viridis(256), main = "Rho field (trimmed)")
        par(mfrow = c(1,1))
      }
      
      # turn into vectors to combine with the spatial fields 
      kappa2 <- c(kappa2)
      theta <- c(theta)
      rho <- c(rhox^2)
      
      # combine all into one array 
      # we don't save rhoy or rhox, they come from rho
      final_sim <- cbind(f, kappa2, theta, rho)
      rm(f, kappa2, theta, rho)
      # print(dim(final_sim))
      dataset[,,sim] <- final_sim
      rm(final_sim)
    } # simulation (outer) loop
  ) # timing
  
  print(paste("Simulation took ", script_time[[3]]/60," minutes to run."))
  return(dataset)
  
} #function



############################################
## Experimental Setup and Hyperparameters ##
############################################

configs <- c("constant", "taper", "Gaussian", "Gaussian", 
             "coastline", "coastline", "sinwave", "double_Gaussian",
             "double_Gaussian", "double_coastline","double_coastline",
             "GP_gp", "GP_gp", "GP_gp", "GP_gp")

# Many different options for awght prior

# n_awghts <- 400
# awghts <- 4 + exp(seq(log(0.0001),
#                       log(2),
#                       length.out=n_awghts))
# awghts <- 4 + seq(sqrt(0.001), sqrt(2), length.out=n_awghts)^2
# awghts <- 4 + seq(0.0001, 2, length.out=n_awghts)

# Current awght prior
n_awghts_log <- 600      #100
n_awght_unif <- 400      #900
n_awghts <- n_awghts_log + n_awght_unif

awghts_log <- 4 + exp(seq(log(0.0001),
                          log(2),
                          length.out=n_awghts_log))
awghts_unif <- 4 + seq(0.0001, 2, length.out=n_awght_unif)

awghts <- c(awghts_log, awghts_unif)

low_awghts <- awghts[1:(n_awghts/2)]
high_awghts <- awghts[(n_awghts/2 + 1):n_awghts]

#sanity checks
summary(awghts)
summary(low_awghts)
summary(high_awghts)
summary(log(awghts-4))

# plotting for sanity 
par(mfrow = c(1,2))
hist(log(awghts-4), main = "log kappas", col = "lightgreen")
hist(awghts, main = "awghts", col = "gold")
par(mfrow = c(1,1))

rows <- 288
columns <- 192
n_buffer <- 10
random_seed <- 777

n_replicates <- 30
verbose <- FALSE 
sanity_plotting <- FALSE



###########################################
########## Simulation and Saving ##########
###########################################

nx <- rows * columns
ny <- n_replicates + 3

file_name <- here("data", "I2I_data.h5")
# file_name <- here("data", "I2I_sample_data.h5")
dataset_name <- "fields"

# create the file if it does not yet exist 
if (!file.exists(file_name)) {
  h5createFile(file_name)
  h5createDataset(
    file    = file_name,
    dataset = dataset_name,
    dims    = c(nx, ny, 0),              
    maxdims = c(nx, ny, H5Sunlimited()), 
    level   = 9,
    shuffle = FALSE
  )
}

for (start in seq(1, total_sims, by = chunk_size)) {
  this_chunk <- min(chunk_size, total_sims - start + 1)
  
  # Generate the data chunk
  generate_time <- system.time(
    dataset <- generate_i2i_data(
      N_SIMS = this_chunk, 
      n_replicates = n_replicates, 
      n_buffer = n_buffer,
      random_seed = random_seed, 
      configs = configs,
      awghts = awghts, 
      low_awghts = low_awghts, 
      high_awghts = high_awghts,
      rows = rows,
      columns = columns,
      verbose = verbose, 
      sanity_plotting = sanity_plotting
    )
  )
  
  print(paste("Generating data chunk took ", generate_time[[3]]/60," minutes."))
  
  dims <- get_dims()
  nz_old <- dims["nz"]
  nz_new <- this_chunk
  
  h5set_extent(
    file = file_name,
    dataset = dataset_name,
    dims = c(dims["nx"], dims["ny"], nz_old + nz_new)
  )
  
  write_time <- system.time(
    h5write(
      obj = dataset,
      file = file_name, 
      name = dataset_name, 
      index = list(1:nx, 1:ny, (nz_old + 1):(nz_old + nz_new))
    )
  )
  print(paste("Writing data chunk to h5 took ", write_time[[3]]/60," minutes."))
  h5closeAll()
  
  # for memory purposes 
  rm(dataset)
}


###########################################
### Loading in Subset of Data (Sanity) ####
###########################################
# 
# file_name <- here("data", "I2I_data.h5")
# dataset_name <- "fields"
# 
# print(h5ls(file_name))
# 
# rows <- 288
# columns <- 192
# nx <- rows * columns
# ny <- 33
# #choose how many simulations
# nz <- 4
# 
# subset_data <- h5read(file_name, dataset_name, index = list(1:nx, 1:ny, 1:nz))
# print(dim(subset_data))
# 
# 
# gridList<- list( x= seq( 1,rows,length.out= rows),
#                  y= seq( 1,columns,length.out= columns) )
# sGrid<- make.surface.grid(gridList)
# 
# 
# for (i in 1:4){
#   k = i
# par(mfrow = c(2,3))
# for (i in c(1,30,31,32,33)){
#   if (i == 1 || i == 30){
#     image.plot(as.surface(sGrid, subset_data[,i,k]), col = turbo(256) )
#   } else {
#     image.plot(as.surface(sGrid, subset_data[,i,k]), col = viridis(256))
#   }
# }
# par(mfrow = c(1,1))
# }
``````{ end_of_file="R_scripts/i2i_datagen.R" }

``````{ path="R_scripts/required_packages.R"  }
install.packages("LatticeKrig")
install.packages("spam64")
install.packages("tictoc")
install.packages("maps")
install.packages("cmocean")
install.packages("here")
if (!require("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install(
  "rhdf5",
  ask    = FALSE,    
  update = FALSE 
)
``````{ end_of_file="R_scripts/required_packages.R" }

``````{ path="latticevision/cnn/__init__.py"  }

``````{ end_of_file="latticevision/cnn/__init__.py" }

``````{ path="latticevision/cnn/dataset.py"  }
import h5py
import numpy as np
import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from dataclasses import dataclass
import sys


@dataclass(kw_only=True)
class DataConfig:
	"""
	Config object for local CNN dataset.

	Attributes:
		file_path: str
			Path to the HDF5 file containing the data.
		sidelen: int
			Length of one side of the square field images.
		n_replicates: int
			Number of field replicates per simulation.
		n_params: int
			Number of parameters in the dataset.
		log_kappa2: bool
			If True, log-transforms the first parameter (kappa2). Default is True.
		val_size: float
			Proportion of the full dataset to include in the validation set. Default is 0.2.
		test_size: float
			Proportion of the validation dataset to make into the test set. Default is 0.2.
		random_state: int
			Random seed for reproducibility. Default is 777.
		verbose: bool
			If True, prints additional information during processing. Default is True.
	"""

	file_path: str
	fullsidelen: int
	sidelen: int
	n_replicates: int
	n_params: int
	log_kappa2: bool = True
	val_size: float = 0.2
	test_size: float = 0.2
	random_state: int = 777
	verbose: bool = True


class CNNDataClass(Dataset):
	"""
	PyTorch Dataset class for local CNN data.
	Attributes:
	    fields: np.ndarray
	        Stores the input field data.
	    params: np.ndarray
	        Stores the parameter data.

	Methods:
	    __len__: Returns the number of samples in the dataset.
	    __getitem__: Retrieves a sample from the dataset by index, adding a channel dimension for fields.
	"""

	def __init__(self, fields, params):
		self.fields = fields
		self.params = params

	def __len__(self):
		return len(self.fields)

	def __getitem__(self, idx):
		# adding channel dimension for fields
		field = torch.tensor(self.fields[idx], dtype=torch.float32)
		params = torch.tensor(self.params[idx], dtype=torch.float32)
		return field, params


def make_dataset(config: DataConfig) -> dict:
	"""
	Creates local CNN train, validation, and test datasets.

	Args:
		config: DataConfig
			Configuration object containing dataset parameters.

	Returns:
		A dict containing:
		- train_df: CNNDataClass for training data
		- val_df: CNNDataClass for validation data
		- test_df: CNNDataClass for test data
	"""
	with h5py.File(config.file_path, "r") as f:
		if config.verbose:
			print("Components in the file:", list(f.keys()))
		# extract fields
		fields = f["fields"][:]

		if config.verbose:
			print("Dataset size (MB): ", sys.getsizeof(fields) / 1024**2)
			print("Dataset size (GB): ", sys.getsizeof(fields) / 1024**3)

	# extract params
	params = fields[:, -1, : config.n_params]
	fields = fields[:, :-1, :]
	fields = fields[:, : config.n_replicates, :]

	# reshape into proper form and then extract params
	n_sims, n_fields, _ = fields.shape
	fields = fields.reshape(n_sims, n_fields, config.fullsidelen, config.fullsidelen)

	if config.sidelen != config.fullsidelen:
		fields = fields[:, :, : config.sidelen, : config.sidelen]

	# kappas make sense on a log scale
	if config.log_kappa2:
		params[:, 0] = np.log(params[:, 0])

	if config.verbose:
		print("Dims of 'fields':", fields.shape)
		print("Dims of 'params':", params.shape)

	# split into train, validation, and test
	fields_train, fields_temp, params_train, params_temp = train_test_split(
		fields, params, test_size=config.val_size, random_state=config.random_state
	)

	fields_val, fields_test, params_val, params_test = train_test_split(
		fields_temp,
		params_temp,
		test_size=config.test_size,
		random_state=config.random_state,
	)

	if config.verbose:
		print("Train fields shape: ", fields_train.shape)
		print("Train params shape: ", params_train.shape)
		print("Validation fields shape: ", fields_val.shape)
		print("Validation params shape: ", params_val.shape)
		print("Test fields shape: ", fields_test.shape)
		print("Test params shape: ", params_test.shape)

	# create dataset objects
	train_df = CNNDataClass(fields_train, params_train)
	val_df = CNNDataClass(fields_val, params_val)
	test_df = CNNDataClass(fields_test, params_test)

	return {"train_df": train_df, "val_df": val_df, "test_df": test_df}

``````{ end_of_file="latticevision/cnn/dataset.py" }

``````{ path="latticevision/cnn/eval.py"  }
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

from latticevision.img_augment import random_augment
from latticevision.img2img.dataset import DataConfig

import pandas as pd


def eval_model(
	model: nn.Module,
	device: torch.device,
	config: DataConfig,
	test_loader: DataLoader,
	test_df: Dataset,
	plot: bool = True,
	augmentation: bool = False,
	show: bool = True,
) -> dict:
	"""
	Evaluates the CNN model on the test set by computing a number of metrics
	for each parameter (kappa2, theta, rho) and optionally plots the predicted
	vs actual values.

	Metrics computed (for each parameter):
		- RMSE: Root Mean Squared Error
		- MAE: Mean Absolute Error
		- R2: Coefficient of Determination
		- NRMSE: Normalized Root Mean Squared Error

	Args:
		model (nn.Module): The trained CNN model.
		device (torch.device): The device to run the model on.
		config (DataConfig): Data configuration object.
		test_loader (DataLoader): DataLoader for the test set.
		test_df (Dataset): The test dataset.
		plot (bool): If True, plots the predicted vs actual values. Default is True.
		augmentation (bool): If True, applies random augmentations to the fields during evaluation. Default is False.
		show (bool): If True, shows the plots. Default is True.

	Returns:
		dict: A dictionary containing the computed metrics.
	"""
	# set to eval mode
	model.eval()

	# create dicts for predictions and true values for each param.
	preds = {"kappa2": [], "theta": [], "rho": []}
	truths = {"kappa2": [], "theta": [], "rho": []}

	with torch.no_grad():
		for fields_batch, params_batch in test_loader:
			if augmentation:
				fields_batch = random_augment(fields_batch)

			fields_batch = fields_batch.to(device)
			params_batch = params_batch.to(device)

			# pass through the model
			outputs = model(fields_batch)

			# seaparate params (channel dimension)
			preds["kappa2"].append(outputs[:, 0].cpu().numpy())
			preds["theta"].append(outputs[:, 1].cpu().numpy())
			preds["rho"].append(outputs[:, 2].cpu().numpy())

			truths["kappa2"].append(params_batch[:, 0].cpu().numpy())
			truths["theta"].append(params_batch[:, 1].cpu().numpy())
			truths["rho"].append(params_batch[:, 2].cpu().numpy())

	for key in preds.keys():
		preds[key] = np.concatenate(preds[key], axis=0)  # shape: (N,)
		truths[key] = np.concatenate(truths[key], axis=0)

	preds_awght = np.exp(preds["kappa2"]) + 4
	truths_awght = np.exp(truths["kappa2"]) + 4

	def compute_metrics(true: np.ndarray, pred: np.ndarray):
		eps = 1e-6
		mse = np.mean((true - pred) ** 2)
		rmse = np.sqrt(mse)
		mae = np.mean(np.abs(true - pred))
		r2 = 1 - np.sum((true - pred) ** 2) / np.sum((true - np.mean(true)) ** 2 + eps)
		nrmse = rmse / (np.max(true) - np.min(true) + eps)
		return rmse, mae, r2, nrmse

	awght_metrics = compute_metrics(truths_awght, preds_awght)  # computed for reference
	kappa2_metrics = compute_metrics(truths["kappa2"], preds["kappa2"])
	theta_metrics = compute_metrics(truths["theta"], preds["theta"])
	rho_metrics = compute_metrics(truths["rho"], preds["rho"])

	# put into dict
	metrics = {
		"kappa2": {
			"rmse": kappa2_metrics[0],
			"mae": kappa2_metrics[1],
			"r2": kappa2_metrics[2],
			"nrmse": kappa2_metrics[3],
		},
		"theta": {
			"rmse": theta_metrics[0],
			"mae": theta_metrics[1],
			"r2": theta_metrics[2],
			"nrmse": theta_metrics[3],
		},
		"rho": {
			"rmse": rho_metrics[0],
			"mae": rho_metrics[1],
			"r2": rho_metrics[2],
			"nrmse": rho_metrics[3],
		},
	}

	# predicted vs actual plots for each param
	if plot:
		fig, axes = plt.subplots(2, 2, figsize=(10, 7.5))

		# --- log(kappa2) Predicted vs. Actual (Top Left) ---
		ax = axes[0, 0]
		ax.scatter(
			truths["kappa2"], preds["kappa2"], s=6, alpha=0.4, color="mediumslateblue"
		)
		min_val = min(np.min(truths["kappa2"]), np.min(preds["kappa2"]))
		max_val = max(np.max(truths["kappa2"]), np.max(preds["kappa2"]))
		ax.plot([min_val, max_val], [min_val, max_val], "k--")
		ax.set_title(r"$\log(\kappa^2)$ Predicted vs Actual")
		ax.set_xlabel("Actual")
		ax.set_ylabel("Predicted")

		# --- awght Predicted vs. Actual (Top Right) ---
		ax = axes[0, 1]
		ax.scatter(truths_awght, preds_awght, s=6, alpha=0.4, color="orange")
		min_val = min(np.min(truths_awght), np.min(preds_awght))
		max_val = max(np.max(truths_awght), np.max(preds_awght))
		ax.plot([min_val, max_val], [min_val, max_val], "k--")
		ax.set_title("awght Predicted vs Actual")
		ax.set_xlabel("Actual")
		ax.set_ylabel("Predicted")

		# --- theta Predicted vs. Actual (Bottom Left) ---
		ax = axes[1, 0]
		ax.scatter(truths["theta"], preds["theta"], s=6, alpha=0.4, color="deeppink")
		min_val = min(np.min(truths["theta"]), np.min(preds["theta"]))
		max_val = max(np.max(truths["theta"]), np.max(preds["theta"]))
		ax.plot([min_val, max_val], [min_val, max_val], "k--")
		ax.set_title(r"$\theta$ Predicted vs Actual")
		ax.set_xlabel("Actual")
		ax.set_ylabel("Predicted")

		# --- rho Predicted vs. Actual (Bottom Right) ---
		ax = axes[1, 1]
		ax.scatter(truths["rho"], preds["rho"], s=6, alpha=0.4, color="darkturquoise")
		min_val = min(np.min(truths["rho"]), np.min(preds["rho"]))
		max_val = max(np.max(truths["rho"]), np.max(preds["rho"]))
		ax.plot([min_val, max_val], [min_val, max_val], "k--")
		ax.set_title(r"$\rho$ Predicted vs Actual")
		ax.set_xlabel("Actual")
		ax.set_ylabel("Predicted")

		plt.tight_layout()
		if show:
			plt.show()
		else:
			plt.close(fig)

	# metrics dataframe
	rows = []
	for param, metric_vals in metrics.items():
		row = {"parameter": param}
		row.update(metric_vals)
		rows.append(row)
	df_metrics = pd.DataFrame(rows).set_index("parameter")
	print(df_metrics)

	return df_metrics


def fast_cnn_field_tiler(
	model: nn.Module,
	fields: torch.Tensor,
	device: torch.device,
	patch_batch_size: int = 1000,
	padding_mode: str = "reflect",
	verbose: bool = False,
	patch_size: int = 25,
) -> torch.Tensor:
	"""
	Faster CNN tiler for local parameter estimation across a large field,
	with no_grad() and per-batch cache clearing to avoid OOM errors.
	"""
	model = model.to(device).eval()
	B, C, H, W = fields.shape
	pad = patch_size // 2

	# field operations happen on cpu
	fields = fields.cpu()
	fields_padded = F.pad(fields, (pad, pad, pad, pad), mode=padding_mode)

	# unfold into all patches (on cpu)
	patches = (
		fields_padded.unfold(2, patch_size, 1)
		.unfold(3, patch_size, 1)
		.contiguous()
		.view(B, C, H * W, patch_size, patch_size)
		.permute(0, 2, 1, 3, 4)
		.reshape(-1, C, patch_size, patch_size)
	)

	if verbose:
		print(f"Total patches: {patches.size(0)}")

	outputs = []
	total = patches.size(0)

	# loop with no_grad + cleanup each batch
	with torch.no_grad():
		for start in range(0, total, patch_batch_size):
			end = min(start + patch_batch_size, total)
			batch = patches[start:end].to(device)

			out = model(batch)
			out = out.view(out.size(0), -1).cpu()
			outputs.append(out)

			if verbose:
				print(f"  ⋅ processed patches {start}–{end}")

			# memory management
			del batch, out
			torch.cuda.empty_cache()

	# all back on cpu
	patches = None
	all_out = torch.cat(outputs, dim=0)
	all_out = all_out.view(B, H, W, -1).permute(0, 3, 1, 2).contiguous()

	return all_out


def slow_cnn_field_tiler(
	model: torch.nn.Module,
	fields: torch.Tensor,
	device: torch.device,
	padding_mode: str = "reflect",
	verbose: bool = False,
	patch_size: int = 25,
) -> torch.Tensor:
	"""
	Function for using the CNN as a local estimation tool across a large spatial field.
	This function is much slower but uses less cuda memory (primarily uses the cpu).
	This function uses a double for loop to loop over each pixel (inefficient).

	Args:
		model (nn.Module): The trained CNN model.
		fields (torch.Tensor): The input fields to estimate on.
		device (torch.device): The device to run the model on.
		padding_mode (str): The padding mode for the fields. Default is 'replicate'.
		verbose (bool): If True, prints progress updates. Default is False.

	Returns:
		torch.Tensor: The resulting global parameter fields produced by local estimation.
	"""

	patch_size = patch_size
	pad = patch_size // 2

	# make sure data is correct shape
	B, C, H, W = fields.shape

	# pad all fields
	fields_padded = F.pad(fields, (pad, pad, pad, pad), mode=padding_mode)

	# outputs go here
	output_channels = 3
	outputs = torch.zeros((B, output_channels, H, W))

	model.to(device)
	model.eval()

	with torch.no_grad():
		for b in range(B):
			if verbose:
				print(f"Processing field {b + 1}/{B}...")
			# loop over each pixel
			for i in range(H):
				for j in range(W):
					# extract patch centered at that pixel
					patch = fields_padded[b, :, i : i + patch_size, j : j + patch_size]
					patch = patch.unsqueeze(0)
					patch = patch.to(device)

					# pass through the model. prediction vals corresponds to center pixel
					pred = model(patch)
					pred = pred.squeeze()

					# in case model returns a scalar when batch size is 1, ensure we have a tensor of shape (3,)
					if pred.dim() == 0:
						pred = pred.unsqueeze(0)

					# put on cpu and save
					outputs[b, :, i, j] = pred.cpu()

	# make sure all on cpu and cuda cache clear (was having memory issues earlier)
	outputs = outputs.detach().cpu()
	torch.cuda.empty_cache()
	return outputs

``````{ end_of_file="latticevision/cnn/eval.py" }

``````{ path="latticevision/cnn/model.py"  }
import torch.nn as nn
from dataclasses import dataclass
from typing import Tuple, List, Union


@dataclass(kw_only=True)
class ModelConfig:
	"""
	Model configuration for local estimation tasks with CNN.

	Attributes:
	    in_channels (int): Number of input channels.
	    out_params (int): Number of output parameters.
	    conv_channels (List[int]): Number of output channels for each convolutional layer.
	    kernel_size (Tuple[int, int]): Size of the convolutional kernel.
	    stride (Tuple[int, int]): Stride of the convolutional kernel.
	    pool_kernel_size (Tuple[int, int]): Size of the pooling kernel.
	    pool_stride (Tuple[int, int]): Stride of the pooling kernel.
	    linear_sizes (List[int]): Number of neurons for each linear layer.
	"""

	sidelen: int = 25
	in_channels: int = 30
	out_params: int = 3
	conv_channels: List[int] = (64, 128, 256)
	# kernel_size: Tuple[int, int] = (3, 3)
	kernel_sizes: List[int] = (10, 7, 5)
	stride: Tuple[int, int] = (1, 1)
	pool_kernel_size: Tuple[int, int] = (2, 2)
	pool_stride: Tuple[int, int] = (2, 2)
	linear_sizes: List[int] = (256, 128, 64)
	padding: Union[int, str] = 0


class CNN(nn.Module):
	"""
	CNN model for local estimation tasks.
	Attributes:
	    config (ModelConfig): Configuration for the model.

	Methods:
		forward(x: torch.Tensor):
			Forward pass through the model.
	"""

	def __init__(
		self,
		config: ModelConfig,
	):
		super(CNN, self).__init__()
		self.config = config

		conv_layers = []
		in_ch = config.in_channels

		for out_ch, ksz in zip(config.conv_channels, config.kernel_sizes):
			conv_layers.append(
				nn.Conv2d(
					in_channels=in_ch,
					out_channels=out_ch,
					kernel_size=ksz,
					stride=config.stride,
					padding=config.padding,
				)
			)
			# using GELU activation
			conv_layers.append(nn.GELU())
			in_ch = out_ch

		# final conv list is sequential
		self.conv_layers = nn.Sequential(*conv_layers)

		# max pooling layers to downsample and focus on broader features
		self.pool = nn.MaxPool2d(
			kernel_size=config.pool_kernel_size,
			stride=config.pool_kernel_size,
		)

		# output size for flattening
		in_size = in_ch * 3 * 3

		# create fully connected layers
		fc_layers = []
		for size in config.linear_sizes:
			fc_layers.append(nn.Linear(in_size, size))
			fc_layers.append(nn.GELU())
			in_size = size

		# additional layer required to output the correct number of parameters
		fc_layers.append(nn.Linear(in_size, config.out_params))

		# final fc list is sequential
		self.fc_layers = nn.Sequential(*fc_layers)

	def forward(self, x):
		# convolutional layers
		x = self.conv_layers(x)
		# pooling (downsample)
		x = self.pool(x)
		# reshape (flatten) for fc layers
		x = x.view(x.size(0), -1)
		# fully connected layers
		x = self.fc_layers(x)

		return x

``````{ end_of_file="latticevision/cnn/model.py" }

``````{ path="latticevision/cnn/train.py"  }
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from dataclasses import dataclass
# from torchvision import models

from latticevision.img_augment import random_augment


@dataclass(kw_only=True)
class TrainingConfig:
	"""
	Training configuration for local estimation tasks with CNN.

	Attributes:
	    model: nn.Module
	        The neural network model to be trained (e.g., STUN or CNN).
	    device: torch.device
	        The device on which computations are performed (e.g., "cuda" or "cpu").
	    train_loader: DataLoader
	        DataLoader for training data.
	    val_loader: DataLoader
	        DataLoader for validation data.
	    train_df: Dataset
	        The training dataset, used to calculate the training loss.
	    val_df: Dataset
			The validation dataset, used to calculate the validation loss.
	    lr: float, optional
	        The learning rate for the optimizer. Default is 5e-5.
	    n_epochs: int, optional
	        Maximum number of epochs for training. Default is 100.
	    stop_patience: int, optional
	        Number of epochs with no improvement to trigger early stopping. Default is 10.
	    scheduler_patience: int, optional
	        Number of epochs with no improvement to reduce learning rate. Default is 5.
	    scheduler_factor: float, optional
	        Factor to reduce learning rate when validation loss plateaus. Default is 0.5.
	    augmentation: bool, optional
	        If True, applies random augmentations to the fields during training. Default is False.
	    save: bool, optional
	        If True, saves the best model weights. Default is True.
	    save_directory: str, optional
	        The directory to save the model weights. Default is "../results/model_wghts/".
	    savename: str, optional
	        The name of the saved model weights file. Default is "cnn_wghts.pth".
	    verbose: bool, optional
	        If True, prints progress, including loss values and learning rate updates. Default is True.
	    normalize: bool, optional
	        If True, normalizes the input fields inside of the training loop and computes the loss as such.
			Default is True.
	    shuffle: bool, optional
	        If True, shuffles the input fields. Default is True.
	"""

	model: nn.Module
	device: torch.device
	train_loader: DataLoader
	val_loader: DataLoader
	train_df: Dataset
	val_df: Dataset
	lr: float = 5e-5
	n_epochs: int = 200
	stop_patience: int = 10
	scheduler_patience: int = 5
	scheduler_factor: float = 0.5
	augmentation: bool = False
	save: bool = True
	save_directory: str = "../results/model_wghts/"
	savename: str = "cnn_wghts.pth"
	verbose: bool = True
	normalize: bool = True
	shuffle: bool = True


def train_model(config: TrainingConfig) -> dict:
	"""
	Training function for a CNN model.

	Args:
		config: TrainingConfig
			Training configuration for the CNN model.

	Returns:
		dict: A dict containing:
			- model: nn.Module
				The trained model with the best validation weights loaded.
			- train_losses: list
				List of training loss values over epochs.
			- val_losses: list
				List of validation loss values over epochs.
	"""
	model = config.model

	if config.verbose:
		print(f"Training for {config.n_epochs} epochs with learning rate {config.lr}.")
		if config.save == True:
			print(
				"Model weights will be saved at "
				+ config.save_directory
				+ config.savename
			)
		if config.save == False:
			print("Model weights will not be saved.")
		if config.augmentation:
			print("Augmentation has been enabled.")
		if config.augmentation == False:
			print("Augmentation has been disabled.")
		if config.normalize:
			print("Normalization has been enabled.")
		if config.normalize == False:
			print("Normalization has been disabled.")
		if config.shuffle:
			print("Shuffling has been enabled.")
		if config.shuffle == False:
			print("Shuffling has been disabled.")

	# calculate mean and std across entire training set for normalization
	if config.normalize:
		kappa2_mean = torch.mean(config.train_df[:][1][:, 0])
		kappa2_std = torch.std(config.train_df[:][1][:, 0])

		theta_mean = torch.mean(config.train_df[:][1][:, 1])
		theta_std = torch.std(config.train_df[:][1][:, 1])

		rho_mean = torch.mean(config.train_df[:][1][:, 2])
		rho_std = torch.std(config.train_df[:][1][:, 2])

	# to avoid division by zero
	eps = 1e-6

	# hyperparameter settings
	criterion = nn.MSELoss()
	optimizer = optim.AdamW(model.parameters(), lr=config.lr)
	scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
		optimizer,
		mode="min",
		factor=config.scheduler_factor,
		patience=config.scheduler_patience,
		# verbose=True,
	)

	patience = config.stop_patience
	best_val_loss = float("inf")
	early_stop_counter = 0

	# items to return
	train_losses = []
	val_losses = []
	best_model_wts = model.state_dict()

	# training
	for epoch in range(config.n_epochs):
		model.train()
		train_loss_current = 0

		# Training loop -----------------------------------------
		for batch in config.train_loader:
			fields_batch, params_batch = batch

			# augmentation happens half the time
			if config.augmentation:
				aug_choice = np.random.choice([True, False])
				if aug_choice:
					fields_batch = random_augment(fields_batch)

			fields_batch, params_batch = (
				fields_batch.to(config.device),
				params_batch.to(config.device),
			)

			if config.shuffle:
				fields_batch = fields_batch[
					:, torch.randperm(fields_batch.size(1)), :, :
				]

			optimizer.zero_grad()
			output = model(fields_batch)

			if config.normalize:
				# normalized loss computation
				kappa2_output_norm = (output[:, 0] - kappa2_mean) / (kappa2_std + eps)
				theta_output_norm = (output[:, 1] - theta_mean) / (theta_std + eps)
				rho_output_norm = (output[:, 2] - rho_mean) / (rho_std + eps)

				kappa2_norm = (params_batch[:, 0] - kappa2_mean) / (kappa2_std + eps)
				theta_norm = (params_batch[:, 1] - theta_mean) / (theta_std + eps)
				rho_norm = (params_batch[:, 2] - rho_mean) / (rho_std + eps)

				kappa2_loss = criterion(kappa2_output_norm, kappa2_norm)
				theta_loss = criterion(theta_output_norm, theta_norm)
				rho_loss = criterion(rho_output_norm, rho_norm)
				loss = kappa2_loss + theta_loss + rho_loss

			else:
				kappa2_loss = criterion(output[:, 0], params_batch[:, 0])
				theta_loss = criterion(output[:, 1], params_batch[:, 1])
				rho_loss = criterion(output[:, 2], params_batch[:, 2])
				loss = kappa2_loss + theta_loss + rho_loss

			loss.backward()
			optimizer.step()
			train_loss_current += loss.item() * fields_batch.size(0)

		train_loss = train_loss_current / len(config.train_df)
		train_losses.append(train_loss)

		# Validation loop --------------------------------------
		model.eval()
		val_loss_current = 0
		with torch.no_grad():
			for batch in config.val_loader:
				fields_batch, params_batch = batch

				# augmentation happens half the time
				if config.augmentation:
					aug_choice = np.random.choice([True, False])
					if aug_choice:
						fields_batch = random_augment(fields_batch)

				fields_batch, params_batch = (
					fields_batch.to(config.device),
					params_batch.to(config.device),
				)

				if config.shuffle:
					fields_batch = fields_batch[
						:, torch.randperm(fields_batch.size(1)), :, :
					]

				output = model(fields_batch)

				if config.normalize:
					# normalized loss computation
					kappa2_output_norm = (output[:, 0] - kappa2_mean) / (
						kappa2_std + eps
					)
					theta_output_norm = (output[:, 1] - theta_mean) / (theta_std + eps)
					rho_output_norm = (output[:, 2] - rho_mean) / (rho_std + eps)

					kappa2_norm = (params_batch[:, 0] - kappa2_mean) / (
						kappa2_std + eps
					)
					theta_norm = (params_batch[:, 1] - theta_mean) / (theta_std + eps)
					rho_norm = (params_batch[:, 2] - rho_mean) / (rho_std + eps)

					kappa2_loss = criterion(kappa2_output_norm, kappa2_norm)
					theta_loss = criterion(theta_output_norm, theta_norm)
					rho_loss = criterion(rho_output_norm, rho_norm)
					loss = kappa2_loss + theta_loss + rho_loss

				else:
					kappa2_loss = criterion(output[:, 0], params_batch[:, 0])
					theta_loss = criterion(output[:, 1], params_batch[:, 1])
					rho_loss = criterion(output[:, 2], params_batch[:, 2])
					loss = kappa2_loss + theta_loss + rho_loss

				val_loss_current += loss.item() * fields_batch.size(0)

		val_loss = val_loss_current / len(config.val_df)
		val_losses.append(val_loss)

		current_lr = scheduler.get_last_lr()[0]  # optimizer.param_groups[0]["lr"]
		if config.verbose:
			print(f"Epoch {epoch + 1}/{config.n_epochs}")
			print(f"Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f}")
			print(f"Learning Rate: {current_lr:.6f}")

		# learning rate scheduler step
		scheduler.step(val_loss)

		# check for the best validation loss and save the best model weights
		if val_loss < best_val_loss:
			best_val_loss = val_loss
			best_model_wts = model.state_dict()
			early_stop_counter = 0
		else:
			early_stop_counter += 1

		# early stopping conditional
		if early_stop_counter >= patience:
			if config.verbose:
				print(
					f"Early stopping at epoch {epoch + 1}. Best validation loss: {best_val_loss:.6f}"
				)
			break

	# save different models weights in different locations based on modeltype string
	if config.save:
		torch.save(model.state_dict(), config.save_directory + config.savename)

	# load the best weights after training
	model.load_state_dict(best_model_wts)
	print("Training complete. Best model weights loaded.")

	return {
		"model": model,
		"train_losses": train_losses,
		"val_losses": val_losses,
	}

``````{ end_of_file="latticevision/cnn/train.py" }

``````{ path="latticevision/img2img/models/__init__.py"  }

``````{ end_of_file="latticevision/img2img/models/__init__.py" }

``````{ path="latticevision/img2img/models/stun.py"  }
import torch.nn as nn

from latticevision.img2img.base import (
	DecoderBlock,
	EncoderBlock,
	PatchEmbeddingConv,
	TransformerEncoder,
	ModelConfig,
)


class TransUNet(nn.Module):
	"""
	TransUNet model for spatial image-to-image tasks.
	The model uses convolutional layers for encoding and decoding,
	with a vision transformer in place of the bottleneck.
	Hybrid between UNet and ViT.

	Args:
		config: ModelConfig
			Configuration object for the model, see ModelConfig for more details.

	Methods:
		forward(x: torch.Tensor):
			Forward pass through the model.
	"""

	def __init__(
		self,
		config: ModelConfig,
	):
		super(TransUNet, self).__init__()
		self.config = config

		# encoder blocks
		in_ch = config.in_channels
		self.encoders = nn.ModuleList()
		for i, out_ch in enumerate(config.enc_block_channels):
			self.encoders.append(
				EncoderBlock(
					in_ch, out_ch, num_groups=config.group_norm_groups[i], config=config
				)
			)
			in_ch = out_ch

		# The "bottleneck" is a vit
		# create patch embedding for transformer with these specified patches.
		self.patch_size_h = config.patch_size_h
		self.patch_size_w = config.patch_size_w
		self.patch_embedding = PatchEmbeddingConv(
			config=config,
			patch_channels=in_ch,  # changed from embed_dim to in_ch
		)
		# transformer
		self.transformer: nn.Sequential = nn.Sequential(
			*[TransformerEncoder(config) for _ in range(config.num_layers)]
		)
		# de-embedding
		self.de_embedding = nn.Linear(
			config.embed_dim,
			config.embed_dim * config.patch_size_h * config.patch_size_w,
		)

		# decoder blocks
		in_ch = config.embed_dim
		dec_block_channels = config.enc_block_channels[::-1]
		self.decoders = nn.ModuleList()
		for i, out_ch in enumerate(dec_block_channels):
			self.decoders.append(
				DecoderBlock(
					in_ch,
					out_ch,
					out_ch,
					num_groups=config.group_norm_groups[-(i + 1)],
					config=config,
				)
			)
			in_ch = out_ch

		self.final_conv = nn.Conv2d(in_ch, config.out_channels, kernel_size=1)

	def forward(self, x):
		skip_connections = []

		# encoder
		for encoder in self.encoders:
			x_conv, x = encoder(x)
			skip_connections.append(x_conv)

		# x has shape: (B, last enc channel, H/16, W/16) assuming original input divisible by 16
		B, _, H_enc, W_enc = x.shape
		# make sure H_enc % num_patches_h == 0 and W_enc % num_patches_w == 0
		assert H_enc % self.patch_size_h == 0 and W_enc % self.patch_size_w == 0, (
			"Encoded feature map dimensions must be divisible by the specified number of patches."
		)

		# transformer
		# perform patching, which takes dim from last enc channel to embed dim
		x, (H_patches, W_patches) = self.patch_embedding(x)
		# num_patches = H_patches * W_patches

		# transformer layers
		x = self.transformer(x)

		# de-embedding
		x = self.de_embedding(x)

		# new reshaping, use embed dim instead of last enc channel
		x = x.view(
			B,
			H_patches,
			W_patches,
			self.config.embed_dim,
			self.patch_size_h,
			self.patch_size_w,
		)
		x = x.permute(0, 3, 1, 4, 2, 5).contiguous()
		x = x.view(
			B,
			self.config.embed_dim,
			H_patches * self.patch_size_h,
			W_patches * self.patch_size_w,
		)

		# decoder
		for decoder in self.decoders:
			x = decoder(x, skip_connections.pop())

		# final convolution of output
		out = self.final_conv(x)
		return out

``````{ end_of_file="latticevision/img2img/models/stun.py" }

``````{ path="latticevision/img2img/models/unet.py"  }
import torch.nn as nn

from latticevision.img2img.base import DecoderBlock, EncoderBlock, ModelConfig


class UNet(nn.Module):
	"""
	U-Net model for spatial image-to-image tasks.
	The model uses convolutional layers for encoding and decoding,
	with skip connections. Bottleneck has same number of channels
	as the transformer embedding dimension.

	Args:
	    config: ModelConfig
	        Configuration object for the model, see ModelConfig for more details.

	Methods:
		forward(x: torch.Tensor):
			Forward pass through the model.
	"""

	def __init__(
		self,
		config: ModelConfig,
	):
		super(UNet, self).__init__()
		self.config = config

		in_ch = config.in_channels
		self.encoders = nn.ModuleList()
		for i, out_ch in enumerate(config.enc_block_channels):
			self.encoders.append(
				EncoderBlock(
					in_ch, out_ch, num_groups=config.group_norm_groups[i], config=config
				)
			)
			in_ch = out_ch

		self.bottleneck = nn.Sequential(
			nn.Conv2d(
				config.enc_block_channels[-1],
				config.embed_dim,
				kernel_size=config.kernel_size,
				padding=config.padding,
			),
			nn.GroupNorm(
				num_groups=(config.embed_dim // 8), num_channels=config.embed_dim
			),
			nn.GELU(),
			nn.Conv2d(
				config.embed_dim,
				config.embed_dim,
				kernel_size=config.kernel_size,
				padding=config.padding,
			),
			nn.GroupNorm(
				num_groups=(config.embed_dim // 8), num_channels=config.embed_dim
			),
			nn.GELU(),
		)

		in_ch = config.embed_dim
		dec_block_channels = config.enc_block_channels[::-1]

		self.decoders = nn.ModuleList()
		for i, out_ch in enumerate(dec_block_channels):
			self.decoders.append(
				DecoderBlock(
					in_ch,
					out_ch,
					out_ch,
					num_groups=config.group_norm_groups[-(i + 1)],
					config=config,
				)
			)
			in_ch = out_ch

		self.final_conv = nn.Conv2d(in_ch, config.out_channels, kernel_size=1)

	def forward(self, x):
		skip_connections = []

		# encoder blocks
		for encoder in self.encoders:
			x_conv, x = encoder(x)
			skip_connections.append(x_conv)

		# bottleneck (same size as transformer embedding)
		x = self.bottleneck(x)

		# decoder blocks
		for decoder in self.decoders:
			x = decoder(x, skip_connections.pop())

		# final convolution for output
		out = self.final_conv(x)

		return out

``````{ end_of_file="latticevision/img2img/models/unet.py" }

``````{ path="latticevision/img2img/models/vit.py"  }
import torch.nn as nn

from latticevision.img2img.base import (
	PatchEmbeddingConv,
	TransformerEncoder,
	ModelConfig,
)


class ViT(nn.Module):
	"""
	A Vision Transformer that uses the specified patch sizes for splitting the input,
	then applies a series of Transformer encoder layers and reconstructs the output.

	Args:
		config: ModelConfig
			Configuration object for the model, see ModelConfig for more details.

	Methods:
		forward(x: torch.Tensor):
			Forward pass through the model.
	"""

	def __init__(
		self,
		config: ModelConfig,
	):
		super().__init__()
		# store config
		self.config: ModelConfig = config

		self.patch_embedding: PatchEmbeddingConv = PatchEmbeddingConv(
			config=config,
			patch_channels=config.in_channels,
		)

		# transformer encoder (post norms, with dropout and GELU)
		self.transformer: nn.Sequential = nn.Sequential(
			*[TransformerEncoder(config) for _ in range(config.num_layers)]
		)

		# takes a single embedding vector to a patch
		self.de_embedding: nn.Linear = nn.Linear(
			config.embed_dim,
			config.patch_size_h * config.patch_size_w * config.out_channels,
		)

	def forward(self, x):
		B, C, H, W = x.shape
		# perform patch and positional embedding
		x, (H_patches, W_patches) = self.patch_embedding(x)

		# transformer layers
		x = self.transformer(x)

		# de-embed patches back to single channel spatial dims
		x = self.de_embedding(x)

		# reshaping to correct image size
		x = x.view(
			B,
			H_patches,
			W_patches,
			self.config.patch_size_h,
			self.config.patch_size_w,
			self.config.out_channels,
		)
		x = x.permute(0, 5, 1, 3, 2, 4).reshape(B, self.config.out_channels, H, W)
		# old reshaping (keep just in case)
		# x = x.view(B, H_patches, W_patches, self.config.patch_size_h, self.config.patch_size_w).permute(0, 3, 1, 4, 2).reshape(B, -1, H, W)
		return x

``````{ end_of_file="latticevision/img2img/models/vit.py" }

``````{ path="latticevision/img2img/__init__.py"  }
from latticevision.img2img.models.stun import TransUNet
from latticevision.img2img.models.unet import UNet
from latticevision.img2img.models.vit import ViT

__all__ = [
	# model classes
	"TransUNet",
	"UNet",
	"ViT",
	# modules
	"models",
	"base",
	"eval",
	"train",
	"dataset",
]

``````{ end_of_file="latticevision/img2img/__init__.py" }

``````{ path="latticevision/img2img/base.py"  }
import math
import torch
import torch.nn as nn

from torchtune.modules import MultiHeadAttention as TT_MultiHeadAttention
from torchtune.modules import RotaryPositionalEmbeddings

from dataclasses import dataclass
from typing import Tuple, List


# abstract base class for positional embeddings
class PosEmbed(nn.Module):
	"""This class provides a structure for pos embedding classes to inherit from"""

	at_patch_embed: bool

	def __init__(self, embed_dim: int, max_n: int):
		super().__init__()
		# raise NotImplementedError(
		# 	"this is an abstract class, use something that inherits from it,
		#  like `NullPosEmbed`, `LearnedPosEmbed`, `SinusoidalPosEmbed`, or `RotaryPosEmbed`."
		# )


class NullPosEmbed(PosEmbed):
	"""Base class for testing without positional embeddings"""

	at_patch_embed: bool = True

	def __init__(self, embed_dim: int, max_n: int):
		super().__init__(embed_dim, max_n)
		self.embed_dim = embed_dim

	def forward(self, n: int) -> torch.Tensor:
		# Return a zero tensor of shape [embed_dim, n]
		return torch.zeros(self.embed_dim, n)


class LearnedPosEmbed(PosEmbed):
	"""Learnable positional embeddings"""

	at_patch_embed: bool = True

	def __init__(self, embed_dim: int, max_n: int):
		super().__init__(embed_dim, max_n)
		# essentially just an additional, learned layer
		self.pos_embed: nn.Embedding = nn.Embedding(max_n, embed_dim)

	def forward(self, n: int) -> torch.Tensor:
		# print(self.pos_embed.weight.device)
		return self.pos_embed(
			torch.arange(n, device=self.pos_embed.weight.device)
		).T  # transpose to shape (embed_dim, n)


class SinusoidalPosEmbed(PosEmbed):
	"""Sinusoidal positional embeddings"""

	at_patch_embed: bool = True

	def __init__(self, embed_dim: int, max_n: int):
		super().__init__(embed_dim, max_n)
		self.embed_dim: int = embed_dim
		self.max_n: int = max_n

	def forward(self, n: int):
		# generate the fixed positional encodings
		pos = torch.arange(n, dtype=torch.float32).unsqueeze(1)
		div_term = torch.exp(
			torch.arange(0, self.embed_dim, 2) * (-math.log(10000.0) / self.embed_dim)
		)
		pos_encoding = torch.zeros(n, self.embed_dim)
		# apply sin to even inds, cos to odd inds
		pos_encoding[:, 0::2] = torch.sin(pos * div_term)
		pos_encoding[:, 1::2] = torch.cos(pos * div_term)
		return pos_encoding.T  # transpose to shape (embed_dim, n)


class RotaryPosEmbed(PosEmbed):
	"""Rotary positional embeddings, dependent on torchtune module"""

	at_patch_embed: bool = False

	def __init__(self, embed_dim: int, max_n: int):
		super().__init__(embed_dim, max_n)
		self.rotary_pos_embed: RotaryPositionalEmbeddings = RotaryPositionalEmbeddings(
			dim=embed_dim,
			max_seq_len=max_n,
		)

	# forward method allows for more flexibility with arguments passed
	def forward(self, x: torch.Tensor, *args, **kwargs):
		return self.rotary_pos_embed(x, *args, **kwargs)


@dataclass(kw_only=True)
class ModelConfig:
	"""
	Model configuration for image-to-image tasks.

	Args:
	    in_channels: int
	        Number of input channels.
	    out_channels: int
	        Number of output channels.
	    embed_dim: int
	        Dimension of the embedding vectors.
	    enc_block_channels: List[int]
	        Number of channels in each convolutional encoder block.
	    dec_block_channels: List[int]
	        Number of channels in each convolutional decoder block.
	    group_norm_groups: List[int]
	        Number of groups for GroupNorm in each block.
	    kernel_size: Tuple[int, int]
	        Size of the convolutional kernel.
	    stride: Tuple[int, int]
	        Stride of the convolutional kernel.
	    padding: int
	        Padding for the convolutional kernel.
	    pool_kernel_size: Tuple[int, int]
	        Size of the pooling kernel.
	    pool_stride: Tuple[int, int]
	        Stride of the pooling kernel.
	    patch_size_h: int
	        Height of the patch size.
	    patch_size_w: int
	        Width of the patch size.
	    num_heads: int
	        Number of attention heads.
	    mlp_dim: int
	        Dimension of the feedforward network.
	    num_layers: int
	        Number of transformer layers.
	    dropout: float
	        Dropout rate.
	    patch_conv_bias: bool
	        Whether to include bias in the patch convolutional layer.
	    pos_embed_max_n_axis: int
	        Maximum number of patches in the x and y axes.
	    pos_embed_cls: type[PosEmbed]
	        Class for positional embeddings.
	    head_bias: bool
	        Whether to include bias in the attention heads.

		Properties:
			padding: int
				Calculates the padding for the convolutional kernel.
			patch_size_tup: tuple[int, int]
				Returns the patch size as a tuple.
			head_dim: int
				Calculates the dimension of the attention heads
	"""

	# General settings -----
	in_channels: int = 30
	out_channels: int = 3
	embed_dim: int = 768

	# Convolutional settings -----
	enc_block_channels: List[int] = (64, 128, 256, 512)
	# dec_block_channels: List[int] = (512, 256, 128, 64)block
	group_norm_groups: List[int] = (8, 16, 32, 64)
	kernel_size: Tuple[int, int] = (3, 3)
	stride: Tuple[int, int] = (1, 1)

	# for same padding use kernel_size[0]//2
	@property
	def padding(self) -> int:
		return self.kernel_size[0] // 2

	pool_kernel_size: Tuple[int, int] = (2, 2)
	pool_stride: Tuple[int, int] = (2, 2)

	# Transformer settings -----
	# set patch sizes each time for diff models
	# default is None so UNet can use the config
	patch_size_h: int = None
	patch_size_w: int = None
	num_heads: int = 12
	mlp_dim: int = 3072
	num_layers: int = 12
	dropout: float = 0.1
	patch_conv_bias: bool = False
	pos_embed_max_n_axis: int = 64
	# default to learned embeddings
	pos_embed_cls: type[PosEmbed] = LearnedPosEmbed
	head_bias: bool = False

	@property
	def patch_size_tup(self) -> tuple[int, int]:
		return (self.patch_size_h, self.patch_size_w)

	@property
	def head_dim(self) -> int:
		return self.embed_dim // self.num_heads


class EncoderBlock(nn.Module):
	"""
	Convolutional encoder block with pooling for downsampling. Used to progressively
	reduce spatial dimensions while increasing feature representation.

	Args:
	    in_channels: int
	        Number of input channels.
	    out_channels: int
	        Number of output channels.
		num_groups: int
			Number of groups for GroupNorm.

	Methods:
	    forward(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
	        Applies 2*(convolution, groupnorm, GELU, and dropout), followed by a max pooling layer.
	        Returns the transformed feature map and the pooled feature map for skip connections.
	"""

	def __init__(self, in_channels, out_channels, num_groups, config: ModelConfig):
		super(EncoderBlock, self).__init__()
		self.conv = nn.Sequential(
			nn.Conv2d(
				in_channels,
				out_channels,
				kernel_size=config.kernel_size,
				padding=config.padding,
			),
			# GroupNorm is not dependent on batch size, also good balance between layernorm and instancenorm
			nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),
			# GELU for smoother gradients
			nn.GELU(),
			# nn.Dropout2d(p=0.1),
			nn.Conv2d(
				out_channels,
				out_channels,
				kernel_size=config.kernel_size,
				padding=config.padding,
			),
			nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),
			nn.GELU(),
			# nn.Dropout2d(p=0.1),
		)
		# pooling layer halves the spatial dimensions
		self.pool = nn.MaxPool2d(
			kernel_size=config.pool_kernel_size, stride=config.pool_stride
		)

	def forward(self, x):
		x = self.conv(x)
		x_pooled = self.pool(x)
		# return both for skip connections
		return x, x_pooled


class DecoderBlock(nn.Module):
	"""
	Convolutional decoder block with transposed convolution for upsampling and skip connections.
	Used to progressively restore spatial dimensions and combine extracted spatial features.

	Args:
	    in_channels: int
	        Number of input channels.
	    skip_channels: int
	        Number of channels from skip connection.
	    out_channels: int
	        Number of output channels.
		num_groups: int
			Number of groups for GroupNorm.

	Methods:
	    forward(x: torch.Tensor, skip_x: torch.Tensor) -> torch.Tensor
	        Upsamples the input and concatenates it with the skip connection feature map, then applies
	        convolutions to refine the output. Returns the upsampled feature map.
	    Essentially symmetric to the encoder block after upsampling: 2* (convs, groupnorm, gelu, dropout).
	"""

	def __init__(
		self, in_channels, skip_channels, out_channels, num_groups, config: ModelConfig
	):
		super(DecoderBlock, self).__init__()

		# upsampling layer doubles spatial dimensions
		self.upconv = nn.ConvTranspose2d(
			in_channels,
			out_channels,
			kernel_size=config.pool_kernel_size,
			stride=config.pool_stride,
		)

		# convolutions symmetric to encoder block
		self.conv = nn.Sequential(
			nn.Conv2d(
				out_channels + skip_channels,
				out_channels,
				kernel_size=config.kernel_size,
				padding=config.padding,
			),
			nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),
			nn.GELU(),
			# nn.Dropout2d(p=0.1),
			nn.Conv2d(
				out_channels,
				out_channels,
				kernel_size=config.kernel_size,
				padding=config.padding,
			),
			nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),
			nn.GELU(),
			# nn.Dropout2d(p=0.1),
		)

	def forward(self, x, skip_x):
		x = self.upconv(x)
		# concatenate with skip connection
		x = torch.cat((x, skip_x), dim=1)
		x = self.conv(x)
		return x


class PatchEmbeddingConv(nn.Module):
	"""
	Patch embedding layer that uses a convolutional layer to extract patches
	and project each patch into an embedding vector.

	Args:
		config: ModelConfig
			Configuration object.
		patch_channels: int
			Number of channels in the patch. May change whether this is directly
			taking in data (ViT) or is deeper inside a model (TransUNet).

	Methods:
		forward(x: torch.Tensor) -> torch.Tensor
			Applies a convolutional layer to extract patches and project them into embedding space.
			Optionally adds positional embeddings.
	"""

	def __init__(self, config: ModelConfig, patch_channels: int):
		super().__init__()
		self.config: ModelConfig = config
		self.patch_channels: int = patch_channels
		# conv layer to extract patches and project them into embedding space
		self.conv: nn.Conv2d = nn.Conv2d(
			in_channels=self.patch_channels,
			out_channels=config.embed_dim,
			kernel_size=config.patch_size_tup,
			stride=config.patch_size_tup,
			bias=config.patch_conv_bias,
		)

		# if pos embeddings are made in patching
		# then create them here for both x and y dimensions
		if config.pos_embed_cls.at_patch_embed:
			self.pos_embed_x: PosEmbed = config.pos_embed_cls(
				config.embed_dim, config.pos_embed_max_n_axis
			)
			self.pos_embed_y: PosEmbed = config.pos_embed_cls(
				config.embed_dim, config.pos_embed_max_n_axis
			)

	def forward(self, x: torch.Tensor) -> torch.Tensor:
		"""returns (batch patches embed_dim)"""
		B, C, H, W = x.size()
		assert C == self.patch_channels, (
			f"Input channels mismatch: {C = } vs {self.patch_channels = }"
		)
		assert (
			H % self.config.patch_size_h == 0 and W % self.config.patch_size_w == 0
		), "Image dimensions must be divisible by the specified patch size."

		# apply conv layer
		# shape: (B, embed_dim, num_patches_h, num_patches_w)
		# Float[torch.Tensor, "batch embed_dim num_patches_h num_patches_w"]
		x = self.conv(x)

		B, embed_dim, num_patches_h, num_patches_w = x.size()
		assert embed_dim == self.config.embed_dim, (
			f"Embedding dimension mismatch: {embed_dim = } vs {self.config.embed_dim = }"
		)

		if self.config.pos_embed_cls.at_patch_embed:
			# create positional emebds
			# embed_dim num_patches_h
			pos_x = self.pos_embed_x(num_patches_w).to(x.device).unsqueeze(-2)
			# embed_dim, num_patches_w
			pos_y = self.pos_embed_y(num_patches_h).to(x.device).unsqueeze(-1)

			# print(f"{x.shape = }, {pos_x.shape = }, {pos_y.shape = }")
			# unsqueeze and add positional embeddings
			x = x + pos_x.unsqueeze(0) + pos_y.unsqueeze(0)

		# flatten along patch dim
		x = x.flatten(-2).transpose(
			-1, -2
		)  # shape: (B, num_patches = seq_length, embed_dim)

		return x, (num_patches_h, num_patches_w)


class DyT(nn.Module):
	"""
	Drop in replacement for Layernorm
	from new paper "Transformers without Normalization" by
	Zhu et al. Observes that layernorm outputs often look
	like tanh function, and this may be a faster alternative
	with similar performance.

	Args:
		C: int
			Number of channels.
		init_alpha: float
			Initial value for alpha parameter. Default set to
			0.5 as recommended in the paper.
	"""

	def __init__(self, C: int, init_alpha: float = 0.5):
		super(DyT, self).__init__()
		self.alpha = nn.Parameter(torch.ones(1) * init_alpha)
		self.gamma = nn.Parameter(torch.ones(C))
		self.beta = nn.Parameter(torch.zeros(C))

	def forward(self, x: torch.Tensor) -> torch.Tensor:
		x = torch.tanh(self.alpha * x)
		return self.gamma * x + self.beta


class TransformerEncoder(nn.Module):
	"""
	Transformer encoder block.

	Args:
		config: ModelConfig
			Configuration object.

	Methods:
		forward(x: torch.Tensor) -> torch.Tensor
			Applies multi-head self-attention and feedforward network to the input tensor.
			Returns the transformed tensor.
	"""

	def __init__(self, config: ModelConfig):
		super(TransformerEncoder, self).__init__()
		self.config: ModelConfig = config
		self.dropout: nn.Dropout = nn.Dropout(config.dropout)

		# initialize the positional embedding
		self.pos_embed: PosEmbed | None
		# first, figure out if we do the pos embed at patching
		is_positional_embedding_at_patching: bool = (
			self.config.pos_embed_cls.at_patch_embed
		)
		# if we arent doing pos embed at patching, that must mean we are doing it here
		# (and probably using RoPE)
		if not is_positional_embedding_at_patching:
			# create the positional embedding
			self.pos_embed = config.pos_embed_cls(
				config.head_dim,  # creates vectors of the same size as the embeddings
				config.pos_embed_max_n_axis
				** 2,  # max number of patches in the axis is ^2 for RoPE
			)
		else:
			# it will be `None` if we are doing the pos embed at patching and not here
			self.pos_embed = None

		# old attention (just in case, does not work with RoPE)
		# self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)

		# torchtune MHA used for RoPE
		self.attention: TT_MultiHeadAttention = TT_MultiHeadAttention(
			embed_dim=config.embed_dim,
			num_heads=config.num_heads,
			num_kv_heads=config.num_heads,  # for standard MHA, set kv heads to be same as heads
			attn_dropout=config.dropout,
			pos_embeddings=self.pos_embed,  # either RoPE or "None": happen at patching
			# initialize 'head_dim', 'q_proj', 'k_proj', 'v_proj', and 'output_proj'
			head_dim=config.head_dim,
			q_proj=nn.Linear(config.embed_dim, config.embed_dim, bias=config.head_bias),
			k_proj=nn.Linear(config.embed_dim, config.embed_dim, bias=config.head_bias),
			v_proj=nn.Linear(config.embed_dim, config.embed_dim, bias=config.head_bias),
			output_proj=nn.Linear(
				config.embed_dim, config.embed_dim, bias=config.head_bias
			),
			# batch_first=True,
		)
		self.norm1: nn.LayerNorm = nn.LayerNorm(config.embed_dim)

		self.mlp: nn.Sequential = nn.Sequential(
			nn.Linear(config.embed_dim, config.mlp_dim),
			nn.GELU(),
			nn.Linear(config.mlp_dim, config.embed_dim),
		)
		self.norm2: nn.LayerNorm = nn.LayerNorm(config.embed_dim)

	def forward(self, x):
		# attention
		attn_out = self.attention(x, x)
		x = x + self.dropout(attn_out)
		x = self.norm1(x)

		# mlp
		mlp_out = self.mlp(x)
		x = x + self.dropout(mlp_out)
		x = self.norm2(x)
		return x

``````{ end_of_file="latticevision/img2img/base.py" }

``````{ path="latticevision/img2img/dataset.py"  }
from typing import Callable
import h5py
import numpy as np
import torch
import sys
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from dataclasses import dataclass


def no_transform(params: np.ndarray) -> np.ndarray:
	return params


# experimenting with parameter transformations
def polar_transform(params: np.ndarray) -> np.ndarray:
	"""
	Transforms the input params according to Rai and Brown (2023).
	"""
	theta = params[:, 1, :, :]
	rho = params[:, 2, :, :]
	f1 = np.sqrt(rho - 1) * np.sin(2 * theta)
	f2 = np.sqrt(rho - 1) * np.cos(2 * theta)
	params[:, 1, :, :] = f1
	params[:, 2, :, :] = f2
	return params


@dataclass(kw_only=True)
class DataConfig:
	"""
	Config object for dataset.

	Attributes:
		file_path: str
			Path to the HDF5 file containing the data.
		n_rows: int
			Number of rows in the field images.
		n_cols: int
			Number of columns in the field images.
		n_replicates: int
			Number of field replicates per simulation.
		transform_function: Callable
			Function to apply to the data. Options are currently:
			polar repameterization.
		n_params: int
			Number of parameters in the dataset.
		log_kappa2: bool
			If True, log-transforms the first parameter (kappa2). Default is True.
		shift_theta: bool
			If True, shifts the theta parameter to be between pi/2 and -pi/2. Default is True.
		val_size: float
			Proportion of the full dataset to include in the validation set. Default is 0.2.
		test_size: float
			Proportion of the validation dataset to make into the test set. Default is 0.2.
		random_state: int
			Random seed for reproducibility. Default is 777.
		verbose: bool
			If True, prints dataset information. Default is True.
	"""

	file_path: str
	n_rows: int = 192
	n_cols: int = 288
	n_replicates: int
	transform_function: Callable = no_transform
	n_params: int = 3
	log_kappa2: bool = True
	shift_theta: bool = True
	val_size: float = 0.2
	test_size: float = 0.2
	random_state: int = 777
	verbose: bool = True


class ClimDataClass(Dataset):
	"""
	Custom PyTorch dataset for handling STUN model data, including fields, parameters, and configuration.

	Attributes:
	    fields: np.ndarray
	        Stores the input field data.
	    param_fields: np.ndarray
	        Stores the parameter field data.

	Methods:
	    __len__():
	        Returns the total number of samples in the dataset.
	    __getitem__(idx: int):
	        Retrieves a sample with its replicated fields and parameters at a given index as PyTorch tensors.
	"""

	def __init__(self, fields, param_fields):
		self.fields = fields
		self.param_fields = param_fields

	def __len__(self):
		return len(self.fields)

	def __getitem__(self, idx):
		field = torch.tensor(self.fields[idx], dtype=torch.float32)
		params = torch.tensor(self.param_fields[idx], dtype=torch.float32)
		return field, params


def make_dataset(config: DataConfig) -> dict:
	"""
	Loads and preprocesses img2img data from an HDF5 file,
	reshaping and splitting it into training, validation, and test sets.

	Args:
		config: DataConfig
			Configuration object for the dataset, see DataConfig for more details.

	Returns:
		dict: A dict containing:
			train_df: Dataset
				Training dataset for the img2img model.
			val_df: Dataset
				Validation dataset for the img2img model.
			test_df: Dataset
				Test dataset for the img2img model.
	"""

	with h5py.File(config.file_path, "r") as f:
		if config.verbose:
			print("Components in the file:", list(f.keys()))
		# extract fields
		fields = f["fields"][:]

		if config.verbose:
			print("Dataset size (MB): ", sys.getsizeof(fields) / 1024**2)
			print("Dataset size (GB): ", sys.getsizeof(fields) / 1024**3)

	# reshape into proper form and then extract params
	n_sims, n_fields, _ = fields.shape
	fields = fields.reshape(n_sims, n_fields, config.n_rows, config.n_cols)
	# take the last fields as the params
	params = fields[:, -config.n_params :, :, :]
	# start at the beginning for replicates
	fields = fields[:, 0 : config.n_replicates, :, :]

	if config.verbose:
		print("Fields shape: ", fields.shape)
		print("Params shape: ", params.shape)

	# log transform kappa2
	if config.log_kappa2:
		kappa2 = params[:, 0, :, :]
		params[:, 0, :, :] = np.log(kappa2)

	if config.shift_theta:
		theta = params[:, 1, :, :]
		params[:, 1, :, :] = np.pi / 2 - theta

	# apply transform function
	params = config.transform_function(params)

	# train-val-test split
	fields_train, fields_temp, params_train, params_temp = train_test_split(
		fields, params, test_size=config.val_size, random_state=config.random_state
	)
	fields_val, fields_test, params_val, params_test = train_test_split(
		fields_temp,
		params_temp,
		test_size=config.test_size,
		random_state=config.random_state,
	)

	if config.verbose:
		print("Train fields shape: ", fields_train.shape)
		print("Train params shape: ", params_train.shape)
		print("Validation fields shape: ", fields_val.shape)
		print("Validation params shape: ", params_val.shape)
		print("Test fields shape: ", fields_test.shape)
		print("Test params shape: ", params_test.shape)

	train_df = ClimDataClass(fields_train, params_train)
	val_df = ClimDataClass(fields_val, params_val)
	test_df = ClimDataClass(fields_test, params_test)
	# del fields, params, fields_train, params_train, fields_val, params_val, fields_test, params_test

	return {"train_df": train_df, "val_df": val_df, "test_df": test_df}

``````{ end_of_file="latticevision/img2img/dataset.py" }

``````{ path="latticevision/img2img/eval.py"  }
import matplotlib.pyplot as plt
import numpy as np
import torch
import pandas as pd
from typing import Optional


# from skimage.metrics import normalized_root_mse as nrmse
from skimage.metrics import (
	structural_similarity as ssim_metric,
	peak_signal_noise_ratio as psnr_metric,
)

from latticevision.img_augment import random_augment_clim
from latticevision.img2img.dataset import DataConfig, no_transform, polar_transform


def eval_model(
	model: torch.nn.Module,
	device: torch.device,
	config: DataConfig,
	test_loader: torch.utils.data.DataLoader,
	test_df: torch.utils.data.Dataset,
	plot: bool = True,
	augmentation: bool = False,
	show: bool = True,
	n_pixels: int = 5000,
	invert_transform: bool = False,
	cnn_mode: bool = False,
	cnn_results: Optional[torch.tensor] = None,
) -> dict:
	"""
	Evaluates the STUN model on the test set by computing a number of metrics
	for each parameter (kappa2, theta, rho) and optionally plots the predicted
	vs actual values (pixelwise). To make life easy, a `cnn_mode` has been
	integrated to plot the results from local CNN estimation also. In order to
	keep things simple with the tiling/local estimation functions, the CNN results
	need to provided ahead of time, rather than being computed inside the function.

	Metrics computed (for each parameter):
	    - RMSE (Root Mean Squared Error)
	    - MAE (Mean Absolute Error)
	    - R² (Coefficient of Determination)
	    - SSIM (Structural Similarity Index)
	    - PSNR (Peak Signal-to-Noise Ratio)
	    - NRMSE (Normalized RMSE)

	Args:
	    model (nn.Module): The trained model.
		config (DataConfig): Configuration for the dataset.
	    device (torch.device): Device to perform computations on.
	    test_loader (DataLoader): DataLoader for the test set.
	    test_df (Dataset): Test dataset (used only for computing the number of test samples).
	    plot (bool, optional): If True, plots predicted vs actual graphs. Defaults to True.
	    augmentation (bool, optional): If True, applies augmentation. Defaults to False.
		show (bool, optional): If True, shows the plot. Defaults to True.
		n_pixels (int, optional): Number of pixels to sample for the predicted vs actual plot. Defaults to 5000.
		invert_transform (bool, optional): If True, applies the inverse transform to the predicted values. Defaults to False.
		cnn_mode (bool, optional): If True, uses CNN data. Defaults to False.
		cnn_results (Optional[torch.tensor], optional): The CNN results to use for plotting. Defaults to None.

	Returns:
	    dict: Dictionary containing computed metrics for each parameter.
	"""

	model.eval()

	# create dicts for predictions and true values for each param.
	preds = {"kappa2": [], "theta": [], "rho": []}
	truths = {"kappa2": [], "theta": [], "rho": []}

	if cnn_mode == False:
		with torch.no_grad():
			for fields_batch, params_batch in test_loader:
				if augmentation:
					fields_batch, params_batch = random_augment_clim(
						fields_batch, params_batch
					)

				fields_batch = fields_batch.to(device)
				params_batch = params_batch.to(device)

				# pass through model
				outputs = model(fields_batch)

				# seaparate params (channel dimension)
				preds["kappa2"].append(outputs[:, 0, :, :].cpu().numpy())
				preds["theta"].append(outputs[:, 1, :, :].cpu().numpy())
				preds["rho"].append(outputs[:, 2, :, :].cpu().numpy())

				truths["kappa2"].append(params_batch[:, 0, :, :].cpu().numpy())
				truths["theta"].append(params_batch[:, 1, :, :].cpu().numpy())
				truths["rho"].append(params_batch[:, 2, :, :].cpu().numpy())
	else:
		outputs = cnn_results
		params_batch = test_df[:][1]
		preds["kappa2"].append(outputs[:, 0, :, :].cpu().numpy())
		preds["theta"].append(outputs[:, 1, :, :].cpu().numpy())
		preds["rho"].append(outputs[:, 2, :, :].cpu().numpy())

		truths["kappa2"].append(params_batch[:, 0, :, :].cpu().numpy())
		truths["theta"].append(params_batch[:, 1, :, :].cpu().numpy())
		truths["rho"].append(params_batch[:, 2, :, :].cpu().numpy())

	# concatenate predictions/ground-truth along the batch dimension
	for key in preds.keys():
		preds[key] = np.concatenate(preds[key], axis=0)  # shape: (N, 192, 288)
		truths[key] = np.concatenate(truths[key], axis=0)

	# apply transform to recover awght param (more familiar for LK)
	preds_awght = np.exp(preds["kappa2"]) + 4
	truths_awght = np.exp(truths["kappa2"]) + 4
	if invert_transform:
		if config.transform_function == polar_transform:
			theta = preds["theta"].copy()
			rho = preds["rho"].copy()

			preds["theta"] = np.arctan(theta / rho) / 2
			preds["rho"] = np.square(theta) + np.square(rho) + 1

	# helper function to compute metrics
	def compute_metrics(true: np.ndarray, pred: np.ndarray):
		"""
		Computes RMSE, MAE, R2, SSIM, PSNR, and NRMSE between two arrays.
		Assumes true and pred have shape (num_images, height, width).
		"""
		eps = 1e-6
		mse = np.mean((true - pred) ** 2)
		rmse = np.sqrt(mse)
		mae = np.mean(np.abs(true - pred))
		r2 = 1 - np.sum((true - pred) ** 2) / np.sum((true - np.mean(true)) ** 2 + eps)
		nrmse = rmse / (np.max(true) - np.min(true) + eps)

		# compute SSIM and PSNR for each image and then average
		ssim_list = []
		psnr_list = []
		for i in range(true.shape[0]):
			# define data range for each image (avoid division by zero)
			data_range = np.max(true[i]) - np.min(true[i])
			if data_range == 0:
				data_range = 1.0
			ssim_val = ssim_metric(true[i], pred[i], data_range=data_range)
			psnr_val = psnr_metric(true[i], pred[i], data_range=data_range)
			ssim_list.append(ssim_val)
			psnr_list.append(psnr_val)
		ssim_avg = np.mean(ssim_list)
		psnr_avg = np.mean(psnr_list)

		return rmse, mae, r2, ssim_avg, psnr_avg, nrmse

	# compute metrics for each param
	# we dont use this variable yet, going to compute it though for tests
	awght_metrics = compute_metrics(truths_awght, preds_awght)  # noqa: F841
	kappa2_metrics = compute_metrics(truths["kappa2"], preds["kappa2"])
	theta_metrics = compute_metrics(truths["theta"], preds["theta"])
	rho_metrics = compute_metrics(truths["rho"], preds["rho"])

	# put metrics into dictionary
	metrics = {
		"kappa2": {
			"rmse": kappa2_metrics[0],
			"mae": kappa2_metrics[1],
			"r2": kappa2_metrics[2],
			"ssim": kappa2_metrics[3],
			"psnr": kappa2_metrics[4],
			"nrmse": kappa2_metrics[5],
		},
		"theta": {
			"rmse": theta_metrics[0],
			"mae": theta_metrics[1],
			"r2": theta_metrics[2],
			"ssim": theta_metrics[3],
			"psnr": theta_metrics[4],
			"nrmse": theta_metrics[5],
		},
		"rho": {
			"rmse": rho_metrics[0],
			"mae": rho_metrics[1],
			"r2": rho_metrics[2],
			"ssim": rho_metrics[3],
			"psnr": rho_metrics[4],
			"nrmse": rho_metrics[5],
		},
	}

	# plotting (optional)
	if plot:
		fig, axes = plt.subplots(2, 2, figsize=(10, 7.5))

		# --- Kappa2 predicted vs. actual plot ---
		ax = axes[0, 0]
		true_vals = truths["kappa2"].flatten()
		pred_vals = preds["kappa2"].flatten()
		idx = np.random.choice(
			len(true_vals), size=min(n_pixels, len(true_vals)), replace=False
		)
		ax.scatter(
			true_vals[idx], pred_vals[idx], s=6, alpha=0.2, color="mediumslateblue"
		)
		min_val = min(true_vals.min(), pred_vals.min())
		max_val = max(true_vals.max(), pred_vals.max())
		ax.plot([min_val, max_val], [min_val, max_val], "k--")
		ax.set_title(r"$log(\kappa^2)$ Predicted vs Actual")
		ax.set_xlabel("Actual")
		ax.set_ylabel("Predicted")

		# --- Awght predicted vs. actual plot ---
		ax = axes[0, 1]
		true_vals = truths_awght.flatten()
		pred_vals = preds_awght.flatten()
		idx = np.random.choice(
			len(true_vals), size=min(n_pixels, len(true_vals)), replace=False
		)
		ax.scatter(true_vals[idx], pred_vals[idx], s=6, alpha=0.2, color="orange")
		min_val = min(true_vals.min(), pred_vals.min())
		max_val = max(true_vals.max(), pred_vals.max())
		ax.plot([min_val, max_val], [min_val, max_val], "k--")
		ax.set_title("awght Predicted vs Actual")
		ax.set_xlabel("Actual")
		ax.set_ylabel("Predicted")

		# --- theta predicted vs. actual plot ---
		ax = axes[1, 0]
		true_vals = truths["theta"].flatten()
		pred_vals = preds["theta"].flatten()
		idx = np.random.choice(
			len(true_vals), size=min(n_pixels, len(true_vals)), replace=False
		)
		ax.scatter(true_vals[idx], pred_vals[idx], s=6, alpha=0.2, color="deeppink")
		min_val = min(true_vals.min(), pred_vals.min())
		max_val = max(true_vals.max(), pred_vals.max())
		ax.plot([min_val, max_val], [min_val, max_val], "k--")
		if config.transform_function == no_transform:
			ax.set_title(r"$\theta$ Predicted vs Actual")
		elif config.transform_function == polar_transform:
			ax.set_title(r"$f_1$ Predicted vs Actual")
			if invert_transform:
				ax.set_title(r"$\theta$ Predicted vs Actual")
		ax.set_xlabel("Actual")
		ax.set_ylabel("Predicted")

		# --- rho predicted vs. actual plot ---
		ax = axes[1, 1]
		true_vals = truths["rho"].flatten()
		pred_vals = preds["rho"].flatten()
		idx = np.random.choice(
			len(true_vals), size=min(n_pixels, len(true_vals)), replace=False
		)
		ax.scatter(
			true_vals[idx], pred_vals[idx], s=6, alpha=0.2, color="darkturquoise"
		)
		min_val = min(true_vals.min(), pred_vals.min())
		max_val = max(true_vals.max(), pred_vals.max())
		ax.plot([min_val, max_val], [min_val, max_val], "k--")
		if config.transform_function == no_transform:
			ax.set_title(r"$\rho$ Predicted vs Actual")
		elif config.transform_function == polar_transform:
			ax.set_title(r"$f_2$ Predicted vs Actual")
			if invert_transform:
				ax.set_title(r"$\rho$ Predicted vs Actual")
		ax.set_xlabel("Actual")
		ax.set_ylabel("Predicted")

		plt.tight_layout()

		if show:
			plt.show()
		else:
			plt.close(fig)

	rows = []
	for param, metric_vals in metrics.items():
		row = {"parameter": param}
		row.update(metric_vals)
		rows.append(row)
	df_metrics = pd.DataFrame(rows).set_index("parameter")

	# print the dataframe
	print(df_metrics)

	return df_metrics

	# print the metrics regardless of plot or not
	# print("Evaluation Metrics:")
	# for param, m in metrics.items():
	# 	print(f"{param}:")
	# 	for met_name, value in m.items():
	# 		print(f"  {met_name}: {value:.4f}")

	# return metrics

``````{ end_of_file="latticevision/img2img/eval.py" }

``````{ path="latticevision/img2img/train.py"  }
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from dataclasses import dataclass

# from torchvision import models
from torch.utils.data import DataLoader, Dataset

from latticevision.img_augment import random_augment_clim


@dataclass(kw_only=True)
class TrainingConfig:
	"""
	Config object for training img2img models.

	Attributes:
	    model: nn.Module
	        The neural network model to be trained (e.g., STUN or CNN).
	    device: torch.device
	        The device on which computations are performed (e.g., "cuda" or "cpu").
	    train_loader: DataLoader
	        DataLoader for training data.
	    val_loader: DataLoader
	        DataLoader for validation data.
	    train_df: Dataset
	        The training dataset, used to calculate the training loss.
	    val_df: Dataset
	        The validation dataset, used to calculate the validation loss.
	    lr: float, optional
	        The learning rate for the optimizer. Default is 5e-5.
	    n_epochs: int, optional
	        Maximum number of epochs for training. Default is 100.
	    stop_patience: int, optional
	        Number of epochs with no improvement to trigger early stopping. Default is 10.
	    scheduler_patience: int, optional
	        Number of epochs with no improvement to reduce learning rate. Default is 5.
	    scheduler_factor: float, optional
	        Factor to reduce learning rate when validation loss plateaus. Default is 0.5.
	    augmentation: bool, optional
	        If True, applies random augmentations to the both spatial and param fields. Default is False.
	    save: bool, optional
	        If True, saves the best model weights. Default is True.
		save_directory: str, optional
			Directory to save the model weights. Default is "../results/model_wghts/".
		savename: str, optional
			Name of the file to save the model weights. Default is "stun_wghts.pth".
	    verbose: bool, optional
	        If True, prints progress, including loss values and learning rate updates. Default is True.
		normalize: bool, optional
			If true, normalizes only for the loss computation (not the actual outputs). This regulates the
			magnitude of the params (otherwise |log(kappa2)| will dominate the loss). This is done
			in the loss computation only, so the model will still output the original values.
			Default is True.
		shuffle: bool, optional
			If true, shuffles the input fields for permutation invariance (soft constraint). Default is True.
	"""

	model: nn.Module
	device: torch.device
	train_loader: DataLoader
	val_loader: DataLoader
	train_df: Dataset
	val_df: Dataset
	lr: float = 5e-5
	n_epochs: int = 200
	stop_patience: int = 10
	scheduler_patience: int = 5
	scheduler_factor: float = 0.5
	augmentation: bool = False
	save: bool = True
	save_directory: str = "../results/model_wghts/"
	savename: str = "stun_wghts.pth"
	verbose: bool = True
	normalize: bool = True
	shuffle: bool = True


def train_model(config: TrainingConfig) -> dict:
	"""
	Trains a STUN model with early stopping and learning rate scheduling, returning the best model and loss histories.

	Args:
	    config (TrainingConfig):
			- Configuration object containing training parameters and settings.

	Returns:
	    dict: A dict containing:
	        - model: nn.Module
	            The trained model with the best validation weights loaded.
	        - train_losses: list
	            List of training loss values over epochs.
			- baseline_losses: list
				List of baseline loss values over epochs.
	        - val_losses: list
	            List of validation loss values over epochs.
	"""
	model = config.model

	if config.verbose:
		print(f"Training for {config.n_epochs} epochs with learning rate {config.lr}.")
		if config.save == True:
			print(
				"Model weights will be saved at "
				+ config.save_directory
				+ config.savename
			)
		if config.save == False:
			print("Model weights will not be saved.")
		if config.augmentation:
			print("Augmentation has been enabled.")
		if config.augmentation == False:
			print("Augmentation has been disabled.")
		if config.normalize:
			print("Normalization has been enabled.")
		if config.normalize == False:
			print("Normalization has been disabled.")
		if config.shuffle:
			print("Shuffling has been enabled.")
		if config.shuffle == False:
			print("Shuffling has been disabled.")

	# calculate mean and std across entire training set for normalization
	if config.normalize:
		kappa2_mean = torch.mean(config.train_df[:][1][:, 0, :, :])
		kappa2_std = torch.std(config.train_df[:][1][:, 0, :, :])

		theta_mean = torch.mean(config.train_df[:][1][:, 1, :, :])
		theta_std = torch.std(config.train_df[:][1][:, 1, :, :])

		rho_mean = torch.mean(config.train_df[:][1][:, 2, :, :])
		rho_std = torch.std(config.train_df[:][1][:, 2, :, :])

	# to avoid division by zero
	eps = 1e-6

	# hyperparameter settings
	criterion = nn.MSELoss()
	optimizer = optim.AdamW(model.parameters(), lr=config.lr)
	scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
		optimizer,
		mode="min",
		factor=config.scheduler_factor,
		patience=config.scheduler_patience,
		# verbose=True,
	)

	patience = config.stop_patience
	best_val_loss = float("inf")
	early_stop_counter = 0

	# items to return
	train_losses = []
	val_losses = []
	# baseline loss sees if the model learns something better
	# than just predicting the mean of the training data
	baseline_losses = []
	best_model_wts = model.state_dict()

	# training
	for epoch in range(config.n_epochs):
		model.train()
		train_loss_current = 0
		baseline_losses_current = 0

		# TRAINING LOOP ---------------------------------------------------------
		for batch in config.train_loader:
			fields_batch, params_batch = batch

			# augmentation happens half the time
			if config.augmentation:
				aug_choice = np.random.choice([True, False])
				if aug_choice:
					fields_batch, params_batch = random_augment_clim(
						fields_batch, params_batch
					)

			fields_batch, params_batch = (
				fields_batch.to(config.device),
				params_batch.to(config.device),
			)

			# shufflying for permutation invariance (soft constraint)
			if config.shuffle:
				fields_batch = fields_batch[
					:, torch.randperm(fields_batch.size(1)), :, :
				]

			optimizer.zero_grad()

			# print(f"{fields_batch.shape = }")
			output = model(fields_batch)

			if config.normalize:
				# normalized loss computation
				kappa2_norm = (params_batch[:, 0, :, :] - kappa2_mean) / kappa2_std
				theta_norm = (params_batch[:, 1, :, :] - theta_mean) / theta_std
				rho_norm = (params_batch[:, 2, :, :] - rho_mean) / rho_std
				params_batch_norm = torch.stack(
					[kappa2_norm, theta_norm, rho_norm], dim=1
				)

				kappa2_output_norm = (output[:, 0, :, :] - kappa2_mean) / kappa2_std
				theta_output_norm = (output[:, 1, :, :] - theta_mean) / theta_std
				rho_output_norm = (output[:, 2, :, :] - rho_mean) / rho_std
				output_norm = torch.stack(
					[kappa2_output_norm, theta_output_norm, rho_output_norm], dim=1
				)

				kappa2_loss = criterion(kappa2_output_norm, kappa2_norm)
				theta_loss = criterion(theta_output_norm, theta_norm)
				rho_loss = criterion(rho_output_norm, rho_norm)
				loss = kappa2_loss + theta_loss + rho_loss

				# compute baseline mean loss
				mean_params = torch.mean(params_batch_norm, dim=(2, 3), keepdim=True)
				mean_params = mean_params.expand_as(params_batch_norm)
				mean_loss = criterion(mean_params, params_batch_norm)

			else:
				kappa2_loss = criterion(output[:, 0, :, :], params_batch[:, 0, :, :])
				theta_loss = criterion(output[:, 1, :, :], params_batch[:, 1, :, :])
				rho_loss = criterion(output[:, 2, :, :], params_batch[:, 2, :, :])
				loss = kappa2_loss + theta_loss + rho_loss

				# compute baseline mean loss
				mean_params = torch.mean(params_batch, dim=(2, 3), keepdim=True)
				mean_params = mean_params.expand_as(params_batch)
				mean_loss = criterion(mean_params, params_batch)

			# backpropagation
			loss.backward()
			optimizer.step()
			# append to training loss
			train_loss_current += loss.item() * fields_batch.size(0)
			# append to baseline loss
			baseline_losses_current += mean_loss.item() * fields_batch.size(0)

		train_loss = train_loss_current / len(config.train_df)
		train_losses.append(train_loss)

		baseline_loss = baseline_losses_current / len(config.train_df)
		baseline_losses.append(baseline_loss)

		# VALIDATION LOOP ---------------------------------------------------------
		model.eval()
		val_loss_current = 0

		with torch.no_grad():
			for batch in config.val_loader:
				fields_batch, params_batch = batch

				if config.augmentation:
					aug_choice = np.random.choice([True, False])
					if aug_choice:
						fields_batch, params_batch = random_augment_clim(
							fields_batch, params_batch
						)

				fields_batch, params_batch = (
					fields_batch.to(config.device),
					params_batch.to(config.device),
				)

				if config.shuffle:
					fields_batch = fields_batch[
						:, torch.randperm(fields_batch.size(1)), :, :
					]

				# print(input_batch.shape)
				output = model(fields_batch)

				if config.normalize:
					# normalized loss computation
					kappa2_norm = (params_batch[:, 0, :, :] - kappa2_mean) / kappa2_std
					theta_norm = (params_batch[:, 1, :, :] - theta_mean) / theta_std
					rho_norm = (params_batch[:, 2, :, :] - rho_mean) / rho_std
					params_batch_norm = torch.stack(
						[kappa2_norm, theta_norm, rho_norm], dim=1
					)

					kappa2_output_norm = (output[:, 0, :, :] - kappa2_mean) / kappa2_std
					theta_output_norm = (output[:, 1, :, :] - theta_mean) / theta_std
					rho_output_norm = (output[:, 2, :, :] - rho_mean) / rho_std
					output_norm = torch.stack(
						[kappa2_output_norm, theta_output_norm, rho_output_norm],
						dim=1,
					)

					# loss = criterion(output_norm, params_batch_norm)
					kappa2_loss = criterion(kappa2_output_norm, kappa2_norm)
					theta_loss = criterion(theta_output_norm, theta_norm)
					rho_loss = criterion(rho_output_norm, rho_norm)
					loss = kappa2_loss + theta_loss + rho_loss
				else:
					kappa2_loss = criterion(
						output[:, 0, :, :], params_batch[:, 0, :, :]
					)
					theta_loss = criterion(output[:, 1, :, :], params_batch[:, 1, :, :])
					rho_loss = criterion(output[:, 2, :, :], params_batch[:, 2, :, :])
					loss = kappa2_loss + theta_loss + rho_loss

				# append to val loss
				val_loss_current += loss.item() * fields_batch.size(0)

		val_loss = val_loss_current / len(config.val_df)
		val_losses.append(val_loss)

		current_lr = scheduler.get_last_lr()[0]  # optimizer.param_groups[0]["lr"]
		if config.verbose:
			print(f"Epoch {epoch + 1}/{config.n_epochs}")
			print(
				f"Train Loss: {train_loss:.6f} - Base (Mean) Loss: {baseline_loss:.6f} - Val Loss: {val_loss:.6f}"
			)
			print(f"Learning Rate: {current_lr:.6f}")

		# learning rate scheduler step
		scheduler.step(val_loss)

		# check for the best validation loss and save the best model weights
		if val_loss < best_val_loss:
			best_val_loss = val_loss
			best_model_wts = model.state_dict()
			early_stop_counter = 0
		else:
			early_stop_counter += 1

		# early stopping conditional
		if early_stop_counter >= patience:
			if config.verbose:
				print(
					f"Early stopping at epoch {epoch + 1}. Best validation loss: {best_val_loss:.6f}"
				)
			break

	# save different models weights in different locations based on modeltype string
	if config.save:
		torch.save(model.state_dict(), config.save_directory + config.savename)

	# load the best weights after training
	model.load_state_dict(best_model_wts)
	print("Training complete. Best model weights loaded.")

	return {
		"model": model,
		"train_losses": train_losses,
		"baseline_losses": baseline_losses,
		"val_losses": val_losses,
	}

``````{ end_of_file="latticevision/img2img/train.py" }

``````{ path="latticevision/__init__.py"  }

``````{ end_of_file="latticevision/__init__.py" }

``````{ path="latticevision/device.py"  }
import torch


def set_device(
	machine: str = "local", gpu: bool = True, gpu_id: str = "cuda", verbose: bool = True
) -> torch.device:
	"""
	Sets the device for computation (CPU or GPU).

	Args:
		machine: str, optional
			"local" or "remote" machine. Default is "local".
		gpu: bool, optional
			If True, uses the GPU. Default is True.
		gpu_id: str, optional
			Specifies the GPU ID. Default is "cuda". Remote needs specification.
		verbose: bool, optional
			If True, prints the device and hardware. Default is True.

	Returns:
		- torch.device
			Device for computation
	"""
	# if gpu is true, try to use the gpu
	if gpu == True:
		if machine == "local":
			if torch.cuda.is_available():
				device = torch.device("cuda")
			elif torch.backends.mps.is_available():
				device = torch.device("mps")
			else:
				device = torch.device("cpu")

		elif machine == "remote":
			# remote requires specification
			device = torch.device(gpu_id if torch.cuda.is_available() else "cpu")

		else:
			print("Invalid Machine, please choose either local or remote")

	# if gpu is false, use the cpu
	else:
		device = torch.device("cpu")

	# print the device and processing unit that will be used
	if verbose:
		if gpu == True:
			print(
				"Using device:", device, "\nHardware: ", torch.cuda.get_device_name(0)
			)
		else:
			print("Using device: ", device)

	return device

``````{ end_of_file="latticevision/device.py" }

``````{ path="latticevision/img_augment.py"  }
import random
import torch


# AUGMENTATION FOR CNN: -----------------
def random_negative(fields):
	"""
	Randomly negates CNN fields.
	"""
	fields = -fields
	return fields


def random_translate(fields):
	"""
	Randomly translates CNN fields by a random shift in the x and y directions.
	"""
	# get dims for bounding the shift
	_, _, rows, cols = fields.shape

	# randomly shift (at max shift whole image)
	xshift = random.randint(-cols, cols)
	yshift = random.randint(-rows, rows)

	# roll field
	fields = torch.roll(fields, shifts=xshift, dims=-1)
	fields = torch.roll(fields, shifts=yshift, dims=-2)

	return fields


def random_augment(fields):
	"""
	Randomly augments CNN fields by applying one or more augmentations.
	Helper function for training, eval, and plotting.
	"""
	# randomly choose one or two augmentations
	augs = [random_negative, random_translate]
	num_augs = random.randint(1, len(augs))
	chosen_augs = random.sample(augs, num_augs)

	for aug in chosen_augs:
		fields = aug(fields)

	return fields


# AUGMENTATION FOR IMG 2 IMG ------------------


def random_negative_clim(fields, params):
	"""
	Randomly negates fields, but has
	no impact on parameters.
	"""
	fields = -fields
	return fields, params


def random_translate_clim(fields, params):
	"""
	Randomly translates a pair of fields and parameters
	by a random shift in the x and y directions.
	"""
	# get dims for bounding the shift
	_, _, rows, cols = fields.shape

	# randomly shift (at max shift whole image)
	xshift = random.randint(-cols, cols)
	yshift = random.randint(-rows, rows)

	# roll fiels
	fields = torch.roll(fields, shifts=xshift, dims=-1)
	fields = torch.roll(fields, shifts=yshift, dims=-2)

	# roll second field the same way (param)
	params = torch.roll(params, shifts=xshift, dims=-1)
	params = torch.roll(params, shifts=yshift, dims=-2)

	return fields, params


def random_augment_clim(fields, params):
	"""
	Randomly augments a pair of fields and parameters
	by applying one or more augmentations.
	"""

	# available augmentations
	augs = [random_translate_clim, random_translate_clim]
	# randomly choose one or more of them
	num_augs = random.randint(1, len(augs))
	chosen_augs = random.sample(augs, num_augs)

	# apply to both fields and params
	for aug in chosen_augs:
		fields, params = aug(fields, params)

	return fields, params

``````{ end_of_file="latticevision/img_augment.py" }

``````{ path="latticevision/plotting.py"  }
import random
from typing import List, Optional

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset

from latticevision.img2img.dataset import no_transform, polar_transform


def plot_example_field(
	dataset,
	config,
	idx: int,
	model_type: str = "STUN",
	field_color: str = "turbo",
	param1_color: str = "viridis",
	param2_color: str = "viridis",
	param3_color: str = "viridis",
	show: bool = True,
) -> None:
	"""
	Plots an example field at a specified index from a dataset.
	Works for both STUN and CNN models. Takes color arguments.

	Args:
	    dataset: Dataset
	        The dataset containing the fields and associated parameters.
		config:
			The configuration for the dataset.
	    idx: int
	        The index of the field to be plotted.
	    model_type: str
	        The type of model used to generate the field. Default is "STUN".
	    field_color: str
	        The color map to use for the field. Default is "turbo".
	    param1_color: str
	        The color map to use for the first parameter. Default is "viridis".
	    param2_color: str
	        The color map to use for the second parameter. Default is "viridis".
	    param3_color: str
	        The color map to use for the third parameter. Default is "viridis".
	    show: bool
	        Whether to display the plot. If False, the figure is created but not shown.

	Returns:
	    None
	"""

	if model_type == "CNN":
		# extract field and params
		field, params = dataset[idx]
		# separate params for plotting
		kappa2 = params[0]
		theta = params[1]
		rho = params[2]

		fig, ax = plt.subplots(figsize=(6, 5))
		im = ax.imshow(field[0], cmap=field_color)
		fig.colorbar(im, ax=ax, orientation="vertical")

		ax.set_title(f"Field #{idx + 1}, Replicate #1")
		ax.set_xlabel(
			rf"$\kappa^2$ = {kappa2:.4f}, $\theta$ = {theta:.2f}, $\rho$ = {rho:.2f}"
		)
		ax.invert_yaxis()

	elif model_type == "STUN":
		# extract field and params
		field = dataset[idx][0][0]
		params = dataset[idx][1]
		# kappa2 transformed back to original scale
		kappa2 = torch.exp(params[0])
		theta = params[1]
		rho = params[2]

		fig, axs = plt.subplots(1, 4, figsize=(20, 5))

		# spatial field plot
		im1 = axs[0].imshow(field, cmap=field_color)
		axs[0].set_title(f"Field at Index {idx}")
		fig.colorbar(im1, ax=axs[0], orientation="horizontal", shrink=0.8)
		axs[0].invert_yaxis()

		# kappa2 plot
		im2 = axs[1].imshow(kappa2, cmap=param1_color)
		axs[1].set_title(r"$\kappa^2$ Field")
		fig.colorbar(im2, ax=axs[1], orientation="horizontal", shrink=0.8)
		axs[1].invert_yaxis()

		# theta plot
		im3 = axs[2].imshow(theta, cmap=param2_color)
		if config.transform_function == no_transform:
			axs[2].set_title(r"$\theta$ Field")
		elif config.transform_function == polar_transform:
			axs[2].set_title(r"$f_1$ (Polar) Field")
		fig.colorbar(im3, ax=axs[2], orientation="horizontal", shrink=0.8)
		axs[2].invert_yaxis()

		# rho_x plot
		im4 = axs[3].imshow(rho, cmap=param3_color)
		if config.transform_function == no_transform:
			axs[3].set_title(r"$\rho$ Field")
		elif config.transform_function == polar_transform:
			axs[3].set_title(r"$f_2$ (Polar) Field")
		fig.colorbar(im4, ax=axs[3], orientation="horizontal", shrink=0.8)
		axs[3].invert_yaxis()

	else:
		print(
			"Invalid model type. Unable to plot field. Please use either STUN or CNN."
		)
		return

	# option set for testing
	if show:
		plt.show()
	else:
		plt.close(fig)


def plot_losses(
	train_losses: list, val_losses: list, base_losses: list = None, show: bool = True
) -> None:
	"""
	Plots the training and validation loss curves. Optionally plots the baseline loss
	if provided.

	Args:
	    train_losses: list
	        List of training loss values for each epoch.
	    val_losses: list
	        List of validation loss values for each epoch.
	    base_losses: list, optional
	        List of baseline loss values for each epoch. Default is None.
	    show: bool
	        Whether to display the plot or not.

	Returns:
		None
	"""
	fig, ax = plt.subplots(figsize=(10, 5))

	ax.plot(train_losses, label="Training Loss", color="mediumturquoise")
	ax.plot(val_losses, label="Validation Loss", color="coral")

	# add if baseline (mean) loss provided
	if base_losses is not None:
		ax.plot(base_losses, label="Baseline Loss", color="yellowgreen")

	ax.set_xlabel("Epoch")
	ax.set_ylabel("Loss")
	ax.set_title("Loss Curves")
	ax.legend()

	if show:
		plt.show()
	else:
		plt.close(fig)


def plot_img2img_samples(
	model: nn.Module,
	config,
	device: torch.device,
	test_df: Dataset,
	indices: Optional[List[int]] = None,
	random_selection: bool = True,
	num_rand_samples: int = 3,
	show: bool = True,
	awght_not_kappa2: bool = False,
	cnn_mode: bool = False,
	cnn_results: Optional[torch.Tensor] = None,
) -> None:
	"""
	Predicts and plots the true and predicted parameter fields for a few samples.
	Typically, just uses STUN model to evaluate the fields at the indices provided
	on the fly. To make life easy, a `cnn_mode` has been integrated to plot the
	results from local CNN estimation also. In order to keep things simple with
	the tiling/local estimation functions, the CNN results need to provided
	ahead of time, rather than being computed inside the function.

	Args:
	    model: nn.Module
	        The trained STUN model used to generate predictions for the fields.
		config:
			The configuration for the dataset.
	    device: torch.device
	        The device on which computations are performed.
	    test_df: Dataset
	        The dataset containing test fields and parameters.
	    indices: Optional[List[int]], optional
	        Specific indices to plot. Ignored if `random_selection` is True.
	    random_selection: bool, optional
	        If True, selects random indices for plotting. Default is True.
	    num_rand_samples: int, optional
	        Number of random samples to plot if `random_selection` is True. Default is 3.
	    show: bool, optional
	        Whether to display the plot. Default is True.
	    awght_not_kappa2: bool, optional
	        If True, plots the `awght` field instead of `kappa2`. Default is False.
		cnn_mode: bool, optional
			Whether to use CNN data. Default is False.
		cnn_results: Optional[torch.Tensor], optional
			The CNN results to use for plotting. Default is None.

	Returns:
	    None
	"""

	# in case random indices are desired
	if random_selection:
		num_samples = num_rand_samples
		indices = random.sample(range(len(test_df)), num_samples)
	else:
		num_samples = len(indices)

	fields_sample = []
	params_sample = []
	# if we are operating with cnn data, results are already provided
	if cnn_mode:
		preds_sample = []

	for i in indices:
		field, params = test_df[i]
		fields_sample.append(field)
		params_sample.append(params)
		if cnn_mode:
			preds_sample.append(cnn_results[i])

	fields_torch = torch.stack(fields_sample).to(device)
	params_torch = torch.stack(params_sample).to(device)
	if cnn_mode:
		preds_torch = torch.stack(preds_sample).to(device)
	# if working with STUN, results are created using the model
	if cnn_mode == False:
		preds_torch = model(fields_torch)

	fig, ax = plt.subplots(num_samples, 7, figsize=(26, 2.4 * num_samples))

	if num_samples == 1:
		# in case of single sample
		ax = ax.reshape(1, 7)

	for i in range(num_samples):
		# fields
		field = fields_torch[i, 0].detach().cpu().numpy()

		# true params
		kappa2 = params_torch[i, 0].detach().cpu().numpy()
		theta = params_torch[i, 1].detach().cpu().numpy()
		rho = params_torch[i, 2].detach().cpu().numpy()
		awght = np.exp(kappa2) + 4

		# predicted params
		kappa2_pred = preds_torch[i, 0].detach().cpu().numpy()
		theta_pred = preds_torch[i, 1].detach().cpu().numpy()
		rho_pred = preds_torch[i, 2].detach().cpu().numpy()
		awght_pred = np.exp(kappa2_pred) + 4

		# Plot 1: spatial field
		im0 = ax[i, 0].imshow(field, cmap="turbo")
		ax[i, 0].set_title(f"Field {indices[i] + 1}")
		fig.colorbar(im0, ax=ax[i, 0], fraction=0.046, pad=0.04)
		ax[i, 0].invert_yaxis()

		# Plot 1 and 2: true and predicted awght or kappa2
		if awght_not_kappa2:
			# true and predicted awght
			im1 = ax[i, 1].imshow(awght, cmap="viridis")
			ax[i, 1].set_title("True Awght")
			fig.colorbar(im1, ax=ax[i, 1], fraction=0.046, pad=0.04)
			ax[i, 1].invert_yaxis()

			im2 = ax[i, 2].imshow(awght_pred, cmap="viridis")
			ax[i, 2].set_title("Predicted Awght")
			fig.colorbar(im2, ax=ax[i, 2], fraction=0.046, pad=0.04)
			ax[i, 2].invert_yaxis()
		else:
			# kappa2
			im1 = ax[i, 1].imshow(kappa2, cmap="viridis")
			ax[i, 1].set_title(r"True $log(\kappa^2)$")
			fig.colorbar(im1, ax=ax[i, 1], fraction=0.046, pad=0.04)
			ax[i, 1].invert_yaxis()

			im2 = ax[i, 2].imshow(kappa2_pred, cmap="viridis")
			ax[i, 2].set_title(r"Predicted $log(\kappa^2)$")
			fig.colorbar(im2, ax=ax[i, 2], fraction=0.046, pad=0.04)
			ax[i, 2].invert_yaxis()

		# Plot 3 and 4: true and predicted theta
		if config.transform_function == no_transform:
			ax[i, 3].set_title(r"True $\theta$")
			ax[i, 4].set_title(r"Predicted $\theta$")
		elif config.transform_function == polar_transform:
			ax[i, 3].set_title(r"True $f_1$")
			ax[i, 4].set_title(r"Predicted $f_1$")

		im3 = ax[i, 3].imshow(theta, cmap="viridis")
		fig.colorbar(im3, ax=ax[i, 3], fraction=0.046, pad=0.04)
		ax[i, 3].invert_yaxis()

		im4 = ax[i, 4].imshow(theta_pred, cmap="viridis")
		fig.colorbar(im4, ax=ax[i, 4], fraction=0.046, pad=0.04)
		ax[i, 4].invert_yaxis()

		# Plot 5 and 6: true and predicted rho
		if config.transform_function == no_transform:
			ax[i, 5].set_title(r"True $\rho$")
			ax[i, 6].set_title(r"Predicted $\rho$")
		elif config.transform_function == polar_transform:
			ax[i, 5].set_title(r"True $f_2$")
			ax[i, 6].set_title(r"Predicted $f_2$")

		im5 = ax[i, 5].imshow(rho, cmap="viridis")
		fig.colorbar(im5, ax=ax[i, 5], fraction=0.046, pad=0.04)
		ax[i, 5].invert_yaxis()

		im6 = ax[i, 6].imshow(rho_pred, cmap="viridis")
		fig.colorbar(im6, ax=ax[i, 6], fraction=0.046, pad=0.04)
		ax[i, 6].invert_yaxis()

	plt.tight_layout()
	if show:
		plt.show()
	else:
		plt.close(fig)

``````{ end_of_file="latticevision/plotting.py" }

``````{ path="latticevision/seed.py"  }
import os
import random
import numpy as np
import torch


def set_all_random_seeds(seed: int, cudnn_deterministic: bool = False):
	# python
	random.seed(seed)
	os.environ["PYTHONHASHSEED"] = str(seed)

	# numpy
	np.random.seed(seed)

	# torch
	torch.manual_seed(seed)
	torch.cuda.manual_seed(seed)
	torch.cuda.manual_seed_all(seed)

	# cuDNN is optional (deterministic is slow)
	if cudnn_deterministic:
		torch.backends.cudnn.deterministic = True
		torch.backends.cudnn.benchmark = False
	else:
		# cuDNN picks fastest kernels (non‑deterministic)
		torch.backends.cudnn.deterministic = False
		torch.backends.cudnn.benchmark = True

``````{ end_of_file="latticevision/seed.py" }

``````{ path="notebooks/cesm_application.ipynb" processed_with="ipynb_to_md" }
## Application to CESM LENS Fields

In this notebook we pass the CESM LENS Temperature Sensitivity Anomalies Data through both an I2I model and a CNN model of choice. 

```python
import torch
import matplotlib.pyplot as plt
import numpy as np
import h5py

from latticevision.device import set_device
from latticevision.img2img import TransUNet, UNet, ViT
from latticevision.img2img.base import (
	ModelConfig,
	NullPosEmbed,
	LearnedPosEmbed,
	SinusoidalPosEmbed,
	RotaryPosEmbed,
)

from latticevision.cnn.model import ModelConfig as CNNModelConfig
from latticevision.cnn.model import CNN
from latticevision.cnn.eval import fast_cnn_field_tiler
from latticevision.seed import set_all_random_seeds
```

```python
# fix random seed for reproducibility
set_all_random_seeds(777)

device = set_device(machine="remote", gpu_id="cuda:0", verbose=True)  # for remote use
# device = set_device(machine="local", gpu = False, verbose=True)  # for local use
```

Here we pull in the climate fields. 

```python
file_path = "../data/CESM_LENS_fields.h5"
with h5py.File(file_path, "r") as f:
	print("Components in the file:", list(f.keys()))

	# extract components
	clim_fields = f["clim_fields"][:]
	clim_fields_norm = f["clim_fields_norm"][:]

	print("clim_fields shape:", clim_fields.shape)
```

```python
# center in the pacific and add batch dimension
clim_fields_norm = np.roll(clim_fields_norm, shift=144, axis=-1)
clim_fields_norm = torch.tensor(clim_fields_norm).unsqueeze(0).float().to(device)
clim_fields_norm.shape
```

Pulling in our I2I model of choice: 

```python
modeltype = "TransUNet"  # can also be "UNet, "ViT"
modelpath = "../results/model_wghts/modelTransUNet_reps30_posRotaryPosEmbed.pth"
n_reps = 30
pos_embeds = [NullPosEmbed, LearnedPosEmbed, SinusoidalPosEmbed, RotaryPosEmbed]

if modeltype == "UNet":
	config = ModelConfig(
		in_channels=n_reps,
	)
	model = UNet(config)
	model.load_state_dict(torch.load(modelpath))

elif modeltype == "TransUNet":
	config = ModelConfig(
		in_channels=n_reps,
		patch_size_h=2,
		patch_size_w=2,
		pos_embed_cls=pos_embeds[3],
	)
	model = TransUNet(config)
	model.load_state_dict(torch.load(modelpath))
elif modeltype == "ViT":
	config = ModelConfig(
		in_channels=n_reps,
		patch_size_h=16,
		patch_size_w=16,
		pos_embed_cls=pos_embeds[3],
	)
	model = ViT(config)
	model.load_state_dict(torch.load(modelpath))
else:
	raise ValueError("modeltype must be one of 'UNet', 'TransUNet', or 'ViT'")

model = model.to(device)
model.eval()
```

We then pass the climate fields through the model and visualize the resulting parameters:

```python
# Passing the climate fields through the model
output = model(clim_fields_norm[:, :n_reps, :, :])
print(output.shape)

kappa2 = np.flip(output[0, 0, :, :].detach().cpu().numpy(), axis=0)
theta = np.pi / 2 - np.flip(output[0, 1, :, :].detach().cpu().numpy(), axis=0)
rho = np.flip(output[0, 2, :, :].detach().cpu().numpy(), axis=0)
clim_field = np.flip(clim_fields_norm[0, 0, :, :].detach().cpu().numpy(), axis=0)
awght = np.exp(kappa2) + 4
print(
	np.min(awght), np.max(awght), np.min(theta), np.max(theta), np.min(rho), np.max(rho)
)
```

```python
# plot the first original clim field and the outputs (3 channels)
fig, ax = plt.subplots(2, 2, figsize=(13, 8))

ax[0, 0].imshow(clim_field, cmap="turbo")
ax[0, 0].set_title("First clim field")
fig.colorbar(ax[0, 0].imshow(clim_field, cmap="turbo"), ax=ax[0, 0], shrink=0.9)

ax[0, 1].imshow(kappa2, cmap="turbo")
ax[0, 1].set_title("awght")
fig.colorbar(ax[0, 1].imshow(awght, cmap="viridis"), ax=ax[0, 1], shrink=0.9)

ax[1, 0].imshow(theta, cmap="viridis")
ax[1, 0].set_title(r"$\theta$")
fig.colorbar(ax[1, 0].imshow(theta, cmap="viridis"), ax=ax[1, 0], shrink=0.9)

ax[1, 1].imshow(rho, cmap="viridis")
ax[1, 1].set_title(r"$\rho_x$")
fig.colorbar(ax[1, 1].imshow(rho, cmap="viridis"), ax=ax[1, 1], shrink=0.9)
plt.tight_layout()
plt.show()
```

One can then uncomment the lines below to save the model outputs (parameters):

```python
# final = output.detach().cpu().numpy()
# # remove first dim from final
# final = np.squeeze(final)
# final.shape
```

```python
# with h5py.File('../sample_data/STUN30rep_clim_output_example.h5', 'w') as f:
#     f.create_dataset('clim_output', data=final)
```

## Local CNN

We repeat all of the steps above with only minor differences to accomodate the moving window estimation approach with a CNN. 

```python
config = CNNModelConfig(
	sidelen=25,
	in_channels=30,
	out_params=3,
	conv_channels=(64, 128, 256),
	linear_sizes=(500, 64),
	kernel_sizes=(10, 7, 5),
	padding=0,
)

# config = CNNModelConfig(
#     sidelen=17,
# 	in_channels=30,
# 	out_params=3,
# 	conv_channels=(64, 128, 256),
# 	linear_sizes=(500,64),
#     kernel_sizes = (6,4,4),
#     padding = 0,
# )

# config = CNNModelConfig(
#     sidelen=9,
# 	in_channels=1,
# 	out_params=3,
# 	conv_channels=(64, 128, 256),
# 	linear_sizes=(500,64),
#     kernel_sizes = (2,2,2),
#     padding = 0,
# )

model = CNN(config)
state_dict = torch.load(
	"../results/model_wghts/modelCNN_size25_reps30.pth",
	map_location=torch.device(device),
)
model.load_state_dict(state_dict)
model.to(device)
```

```python
cnn_outputs = torch.zeros(output.size())
cnn_outputs.shape
```

```python
clim_fields_norm = clim_fields_norm[:, :n_reps, :, :]
```

```python
for i in range(len(cnn_outputs)):
	print(clim_fields_norm[i].unsqueeze(0).shape)
```

```python
for i in range(len(cnn_outputs)):
	cnn_output = fast_cnn_field_tiler(
		model=model,
		fields=clim_fields_norm[i].unsqueeze(0),
		device=device,
		patch_batch_size=10000,
		verbose=False,
		padding_mode="reflect",
		patch_size=25,
	)
	cnn_outputs[i] = cnn_output

cnn_outputs.shape
```

```python
kappa2_cnn = np.flip(cnn_outputs[0, 0, :, :].detach().cpu().numpy(), axis=0)
theta_cnn = np.pi / 2 - np.flip(cnn_outputs[0, 1, :, :].detach().cpu().numpy(), axis=0)
rho_cnn = np.flip(cnn_outputs[0, 2, :, :].detach().cpu().numpy(), axis=0)
awght_cnn = np.exp(kappa2_cnn) + 4
print(
	np.min(awght_cnn),
	np.max(awght_cnn),
	np.min(theta_cnn),
	np.max(theta_cnn),
	np.min(rho_cnn),
	np.max(rho_cnn),
)
```

```python
# plot the first original clim field and the outputs (3 channels)
fig, ax = plt.subplots(2, 2, figsize=(13, 8))

ax[0, 0].imshow(clim_field, cmap="turbo")
ax[0, 0].set_title("First clim field")
fig.colorbar(ax[0, 0].imshow(clim_field, cmap="turbo"), ax=ax[0, 0], shrink=0.9)

ax[0, 1].imshow(kappa2_cnn, cmap="turbo")
ax[0, 1].set_title("awght")
fig.colorbar(ax[0, 1].imshow(awght_cnn, cmap="viridis"), ax=ax[0, 1], shrink=0.9)

ax[1, 0].imshow(theta_cnn, cmap="viridis")
ax[1, 0].set_title(r"$\theta$")
fig.colorbar(ax[1, 0].imshow(theta_cnn, cmap="viridis"), ax=ax[1, 0], shrink=0.9)

ax[1, 1].imshow(rho_cnn, cmap="viridis")
ax[1, 1].set_title(r"$\rho$")
fig.colorbar(ax[1, 1].imshow(rho_cnn, cmap="viridis"), ax=ax[1, 1], shrink=0.9)
plt.tight_layout()
plt.show()
```

Once again, saving is done below. 

```python
# final_cnn = cnn_outputs.detach().cpu().numpy()
# # remove first dim from final
# final_cnn = np.squeeze(final_cnn)
# final_cnn.shape
```

```python
# with h5py.File('../sample_data/CNN25_clim_output_example.h5', 'w') as f:
#     f.create_dataset('clim_output', data=final_cnn)
```


``````{ end_of_file="notebooks/cesm_application.ipynb" }

``````{ path="notebooks/cnn_demo.ipynb" processed_with="ipynb_to_md" }
## CNN Demo Notebook

In this notebook we show how the CNN component in our project is trained and evaluated on smaller, anisotropic spatial data. 

This notebook is very similar in structure to the `i2i_demo.ipynb` notebook. 
For more details and guidance, please visit that notebook.

```python
# imports required for CNN
import time
import torch
from torch.utils.data import DataLoader

from latticevision.device import set_device
from latticevision.plotting import plot_example_field, plot_losses

from latticevision.cnn.dataset import make_dataset, DataConfig
from latticevision.cnn.model import ModelConfig, CNN
from latticevision.cnn.train import TrainingConfig, train_model
from latticevision.cnn.eval import eval_model

# imports required for local CNN estimation on I2I data
from latticevision.img2img.dataset import make_dataset as make_img2img_dataset
from latticevision.img2img.dataset import DataConfig as Img2ImgDataConfig
from latticevision.cnn.eval import fast_cnn_field_tiler
from latticevision.plotting import plot_img2img_samples
from latticevision.img2img.eval import eval_model as eval_img2img_model
from latticevision.seed import set_all_random_seeds
```

```python
# set random seed for reproducibility
set_all_random_seeds(777)

device = set_device(machine="remote", gpu=True, gpu_id="cuda:0", verbose=True)
```

```python
dataset_path = "../data/CNN_sample_data.h5"
val_size = 0.4
test_size = 0.5

# dataset_path = "../data/CNN_data.h5"
# val_size = 0.1
# test_size = 0.2

data_config = DataConfig(
	file_path=dataset_path,
	fullsidelen=25,
	sidelen=25,
	n_replicates=30,
	n_params=3,
	log_kappa2=True,
	val_size=val_size,
	test_size=test_size,
	random_state=777,
	verbose=True,
)

data_dict = make_dataset(data_config)

train_df = data_dict["train_df"]
val_df = data_dict["val_df"]
test_df = data_dict["test_df"]
```

```python
plot_example_field(
	dataset=train_df,
	config=data_config,
	idx=0,
	model_type="CNN",
	field_color="turbo",
)
```

## Training

```python
# n_batch = 64
n_batch = 2

train_loader = DataLoader(train_df, batch_size=n_batch, shuffle=True)
val_loader = DataLoader(val_df, batch_size=n_batch, shuffle=False)
test_loader = DataLoader(test_df, batch_size=n_batch, shuffle=False)

for fields, params in train_loader:
	print("Train Loader:")
	print("Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape)
	break

for fields, params in val_loader:
	print("Validation Loader:")
	print("Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape)
	break

for fields, params in test_loader:
	print("Test Loader:")
	print("Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape)
	break
```

```python
config = ModelConfig(
	sidelen=25,
	in_channels=30,
	out_params=3,
	conv_channels=(64, 128, 256),
	linear_sizes=(500, 64),
	kernel_sizes=(10, 7, 5),
	padding=0,
)

# config = ModelConfig(
#     sidelen=17,
# 	in_channels=30,
# 	out_params=3,
# 	conv_channels=(64, 128, 256),
# 	linear_sizes=(500,64),
#     kernel_sizes = (6,4,4),
#     padding = 0,
# )

# config = ModelConfig(
#     sidelen=9,
# 	in_channels=30,
# 	out_params=3,
# 	conv_channels=(64, 128, 256),
# 	linear_sizes=(500,64),
#     kernel_sizes = (2,2,2),
#     padding = 0,
# )

model = CNN(config).to(device)
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total number of trainable parameters: {total_params}")
```

```python
start_time = time.time()

train_config = TrainingConfig(
	model=model,
	device=device,
	train_loader=train_loader,
	val_loader=val_loader,
	train_df=train_df,
	val_df=val_df,
	lr=1e-4,
	n_epochs=3,
	stop_patience=10,
	scheduler_patience=5,
	scheduler_factor=0.5,
	augmentation=False,
	save=False,  # switch to True to save model after training, currently this is just training for show
	save_directory="../results/model_wghts/",
	savename="cnn_wghts25_example.pth",
	verbose=True,
	normalize=True,
	shuffle=True,
)

training_results = train_model(config=train_config)

time_train = time.time() - start_time
print("Training took: ", time_train / 60, " minutes.")

model = training_results["model"]
train_losses = training_results["train_losses"]
val_losses = training_results["val_losses"]
```

```python
plot_losses(train_losses=train_losses, val_losses=val_losses)
```

## Evaluation

Here we load in existing weights, but you can comment those lines out to simply use the model you just trained. 

```python
config = ModelConfig(
	sidelen=25,
	in_channels=30,
	out_params=3,
	conv_channels=(64, 128, 256),
	linear_sizes=(500, 64),
	kernel_sizes=(10, 7, 5),
	padding=0,
)

# config = ModelConfig(
#     sidelen=17,
# 	in_channels=30,
# 	out_params=3,
# 	conv_channels=(64, 128, 256),
# 	linear_sizes=(500,64),
#     kernel_sizes = (6,4,4),
#     padding = 0,
# )

# config = ModelConfig(
#     sidelen=9,
# 	in_channels=30,
# 	out_params=3,
# 	conv_channels=(64, 128, 256),
# 	linear_sizes=(500,64),
#     kernel_sizes = (2,2,2),
#     padding = 0,
# )

model = CNN(config)
state_dict = torch.load(
	"../results/model_wghts/modelCNN_size25_reps30.pth",
	map_location=torch.device(device),
)
model.load_state_dict(state_dict)
model.to(device)
```

```python
metrics = eval_model(
	model=model,
	device=device,
	config=data_config,
	test_loader=test_loader,
	test_df=test_df,
	plot=True,
	augmentation=True,
)
```

### Local estimation on large image to image data 

We take a moving-window approach to local estimation, returning the same metrics and images as we do with our large vision models:


```python
# for testing
dataset_path_i2i = "../data/I2I_sample_data.h5"
val_size_i2i = 0.4
test_size_i2i = 0.5

# dataset_path_i2i = "../data/I2I_data.h5"
# val_size_i2i = 0.1
# test_size_i2i = 0.2

data_config_i2i = Img2ImgDataConfig(
	file_path=dataset_path_i2i,
	n_rows=192,
	n_cols=288,
	n_replicates=30,
	n_params=3,
	log_kappa2=True,
	shift_theta=True,
	val_size=val_size_i2i,
	test_size=test_size_i2i,
	random_state=777,
	verbose=True,
)

data_dict = make_img2img_dataset(
	config=data_config_i2i,
)

test_df_i2i = data_dict["test_df"]

del data_dict

# n_batch = 64
n_batch = 2

# make loader and df are in same order for eval funcs
test_loader_i2i = DataLoader(test_df_i2i, batch_size=n_batch, shuffle=False)
for fields, params in test_loader_i2i:
	print("Test Loader:")
	print("Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape)
	break
```

```python
test_fields = test_df_i2i[:][0]
test_params = test_df_i2i[:][1]

print("Test fields shape: ", test_fields.shape)
print("Test params shape: ", test_params.shape)

outputs = torch.zeros(test_params.size())
```

```python
# fast option
for i in range(len(outputs)):
	output = fast_cnn_field_tiler(
		model=model,
		fields=test_fields[i].unsqueeze(0),
		device=device,
		patch_batch_size=10000,
		verbose=False,
		padding_mode="reflect",
		patch_size=25,
	)
	outputs[i] = output

# much slower option
# for i in range(len(outputs)):
#     output = slow_cnn_field_tiler(
#         model = model,
#         fields = test_fields[i].unsqueeze(0),
#         device = device,
#         padding_mode = "reflect",
#         verbose = True,
#         patch_size = 25,
#     )
#     outputs[i] = output
```

```python
metrics = eval_img2img_model(
	model=model,
	config=data_config_i2i,
	device=device,
	test_loader=test_loader_i2i,
	test_df=test_df_i2i,
	plot=True,
	augmentation=False,
	n_pixels=5000,
	show=True,
	cnn_mode=True,
	cnn_results=outputs,
)
```

```python
# inds = [0, 9, 16, 32, 37]
inds = [0]  # for test dataset

plot_img2img_samples(
	model=model,
	config=data_config_i2i,
	device=device,
	test_df=test_df_i2i,
	indices=inds,
	random_selection=False,
	num_rand_samples=5,
	awght_not_kappa2=True,
	show=True,
	cnn_mode=True,
	cnn_results=outputs,
)
```


``````{ end_of_file="notebooks/cnn_demo.ipynb" }

``````{ path="notebooks/cnn_trainloop.py"  }
# imports required for CNN
import time
import os
import torch
import numpy as np
import h5py
import pandas as pd
from torch.utils.data import DataLoader

from latticevision.device import set_device

from latticevision.cnn.dataset import make_dataset, DataConfig
from latticevision.cnn.model import ModelConfig, CNN
from latticevision.cnn.train import TrainingConfig, train_model

# imports required for local CNN estimation on STUN data
from latticevision.img2img.dataset import make_dataset as make_img2img_dataset
from latticevision.img2img.dataset import DataConfig as Img2ImgDataConfig
from latticevision.cnn.eval import fast_cnn_field_tiler
from latticevision.img2img.eval import eval_model as eval_img2img_model
from latticevision.seed import set_all_random_seeds
from latticevision.img2img.dataset import polar_transform, no_transform

# make sure folders exist
for d in ("results/model_wghts", "results/metrics", "results/clim_outputs"):
	os.makedirs(d, exist_ok=True)

# set the random seed for reproducibility
set_all_random_seeds(777)

# set the device you are on
device = set_device(
	machine="remote", gpu=True, gpu_id="cuda:0", verbose=True
)  # for remote use

modelsize_list = [9, 17, 25]
num_replicates_list = [1, 5, 15, 30]

# load big i2i data first, depends on num replicates
for num_replicates in num_replicates_list:
	dataset_path_i2i = "data/I2I_data.h5"
	val_size_i2i = 0.1
	test_size_i2i = 0.2

	transform_funcs = [no_transform, polar_transform]

	data_config_i2i = Img2ImgDataConfig(
		file_path=dataset_path_i2i,
		n_rows=192,
		n_cols=288,
		n_replicates=num_replicates,
		transform_function=transform_funcs[0],
		n_params=3,
		log_kappa2=True,
		shift_theta=True,
		val_size=val_size_i2i,
		test_size=test_size_i2i,
		random_state=777,
		verbose=True,
	)

	data_dict = make_img2img_dataset(
		config=data_config_i2i,
	)

	test_df_i2i = data_dict["test_df"]

	del data_dict

	n_batch = 64
	test_loader_i2i = DataLoader(test_df_i2i, batch_size=n_batch, shuffle=False)
	for fields, params in test_loader_i2i:
		print("Test Loader:")
		print(
			"Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape
		)
		break

	# loop through cnn models
	for modelsize in modelsize_list:
		dataset_path = "data/CNN_data.h5"
		val_size = 0.1
		test_size = 0.2

		data_config = DataConfig(
			file_path=dataset_path,
			fullsidelen=25,
			sidelen=modelsize,
			n_replicates=num_replicates,
			n_params=3,
			log_kappa2=True,
			val_size=val_size,
			test_size=test_size,
			random_state=777,
			verbose=True,
		)

		data_dict = make_dataset(data_config)

		train_df = data_dict["train_df"]
		val_df = data_dict["val_df"]
		test_df = data_dict["test_df"]

		n_batch = 64

		train_loader = DataLoader(train_df, batch_size=n_batch, shuffle=True)
		val_loader = DataLoader(val_df, batch_size=n_batch, shuffle=False)
		test_loader = DataLoader(test_df, batch_size=n_batch, shuffle=False)

		for fields, params in train_loader:
			print("Train Loader:")
			print(
				"Fields batch shape: ",
				fields.shape,
				"\nParams batch shape: ",
				params.shape,
			)
			break

		for fields, params in val_loader:
			print("Validation Loader:")
			print(
				"Fields batch shape: ",
				fields.shape,
				"\nParams batch shape: ",
				params.shape,
			)
			break

		for fields, params in test_loader:
			print("Test Loader:")
			print(
				"Fields batch shape: ",
				fields.shape,
				"\nParams batch shape: ",
				params.shape,
			)
			break

		if modelsize == 9:
			config = ModelConfig(
				sidelen=9,
				in_channels=num_replicates,
				out_params=3,
				conv_channels=(64, 128, 256),
				linear_sizes=(500, 64),
				kernel_sizes=(2, 2, 2),
				padding=0,
			)

		elif modelsize == 17:
			config = ModelConfig(
				sidelen=17,
				in_channels=num_replicates,
				out_params=3,
				conv_channels=(64, 128, 256),
				linear_sizes=(500, 64),
				kernel_sizes=(6, 4, 4),
				padding=0,
			)

		elif modelsize == 25:
			config = ModelConfig(
				sidelen=25,
				in_channels=num_replicates,
				out_params=3,
				conv_channels=(64, 128, 256),
				linear_sizes=(500, 64),
				kernel_sizes=(10, 7, 5),
				padding=0,
			)

		else:
			raise ValueError("Model size not recognized. Please use 9, 17, or 25.")

		print("Running CNN with modelsize: ", modelsize)
		print("Running num_replicates: ", num_replicates)

		model = CNN(config).to(device)
		total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
		print(f"Total number of trainable parameters: {total_params}")

		meta = {
			"model": "CNN",
			"size": modelsize,
			"reps": num_replicates,
		}
		basename = "_".join(f"{k}{v}" for k, v in meta.items())

		start_time = time.time()

		train_config = TrainingConfig(
			model=model,
			device=device,
			train_loader=train_loader,
			val_loader=val_loader,
			train_df=train_df,
			val_df=val_df,
			lr=1e-4,
			n_epochs=200,
			stop_patience=10,
			scheduler_patience=5,
			scheduler_factor=0.5,
			augmentation=False,
			save=True,
			save_directory="results/model_wghts/",
			savename=f"{basename}.pth",
			verbose=True,
			normalize=True,
			shuffle=True,
		)

		training_results = train_model(config=train_config)

		print("--- %s seconds ---" % (time.time() - start_time))
		time_train = time.time() - start_time
		print("Training took: ", time_train / 60, " minutes.")

		model = training_results["model"]
		train_losses = training_results["train_losses"]
		val_losses = training_results["val_losses"]

		# eval the model on the i2i data
		test_fields = test_df_i2i[:][0]
		test_params = test_df_i2i[:][1]

		print("Test fields shape: ", test_fields.shape)
		print("Test params shape: ", test_params.shape)

		metrics_start_time = time.time()
		outputs = torch.zeros(test_params.size())
		for i in range(len(outputs)):
			output = fast_cnn_field_tiler(
				model=model,
				fields=test_fields[i].unsqueeze(0),
				device=device,
				patch_batch_size=10000,
				verbose=False,
				padding_mode="reflect",
				patch_size=modelsize,
			)
			outputs[i] = output
		print("--- %s seconds ---" % (time.time() - metrics_start_time))
		time_eval = time.time() - metrics_start_time
		print("Eval took: ", time_eval / 60, " minutes.")

		metrics = eval_img2img_model(
			model=model,
			config=data_config_i2i,
			device=device,
			test_loader=test_loader_i2i,
			test_df=test_df_i2i,
			plot=False,
			augmentation=False,
			n_pixels=5000,
			show=False,
			cnn_mode=True,
			cnn_results=outputs,
		)

		if not isinstance(metrics, pd.DataFrame):
			metrics = pd.DataFrame([metrics])

		metrics_path = "results/metrics/"

		metrics.to_csv(
			metrics_path + f"{basename}_metrics.csv",
			index=False,
		)

		# now we feed in clim fields
		file_path = "data/CESM_LENS_fields.h5"
		with h5py.File(file_path, "r") as f:
			print("Components in the file:", list(f.keys()))

			# extract components
			clim_fields = f["clim_fields"][:]
			clim_fields_norm = f["clim_fields_norm"][:]

			print("clim_fields shape:", clim_fields.shape)

		clim_fields_norm = np.roll(clim_fields_norm, shift=144, axis=-1)
		clim_fields_norm = (
			torch.tensor(clim_fields_norm).unsqueeze(0).float().to(device)
		)

		clim_fields_norm = clim_fields_norm[:, :num_replicates, :, :]
		print("Clim fields shape: ", clim_fields_norm.shape)

		cnn_outputs = torch.zeros(1, 3, 192, 288)
		print(cnn_outputs.shape)

		for i in range(len(cnn_outputs)):
			cnn_output = fast_cnn_field_tiler(
				model=model,
				fields=clim_fields_norm[i].unsqueeze(0),
				device=device,
				patch_batch_size=10000,
				verbose=False,
				padding_mode="reflect",
				patch_size=modelsize,
			)
			cnn_outputs[i] = cnn_output

		print("CNN Clim outputs shape: ", cnn_outputs.shape)

		kappa2_cnn = np.flip(cnn_outputs[0, 0, :, :].detach().cpu().numpy(), axis=0)
		theta_cnn = np.pi / 2 - np.flip(
			cnn_outputs[0, 1, :, :].detach().cpu().numpy(), axis=0
		)
		rho_cnn = np.flip(cnn_outputs[0, 2, :, :].detach().cpu().numpy(), axis=0)
		awght_cnn = np.exp(kappa2_cnn) + 4

		print(
			np.min(awght_cnn),
			np.max(awght_cnn),
			np.min(theta_cnn),
			np.max(theta_cnn),
			np.min(rho_cnn),
			np.max(rho_cnn),
		)

		final = cnn_outputs.detach().cpu().numpy()
		final = np.squeeze(final)
		print(final.shape)

		clim_result_path = "results/clim_outputs/"

		with h5py.File(clim_result_path + f"{basename}_clim_output.h5", "w") as f:
			f.create_dataset("clim_output", data=final)

		del model, training_results
		del data_dict, train_df, val_df, test_df, data_config
		del train_loader, val_loader, test_loader

	torch.cuda.empty_cache()
	import gc

	gc.collect()

``````{ end_of_file="notebooks/cnn_trainloop.py" }

``````{ path="notebooks/i2i_demo.ipynb" processed_with="ipynb_to_md" }
## Image-to-Image (I2I) Network Demo Notebook

In this notebook we show how STUN, UNet, and ViT are trained and evaluated on non-stationary, spatial data. This notebook is intended to be a tutorial/demonstration. 

```python
import torch
from torch.utils.data import DataLoader

import time

# directly import from latticevision library in directory
from latticevision.device import set_device
from latticevision.img2img.dataset import (
	make_dataset,
	DataConfig,
	no_transform,
	polar_transform,
)
from latticevision.plotting import plot_example_field, plot_losses, plot_img2img_samples
from latticevision.img2img import TransUNet, UNet, ViT
from latticevision.img2img.base import (
	ModelConfig,
	NullPosEmbed,
	LearnedPosEmbed,
	SinusoidalPosEmbed,
	RotaryPosEmbed,
)
from latticevision.img2img.train import train_model, TrainingConfig
from latticevision.img2img.eval import eval_model
from latticevision.seed import set_all_random_seeds
```

The `set_device` function from `device.py` is used to set the device based on whether you are running this work locally or on a remote server with named GPUs. 

```python
# set random seed for reproducibility
set_all_random_seeds(777)

# for remote use
device = set_device(machine="remote", gpu=True, gpu_id="cuda:0", verbose=True)

# for local use
# device = set_device(machine="local", gpu=True, verbose=True)
```

The `make_dataset` function from `dataset.py` returns the extracted fields, awghts, and configs from the hdf5 file, and creates our train, val, and test datasets for us. 

```python
# for testing
dataset_path = "../data/I2I_sample_data.h5"
val_size = 0.4
test_size = 0.5

# dataset_path = "../data/I2I_data.h5"
# val_size = 0.1
# test_size = 0.2

transform_funcs = [no_transform, polar_transform]

data_config = DataConfig(
	file_path=dataset_path,
	n_rows=192,
	n_cols=288,
	n_replicates=30,
	n_params=3,
	transform_function=transform_funcs[0],
	log_kappa2=True,
	shift_theta=True,
	val_size=val_size,
	test_size=test_size,
	random_state=777,
	verbose=True,
)

data_dict = make_dataset(
	config=data_config,
)

train_df = data_dict["train_df"]
val_df = data_dict["val_df"]
test_df = data_dict["test_df"]
```

Let's plot an example field using the `plot_example_field` function from `plotting.py`.

```python
plot_example_field(
	dataset=train_df,
	config=data_config,
	idx=0,
	model_type="STUN",
	field_color="turbo",
	param1_color="viridis",
	param2_color="viridis",
	param3_color="viridis",
)
```

## Training

We now use the `DataLoader` class from PyTorch to conveniently batch the data. 

```python
# n_batch = 64
n_batch = 2

train_loader = DataLoader(train_df, batch_size=n_batch, shuffle=True)
val_loader = DataLoader(val_df, batch_size=n_batch, shuffle=False)
test_loader = DataLoader(test_df, batch_size=n_batch, shuffle=False)

for fields, params in train_loader:
	print("Train Loader:")
	print(
		"Fields batch shape: ",
		fields.shape,
		"\nParams batch shape: ",
		params.shape,
	)
	break

for fields, params in val_loader:
	print("Val Loader:")
	print(
		"Fields batch shape: ",
		fields.shape,
		"\nParams batch shape: ",
		params.shape,
	)
	break

for fields, params in test_loader:
	print("Test Loader:")
	print(
		"Fields batch shape: ",
		fields.shape,
		"\nParams batch shape: ",
		params.shape,
	)
	break
```

Now let's pull in our `TransUNet (STUN)`, `UNet`, or `ViT` from `stun.py`, `unet.py`, and `vit.py` respectively. We access all of these through the `__init__.py` file in the `img2img` folder.  

```python
modeltype = "TransUNet"  # can also be "TransUNet, "ViT"

pos_embeds = [NullPosEmbed, LearnedPosEmbed, SinusoidalPosEmbed, RotaryPosEmbed]

if modeltype == "UNet":
	config = ModelConfig()
	model = UNet(config)
elif modeltype == "TransUNet":
	config = ModelConfig(
		patch_size_h=2,
		patch_size_w=2,
		pos_embed_cls=pos_embeds[3],
	)
	model = TransUNet(config)
elif modeltype == "ViT":
	config = ModelConfig(
		patch_size_h=16,
		patch_size_w=16,
		pos_embed_cls=pos_embeds[3],
	)
	model = ViT(config)
else:
	raise ValueError("modeltype must be one of 'UNet', 'TransUNet', or 'ViT'")

# send model to device and count params
model = model.to(device)
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total number of trainable parameters: {total_params}")
```

We train our model using `train_model` from `train.py`.

```python
start_time = time.time()

train_config = TrainingConfig(
	model=model,
	device=device,
	train_loader=train_loader,
	val_loader=val_loader,
	train_df=train_df,
	val_df=val_df,
	lr=1e-4,
	n_epochs=3,
	stop_patience=10,
	scheduler_patience=5,
	scheduler_factor=0.5,
	augmentation=True,
	save=False,  # switch to True to save model after training, currently this is just training for show
	save_directory="results/model_wghts/",
	savename="stun_wghts_example.pth",
	verbose=True,
	normalize=True,
	shuffle=True,
)


training_results = train_model(config=train_config)

time_train = time.time() - start_time
print("Training took: ", time_train / 60, " minutes.")

model = training_results["model"]
train_losses = training_results["train_losses"]
baseline_losses = training_results["baseline_losses"]
val_losses = training_results["val_losses"]
```

Let's take a look at the loss with `plot_losses`, another function from `plotting.py` which should work for both the CNN and I2I models. 

```python
plot_losses(
	train_losses=train_losses,
	val_losses=val_losses,
	base_losses=baseline_losses,
	show=True,
)
```

The `train_model` function saves the model in `results/model_wghts/` if `save = True`. 

## Evaluation

If we want to load it back up again, **we run everything in the notebook except the two cells in which the training loop and loss plotting code are executed**. Then we continue on through the notebook, beginning with loading the model by using the code below: 

```python
modeltype = "TransUNet"  # can also be "UNet, "ViT"
modelpath = "../results/model_wghts/modelTransUNet_reps30_posRotaryPosEmbed.pth"
n_reps = 30
pos_embeds = [NullPosEmbed, LearnedPosEmbed, SinusoidalPosEmbed, RotaryPosEmbed]

if modeltype == "UNet":
	config = ModelConfig(
		in_channels=n_reps,
	)
	model = UNet(config)
	model.load_state_dict(torch.load(modelpath))

elif modeltype == "TransUNet":
	config = ModelConfig(
		in_channels=n_reps,
		patch_size_h=2,
		patch_size_w=2,
		pos_embed_cls=pos_embeds[3],
	)
	model = TransUNet(config)
	model.load_state_dict(torch.load(modelpath))
elif modeltype == "ViT":
	config = ModelConfig(
		in_channels=n_reps,
		patch_size_h=16,
		patch_size_w=16,
		pos_embed_cls=pos_embeds[3],
	)
	model = ViT(config)
	model.load_state_dict(torch.load(modelpath))
else:
	raise ValueError("modeltype must be one of 'UNet', 'TransUNet', or 'ViT'")

model = model.to(device)
model.eval()
```

This `eval_model` function from `eval.py` calculates a number of metrics and does some plotting. 

```python
metrics = eval_model(
	model=model,
	config=data_config,
	device=device,
	test_loader=test_loader,
	test_df=test_df,
	plot=True,
	augmentation=False,
	n_pixels=5000,
	show=True,
)
```

Here we take a couple fields inside of the test data and plot how the model does using `plot_img2img_samples` from `plotting.py`.

```python
# inds = [0, 9, 16, 32, 37]
inds = [0]  # for test dataset

plot_img2img_samples(
	model=model,
	config=data_config,
	device=device,
	test_df=test_df,
	indices=inds,
	random_selection=False,
	num_rand_samples=5,
	awght_not_kappa2=True,
	show=True,
)
```


``````{ end_of_file="notebooks/i2i_demo.ipynb" }

``````{ path="notebooks/i2i_trainloop.py"  }
from torch.utils.data import DataLoader
import pandas as pd
import time
import os

# directly import from latticevision library in directory
from latticevision.device import set_device
from latticevision.img2img.dataset import (
	make_dataset,
	DataConfig,
	no_transform,
	polar_transform,
)
from latticevision.img2img import TransUNet, UNet, ViT
from latticevision.img2img.base import (
	ModelConfig,
	NullPosEmbed,
	LearnedPosEmbed,
	SinusoidalPosEmbed,
	RotaryPosEmbed,
)
from latticevision.img2img.train import train_model, TrainingConfig
from latticevision.img2img.eval import eval_model
from latticevision.seed import set_all_random_seeds


# additional imports for CESM analysis
import torch
import numpy as np
import h5py


# make sure folders exist
for d in ("results/model_wghts", "results/metrics", "results/clim_outputs"):
	os.makedirs(d, exist_ok=True)

# set the random seed for reproducibility
set_all_random_seeds(777)

# set the device you are on
device = set_device(
	machine="remote", gpu=True, gpu_id="cuda:0", verbose=True
)  # for remote use


modeltype_list = ["UNet", "TransUNet", "ViT"]
num_replicates_list = [1, 5, 15, 30]
pos_embeddings_all = [NullPosEmbed, LearnedPosEmbed, SinusoidalPosEmbed, RotaryPosEmbed]


for num_replicates in num_replicates_list:
	# create dataset and dataloaders
	dataset_path = "data/I2I_data.h5"
	val_size = 0.1
	test_size = 0.2

	transform_funcs = [no_transform, polar_transform]

	data_config = DataConfig(
		file_path=dataset_path,
		n_rows=192,
		n_cols=288,
		n_replicates=num_replicates,
		transform_function=transform_funcs[0],
		n_params=3,
		log_kappa2=True,
		shift_theta=True,
		val_size=val_size,
		test_size=test_size,
		random_state=777,
		verbose=True,
	)

	# load the data and time it
	start_time = time.time()
	data_dict = make_dataset(
		config=data_config,
	)
	print("--- %s seconds ---" % (time.time() - start_time))
	time_data = time.time() - start_time
	print("Dataset loading took: ", time_data / 60, " minutes.")

	train_df = data_dict["train_df"]
	val_df = data_dict["val_df"]
	test_df = data_dict["test_df"]

	n_batch = 64
	# n_batch = 2

	train_loader = DataLoader(train_df, batch_size=n_batch, shuffle=True)
	val_loader = DataLoader(val_df, batch_size=n_batch, shuffle=False)
	test_loader = DataLoader(test_df, batch_size=n_batch, shuffle=False)

	for fields, params in train_loader:
		print("Train Loader:")
		print(
			"Fields batch shape: ",
			fields.shape,
			"\nParams batch shape: ",
			params.shape,
		)
		break

	for fields, params in val_loader:
		print("Val Loader:")
		print(
			"Fields batch shape: ",
			fields.shape,
			"\nParams batch shape: ",
			params.shape,
		)
		break

	for fields, params in test_loader:
		print("Test Loader:")
		print(
			"Fields batch shape: ",
			fields.shape,
			"\nParams batch shape: ",
			params.shape,
		)
		break

	for modeltype in modeltype_list:
		# don't make unet iterate over all these pos embeddings
		if modeltype == "UNet":
			pos_embeddings_list = [NullPosEmbed]
		else:
			pos_embeddings_list = pos_embeddings_all

		for pos_embeddings in pos_embeddings_list:
			print("Running modeltype: ", modeltype)
			print("Running num_replicates: ", num_replicates)
			print("Running pos_embeddings: ", pos_embeddings.__name__)

			if modeltype == "UNet":
				config = ModelConfig(
					in_channels=num_replicates,
				)
				model = UNet(config)
			elif modeltype == "TransUNet":
				config = ModelConfig(
					in_channels=num_replicates,
					patch_size_h=2,
					patch_size_w=2,
					pos_embed_cls=pos_embeddings,
				)
				model = TransUNet(config)
			elif modeltype == "ViT":
				config = ModelConfig(
					in_channels=num_replicates,
					patch_size_h=16,
					patch_size_w=16,
					pos_embed_cls=pos_embeddings,
				)
				model = ViT(config)
			else:
				raise ValueError(
					"modeltype must be one of 'UNet', 'TransUNet', or 'ViT'"
				)

			# send model to device and count params
			model = model.to(device)
			total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
			print(f"Total number of trainable parameters: {total_params}")

			meta = {
				"model": modeltype,
				"reps": num_replicates,
				"pos": pos_embeddings.__name__,
			}

			basename = "_".join(f"{k}{v}" for k, v in meta.items())

			# train the model
			start_time = time.time()

			train_config = TrainingConfig(
				model=model,
				device=device,
				train_loader=train_loader,
				val_loader=val_loader,
				train_df=train_df,
				val_df=val_df,
				lr=1e-4,
				n_epochs=200,
				stop_patience=10,
				scheduler_patience=5,
				scheduler_factor=0.5,
				augmentation=True,
				save=True,
				save_directory="results/model_wghts/",
				savename=f"{basename}.pth",
				verbose=True,
				normalize=True,
				shuffle=True,
			)

			training_results = train_model(config=train_config)

			print("--- %s seconds ---" % (time.time() - start_time))
			time_train = time.time() - start_time
			print("Training took: ", time_train / 60, " minutes.")

			# save training results
			model = training_results["model"]
			train_losses = training_results["train_losses"]
			baseline_losses = training_results["baseline_losses"]
			val_losses = training_results["val_losses"]

			metrics_start_time = time.time()
			# evaluate the model performance
			metrics = eval_model(
				model=model,
				config=data_config,
				device=device,
				test_loader=test_loader,
				test_df=test_df,
				plot=False,
				augmentation=False,
				show=False,
			)
			print("--- %s seconds ---" % (time.time() - metrics_start_time))
			time_eval = time.time() - metrics_start_time
			print("Eval took: ", time_eval / 60, " minutes.")

			if not isinstance(metrics, pd.DataFrame):
				metrics = pd.DataFrame([metrics])

			metrics_path = "results/metrics/"

			metrics.to_csv(
				metrics_path + f"{basename}_metrics.csv",
				index=False,
			)

			file_path = "data/CESM_LENS_fields.h5"
			with h5py.File(file_path, "r") as f:
				print("Components in the file:", list(f.keys()))

				# extract components
				clim_fields = f["clim_fields"][:]
				clim_fields_norm = f["clim_fields_norm"][:]

				print("clim_fields shape:", clim_fields.shape)

			clim_fields_norm = np.roll(clim_fields_norm, shift=144, axis=-1)
			clim_fields_norm = (
				torch.tensor(clim_fields_norm).unsqueeze(0).float().to(device)
			)

			# pass through model
			output = model(clim_fields_norm[:, :num_replicates, :, :])

			kappa2 = np.flip(output[0, 0, :, :].detach().cpu().numpy(), axis=0)
			theta = np.pi / 2 - np.flip(
				output[0, 1, :, :].detach().cpu().numpy(), axis=0
			)
			rho = np.flip(output[0, 2, :, :].detach().cpu().numpy(), axis=0)
			clim_field = np.flip(
				clim_fields_norm[0, 0, :, :].detach().cpu().numpy(), axis=0
			)

			awght = np.exp(kappa2) + 4
			print(
				np.min(awght),
				np.max(awght),
				np.min(theta),
				np.max(theta),
				np.min(rho),
				np.max(rho),
			)

			final = output.detach().cpu().numpy()
			# remove first dim from final
			final = np.squeeze(final)
			print(final.shape)

			clim_result_path = "results/clim_outputs/"

			with h5py.File(clim_result_path + f"{basename}_clim_output.h5", "w") as f:
				f.create_dataset("clim_output", data=final)

			del model, training_results

	# after you've used up the datset with a set number of replicates, you can delete it
	del data_dict, train_df, val_df, test_df, data_config
	del train_loader, val_loader, test_loader
	torch.cuda.empty_cache()
	import gc

	gc.collect()

``````{ end_of_file="notebooks/i2i_trainloop.py" }

``````{ path="tests/unit/test_integration_pieces.py"  }
import torch.nn as nn
import pytest
import torch
import os
from torch.utils.data import DataLoader
from itertools import product

# directly import from latticevision library in directory
# from latticevision.device import set_device
from latticevision.img2img.dataset import (
	make_dataset,
	no_transform,
	polar_transform,
	DataConfig,
)
from latticevision.plotting import plot_example_field
from latticevision.img2img.base import (
	ModelConfig,
	PosEmbed,
	NullPosEmbed,
	LearnedPosEmbed,
	SinusoidalPosEmbed,
	RotaryPosEmbed,
)
from latticevision.img2img import TransUNet, UNet, ViT


# check for data locally, if not found, download from gdrive
DATA_DIR = "data"
DATA_FILE = "I2I_sample_data.h5"
DATA_PATH = os.path.join(DATA_DIR, DATA_FILE)
GDRIVE_ID = "1Hz1aRc49sBy0d74iwkfxu_djzwZsEW39"
GDRIVE_URL = f"https://drive.google.com/uc?id={GDRIVE_ID}"


if not os.path.isdir(DATA_DIR):
	os.makedirs(DATA_DIR, exist_ok=True)

if not os.path.isfile(DATA_PATH):
	try:
		import gdown
	except ImportError:
		raise ImportError(
			"Please install gdown so the test can auto-download the data."
		)
	print(f"{DATA_PATH} not found. Downloading from Google Drive...")
	gdown.download(GDRIVE_URL, DATA_PATH, quiet=False)


def test_load_data():
	# load and create dataset
	transform_funcs = [no_transform, polar_transform]

	# load and create dataset
	dataset_path = DATA_PATH

	val_size = 0.4
	test_size = 0.5

	data_config = DataConfig(
		file_path=dataset_path,
		n_rows=192,
		n_cols=288,
		n_replicates=30,
		n_params=3,
		transform_function=transform_funcs[0],
		log_kappa2=True,
		shift_theta=True,
		val_size=val_size,
		test_size=test_size,
		random_state=777,
		verbose=True,
	)

	data_dict = make_dataset(
		config=data_config,
	)

	train_df = data_dict["train_df"]
	val_df = data_dict["val_df"]
	test_df = data_dict["test_df"]

	# test the example field plotter function
	plot_example_field(
		dataset=train_df,
		config=data_config,
		idx=0,  # 27 is nice looking
		model_type="STUN",
		field_color="turbo",
		param1_color="viridis",
		param2_color="viridis",
		param3_color="viridis",
		show=False,
	)

	# create dataloaders
	n_batch = 2

	train_loader = DataLoader(train_df, batch_size=n_batch, shuffle=True)
	val_loader = DataLoader(val_df, batch_size=n_batch, shuffle=False)
	test_loader = DataLoader(test_df, batch_size=n_batch, shuffle=False)

	assert len(train_loader) > 0
	assert len(val_loader) > 0
	assert len(test_loader) > 0


@pytest.mark.parametrize(
	"model_cls",
	[TransUNet, UNet, ViT],
)
def test_create_model(model_cls: nn.Module):
	# create config
	if model_cls == UNet:
		config = ModelConfig(
			embed_dim=16,
			enc_block_channels=(2, 4),
			group_norm_groups=(1, 2),
		)
	elif model_cls == TransUNet:
		config = ModelConfig(
			patch_size_h=2,
			patch_size_w=2,
			embed_dim=16,
			enc_block_channels=(2, 4),
			group_norm_groups=(1, 2),
			num_layers=2,
			num_heads=2,
			mlp_dim=32,
		)
	elif model_cls == ViT:
		config = ModelConfig(
			patch_size_h=16,
			patch_size_w=16,
			embed_dim=16,
			num_layers=2,
			num_heads=2,
			mlp_dim=32,
		)
	else:
		raise ValueError("Invalid model class")

	# create model with specific config, send to device and count params
	model = model_cls(config)
	total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
	print(f"Total number of trainable parameters: {total_params}")


@pytest.mark.parametrize(
	("model_cls", "pos_embed_cls", "batch_size"),
	product(
		[TransUNet, ViT],
		[NullPosEmbed, LearnedPosEmbed, SinusoidalPosEmbed, RotaryPosEmbed],
		[1, 2, 3, 4, 5],
	),
)
def test_pos_embeds(model_cls: nn.Module, pos_embed_cls: PosEmbed, batch_size: int):
	# create config
	if model_cls == UNet:
		config = ModelConfig(
			embed_dim=16,
			enc_block_channels=(2, 4),
			group_norm_groups=(1, 2),
			# pos_embed_cls=pos_embed_cls,
		)
	elif model_cls == TransUNet:
		config = ModelConfig(
			patch_size_h=2,
			patch_size_w=2,
			embed_dim=16,
			enc_block_channels=(2, 4),
			group_norm_groups=(1, 2),
			num_layers=2,
			num_heads=2,
			mlp_dim=32,
			pos_embed_cls=pos_embed_cls,
		)
	elif model_cls == ViT:
		config = ModelConfig(
			patch_size_h=16,
			patch_size_w=16,
			embed_dim=16,
			num_layers=2,
			num_heads=2,
			mlp_dim=32,
			pos_embed_cls=pos_embed_cls,
		)
	else:
		raise ValueError("Invalid model class")

	# create model with specific config, send to device and count params
	model: nn.Module = model_cls(config)
	total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
	print(f"Total number of trainable parameters: {total_params}")

	x_rand = torch.randn(batch_size, config.in_channels, 192, 288)
	# a single forward pass
	model.eval()
	out = model(x_rand)
	assert not out.isnan().any()

	model.train()
	with torch.enable_grad():
		out = model(x_rand)
		assert not out.isnan().any()

		loss = out.sum()
		loss.backward()

``````{ end_of_file="tests/unit/test_integration_pieces.py" }

``````{ path="tests/test_cnn_integration.py"  }
# imports required for CNN
import torch
import time
import os
from torch.utils.data import DataLoader

from latticevision.plotting import plot_example_field, plot_losses

from latticevision.cnn.dataset import make_dataset, DataConfig
from latticevision.cnn.model import ModelConfig, CNN
from latticevision.cnn.train import TrainingConfig, train_model
from latticevision.cnn.eval import eval_model

# imports required for local CNN estimation on STUN data
from latticevision.img2img.dataset import make_dataset as make_img2img_dataset
from latticevision.img2img.dataset import DataConfig as Img2ImgDataConfig
from latticevision.cnn.eval import fast_cnn_field_tiler
from latticevision.plotting import plot_img2img_samples
from latticevision.img2img.eval import eval_model as eval_img2img_model

# check for data locally, if not found, download from gdrive
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)


# helper function to download data from gdrive
def download_from_gdrive(filename: str, gdrive_id: str) -> str:
	"""
	Make sure data/filename exists.  If not, download from the given
	Google Drive file-id (via gdown) into data/.
	Returns the full local path.
	"""
	path = os.path.join(DATA_DIR, filename)
	if not os.path.isfile(path):
		try:
			import gdown
		except ImportError:
			raise ImportError(
				"Missing data file and gdown not installed. "
				"Run `pip install gdown` to enable auto-download."
			)
		url = f"https://drive.google.com/uc?id={gdrive_id}"
		print(f"[setup] {filename} not found—downloading from Google Drive…")
		gdown.download(url, path, quiet=False)
	return path


# i2i data and gdrive location
I2I_FILENAME = "I2I_sample_data.h5"
I2I_GDRIVE_ID = "1Hz1aRc49sBy0d74iwkfxu_djzwZsEW39"
I2I_FILE_PATH = download_from_gdrive(I2I_FILENAME, I2I_GDRIVE_ID)

# cnn data and gdrive location
CNN_FILENAME = "CNN_sample_data.h5"
CNN_GDRIVE_ID = "13o65Gt8KYsC7Jjl7s3Pgg51SuDFZtwUq"
CNN_FILE_PATH = download_from_gdrive(CNN_FILENAME, CNN_GDRIVE_ID)


def test_cnn():
	# set the device you are on to cpu (github servers)
	device = torch.device("cpu")

	# load and create dataset
	dataset_path = CNN_FILE_PATH
	val_size = 0.4
	test_size = 0.5

	data_config = DataConfig(
		file_path=dataset_path,
		fullsidelen=25,
		sidelen=25,
		n_replicates=30,
		n_params=3,
		log_kappa2=True,
		val_size=val_size,
		test_size=test_size,
		random_state=777,
		verbose=True,
	)

	data_dict = make_dataset(data_config)

	train_df = data_dict["train_df"]
	val_df = data_dict["val_df"]
	test_df = data_dict["test_df"]

	# example field plotter
	plot_example_field(
		dataset=train_df,
		config=data_config,
		idx=0,
		model_type="CNN",
		field_color="turbo",
		show=False,
	)

	# put into dataloaders
	n_batch = 2

	train_loader = DataLoader(train_df, batch_size=n_batch, shuffle=True)
	val_loader = DataLoader(val_df, batch_size=n_batch, shuffle=False)
	test_loader = DataLoader(test_df, batch_size=n_batch, shuffle=False)

	for fields, params in train_loader:
		print("Train Loader:")
		print(
			"Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape
		)
		break

	for fields, params in val_loader:
		print("Validation Loader:")
		print(
			"Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape
		)
		break

	for fields, params in test_loader:
		print("Test Loader:")
		print(
			"Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape
		)
		break

	# config = ModelConfig(
	#     sidelen=25,
	# 	in_channels=30,
	# 	out_params=3,
	# 	conv_channels=(64, 128, 256),
	# 	linear_sizes=(500,64),
	#     kernel_sizes = (10,7,5),
	#     padding = 0,
	# )

	# load in tiny model
	config = ModelConfig(
		sidelen=25,
		in_channels=30,
		out_params=3,
		conv_channels=(2, 4),
		linear_sizes=(16, 8),
		kernel_sizes=(11, 10),
		padding=0,
	)

	model = CNN(config).to(device)
	total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
	print(f"Total number of trainable parameters: {total_params}")

	# create directory
	save_path = "tests/_temp/model_wghts/"
	os.makedirs(save_path, exist_ok=True)

	# train model
	start_time = time.time()

	train_config = TrainingConfig(
		model=model,
		device=device,
		train_loader=train_loader,
		val_loader=val_loader,
		train_df=train_df,
		val_df=val_df,
		lr=5e-4,
		n_epochs=2,
		stop_patience=10,
		scheduler_patience=5,
		scheduler_factor=0.5,
		augmentation=True,
		save=True,
		save_directory=save_path,
		savename="cnn_wghts.pth",
		verbose=True,
		normalize=True,
		shuffle=True,
	)

	training_results = train_model(config=train_config)

	print("--- %s seconds ---" % (time.time() - start_time))
	time_train = time.time() - start_time
	print("Training took: ", time_train / 60, " minutes.")

	model = training_results["model"]
	train_losses = training_results["train_losses"]
	val_losses = training_results["val_losses"]

	# test the loss plotting function
	plot_losses(
		train_losses=train_losses,
		val_losses=val_losses,
		show=False,
	)

	# load the model
	model_loaded = CNN(config)
	model_loaded.load_state_dict(torch.load(save_path + "cnn_wghts.pth"))
	model_loaded = model_loaded.to(device)
	model_loaded.eval()

	# test the eval function
	metrics = eval_model(
		model=model,
		device=device,
		config=data_config,
		test_loader=test_loader,
		test_df=test_df,
		plot=True,
		augmentation=True,
		show=False,
	)

	# load in the big i2i data for testing
	dataset_path_i2i = I2I_FILE_PATH
	val_size_i2i = 0.4
	test_size_i2i = 0.5

	data_config_i2i = Img2ImgDataConfig(
		file_path=dataset_path_i2i,
		n_rows=192,
		n_cols=288,
		n_replicates=30,
		n_params=3,
		log_kappa2=True,
		shift_theta=True,
		val_size=val_size_i2i,
		test_size=test_size_i2i,
		random_state=777,
		verbose=True,
	)

	data_dict = make_img2img_dataset(
		config=data_config_i2i,
	)

	test_df_i2i = data_dict["test_df"]

	n_batch = 2
	test_loader_i2i = DataLoader(test_df_i2i, batch_size=n_batch, shuffle=False)
	for fields, params in test_loader_i2i:
		print("Test Loader:")
		print(
			"Fields batch shape: ", fields.shape, "\nParams batch shape: ", params.shape
		)
		break

	# perform cnn local estimation (tiling)
	test_fields = test_df_i2i[:][0]
	test_params = test_df_i2i[:][1]

	outputs = torch.zeros(test_params.size())

	# fast option
	for i in range(len(outputs)):
		output = fast_cnn_field_tiler(
			model=model,
			fields=test_fields[i].unsqueeze(0),
			device=device,
			patch_batch_size=10000,
			verbose=False,
			padding_mode="reflect",
			patch_size=25,
		)
		outputs[i] = output

	metrics = eval_img2img_model(
		model=model,
		config=data_config_i2i,
		device=device,
		test_loader=test_loader_i2i,
		test_df=test_df_i2i,
		plot=True,
		augmentation=False,
		n_pixels=5000,
		cnn_mode=True,
		cnn_results=outputs,
		show=False,
	)

	inds = [0]

	plot_img2img_samples(
		model=model,
		config=data_config_i2i,
		device=device,
		test_df=test_df_i2i,
		indices=inds,
		random_selection=False,
		num_rand_samples=5,
		awght_not_kappa2=True,
		cnn_mode=True,
		cnn_results=outputs,
		show=False,
	)

``````{ end_of_file="tests/test_cnn_integration.py" }

``````{ path="tests/test_i2i_integration.py"  }
from itertools import product
import torch
import torch.nn as nn
import pytest
from torch.utils.data import DataLoader

import time
import os

# directly import from latticevision library in directory
# from latticevision.device import set_device
from latticevision.img2img.dataset import (
	make_dataset,
	DataConfig,
	no_transform,
	polar_transform,
)
from latticevision.plotting import plot_example_field, plot_losses, plot_img2img_samples
from latticevision.img2img.base import (
	ModelConfig,
	PosEmbed,
	NullPosEmbed,
	LearnedPosEmbed,
	SinusoidalPosEmbed,
	RotaryPosEmbed,
)
from latticevision.img2img import TransUNet, UNet, ViT
from latticevision.img2img.train import train_model, TrainingConfig
from latticevision.img2img.eval import eval_model

# check for data locally, if not found, download from gdrive
DATA_DIR = "data"
DATA_FILE = "I2I_sample_data.h5"
DATA_PATH = os.path.join(DATA_DIR, DATA_FILE)
GDRIVE_ID = "1Hz1aRc49sBy0d74iwkfxu_djzwZsEW39"
GDRIVE_URL = f"https://drive.google.com/uc?id={GDRIVE_ID}"


if not os.path.isdir(DATA_DIR):
	os.makedirs(DATA_DIR, exist_ok=True)

if not os.path.isfile(DATA_PATH):
	try:
		import gdown
	except ImportError:
		raise ImportError(
			"Please install gdown so the test can auto-download the data."
		)
	print(f"{DATA_PATH} not found. Downloading from Google Drive...")
	gdown.download(GDRIVE_URL, DATA_PATH, quiet=False)


@pytest.mark.parametrize(
	("model_cls", "pos_embed_cls"),
	product(
		[UNet, TransUNet, ViT],
		[NullPosEmbed, LearnedPosEmbed, SinusoidalPosEmbed, RotaryPosEmbed],
	),
)
def test_stun(model_cls: nn.Module, pos_embed_cls: PosEmbed):
	# set the device you are on
	device = torch.device("cpu")
	transform_funcs = [no_transform, polar_transform]

	# load and create dataset
	dataset_path = "data/I2I_sample_data.h5"
	val_size = 0.4
	test_size = 0.5

	data_config = DataConfig(
		file_path=dataset_path,
		n_rows=192,
		n_cols=288,
		n_replicates=30,
		n_params=3,
		transform_function=transform_funcs[0],
		log_kappa2=True,
		shift_theta=True,
		val_size=val_size,
		test_size=test_size,
		random_state=777,
		verbose=True,
	)

	data_dict = make_dataset(
		config=data_config,
	)

	train_df = data_dict["train_df"]
	val_df = data_dict["val_df"]
	test_df = data_dict["test_df"]

	# test the example field plotter function
	plot_example_field(
		dataset=train_df,
		config=data_config,
		idx=0,
		model_type="STUN",
		field_color="turbo",
		param1_color="viridis",
		param2_color="viridis",
		param3_color="viridis",
		show=False,
	)

	# create dataloaders
	n_batch = 2

	train_loader = DataLoader(train_df, batch_size=n_batch, shuffle=True)
	val_loader = DataLoader(val_df, batch_size=n_batch, shuffle=False)
	test_loader = DataLoader(test_df, batch_size=n_batch, shuffle=False)

	for fields, params in train_loader:
		print("Train Loader:")
		print(
			"Fields batch shape: ",
			fields.shape,
			"\nParams batch shape: ",
			params.shape,
		)
		break

	for fields, params in val_loader:
		print("Val Loader:")
		print(
			"Fields batch shape: ",
			fields.shape,
			"\nParams batch shape: ",
			params.shape,
		)
		break

	for fields, params in test_loader:
		print("Test Loader:")
		print(
			"Fields batch shape: ",
			fields.shape,
			"\nParams batch shape: ",
			params.shape,
		)
		break

	# initialize small models
	if model_cls == UNet:
		config = ModelConfig(
			embed_dim=16,
			enc_block_channels=(2, 4),
			group_norm_groups=(1, 2),
			# pos_embed_cls=pos_embed_cls,
		)
	elif model_cls == TransUNet:
		config = ModelConfig(
			patch_size_h=2,
			patch_size_w=2,
			embed_dim=16,
			enc_block_channels=(2, 4),
			group_norm_groups=(1, 2),
			num_layers=2,
			num_heads=2,
			mlp_dim=32,
			pos_embed_cls=pos_embed_cls,
		)
	elif model_cls == ViT:
		config = ModelConfig(
			patch_size_h=16,
			patch_size_w=16,
			embed_dim=16,
			num_layers=2,
			num_heads=2,
			mlp_dim=32,
			pos_embed_cls=pos_embed_cls,
		)
	else:
		raise ValueError("Invalid model class")

	# create model with specific config, send to device and count params
	model = model_cls(config)
	model = model.to(device)
	total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
	print(f"Total number of trainable parameters: {total_params}")

	# train the model
	start_time = time.time()

	# create directory
	save_path = "tests/_temp/model_wghts/"
	os.makedirs(save_path, exist_ok=True)

	train_config = TrainingConfig(
		model=model,
		device=device,
		train_loader=train_loader,
		val_loader=val_loader,
		train_df=train_df,
		val_df=val_df,
		lr=8e-5,
		n_epochs=2,
		stop_patience=10,
		scheduler_patience=5,
		scheduler_factor=0.5,
		augmentation=True,
		save=True,
		save_directory=save_path,
		savename="stun_wghts.pth",
		verbose=True,
		normalize=True,
		shuffle=True,
	)

	training_results = train_model(config=train_config)

	print("--- %s seconds ---" % (time.time() - start_time))
	time_train = time.time() - start_time
	print("Training took: ", time_train / 60, " minutes.")

	# save training results
	model = training_results["model"]
	train_losses = training_results["train_losses"]
	baseline_losses = training_results["baseline_losses"]
	val_losses = training_results["val_losses"]

	# test the loss plotting function
	plot_losses(
		train_losses=train_losses,
		val_losses=val_losses,
		base_losses=baseline_losses,
		show=False,
	)

	# load the model weights
	model_loaded = model_cls(config)
	model_loaded.load_state_dict(torch.load(save_path + "stun_wghts.pth"))
	model_loaded = model_loaded.to(device)
	model_loaded.eval()

	# evaluate model performance
	metrics = eval_model(
		model=model_loaded,
		config=data_config,
		device=device,
		test_loader=test_loader,
		test_df=test_df,
		plot=True,
		augmentation=True,
		n_pixels=5000,
		show=False,
	)

	print(metrics)

	# test the sample output plotting function
	inds = [0]

	plot_img2img_samples(
		model=model,
		config=data_config,
		device=device,
		test_df=test_df,
		indices=inds,
		random_selection=False,
		awght_not_kappa2=False,
		num_rand_samples=1,
		show=False,
	)

``````{ end_of_file="tests/test_i2i_integration.py" }

``````{ path="README.md"  }
# LatticeVision 

This repository contains everything you need to train image-to-image (I2I) neural networks—using both U-Net and transformer-based architectures—for non-stationary parameter estimation on large spatial datasets. You’ll find tutorial notebooks, data-generation scripts, training and evaluation procedures, and example applications to climate-model outputs. All code accompanies our paper:

*LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data*

**Authors**: Antony Sikorski, Michael Ivanitskiy, Nathan Lenssen, Douglas Nychka, Daniel McKenzie

The paper is currently available on [arXiv](https://arxiv.org/abs/2505.09803).

<p align="center">
  <img src="results/figures/flowchart_v5.png" alt="The main workflow of LatticeVision." width="700"/>
<p align="center"><em>Figure: An illustration of the main workflow of LatticeVision. </em></p>

---

## Installation

Prior to running this code, one will need to download `Python`, `R` and `RStudio`, clone this repository, and install all necessary dependencies. 

- **R:** The `R` programming language may be downloaded [here](https://cran.r-project.org/bin/windows/base/). We strongly recommend [`RStudio`](https://posit.co/download/rstudio-desktop/) for opening and working with the `R` scripts (training data and synthetic field generation). 

- **Python:** The `Python` programming language may be downloaded [here](https://www.python.org/downloads/). We use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) as our package manager. 

- **Cloning this repo:** This repository can be cloned by running the following your terminal:
   ```
   git clone https://github.com/antonyxsik/LatticeVision.git
   ``` 

- **Dependencies:**
  - For `Python`: You can install dependencies and create a virtual environment using [`uv`](https://docs.astral.sh/uv/) by running 

    ```
    uv sync
    ```
   
    If you do not wish to use `uv`, you can create a virtual environment however you wish, and run 

    ```
    pip install -r requirements.txt
    ```
   
  Both install the `latticevision` package, which contains our primary source code.
  - For `R`: All dependencies can be downloaded by opening and running `R_scripts/required_packages.R`.

## Quick Start 

To get started with our code, download our sample data, network weights (for both a STUN and CNN25 network trained on 30 replicates), and their climate application results from [this Google Drive folder](https://drive.google.com/drive/folders/1OcgHHqqNmK48qdvHCP_PQpXXKq_EYCWD?usp=sharing):

1. Prepare `data/` and `results/` folders: 

   - Create `data/` in the root of this project (ignored by Git), then download everything into it from the `data/` folder in Google Drive (sample synthetic data and climate fields). 
   - Create `results/` in the root of this project (also ignored), with subfolders `clim_outputs/` and `model_wghts/`. Download the corresponding Google Drive components for both folders. 

2. Explore our tutorial/demo style notebooks in `notebooks/` (`i2i_demo.ipynb`, `cnn_demo.ipynb`, `cesm_application.ipynb`) to get a sense for our core workflows and codebase. These should all point to/utilize the network weights and sample data that you downloaded. 

3. Optionally, run 

   ```
   make test
   ```

   to run all tests, and run 

   ```
   make help
   ``` 

   to print out other available commands. 

## Reproducing Results

### Data Generation

In order to reproduce our results, one must first generate the data:
- To generate synthetic training/testing data for the I2I networks, run `R_scripts/i2i_datagen.R`. Instructions:
  1. Open the file in `RStudio`.
  2. Set the working directory to be that of the source file location (see the "Important" note in the file).
  3. Choose the total size of the dataset and the chunk size (to avoid overloading RAM) that it will be created in (below the "Important" note and the imports).
  4. Run the script (``Ctrl+A``, then ``Ctrl+Enter``). This will create `data/I2I_data.h5`. 
- To make the data for the CNNs, one repeats the same process as above with `R_scripts/cnn_datagen.R`. This will create `data/CNN_data.h5`.

### Train Models and Test on Synthetic Data

- You can train **_all_** of the I2I networks by running `notebooks/i2i_trainloop.py`. 
- **_All_** CNNs can be trained by running `notebooks/cnn_trainloop.py`.
- Both of these scripts train the networks, save their weights in `results/model_wghts/`, evaluate them on synthetic test data (results saved in `results/metrics/`), and then pass the climate model fields through the networks (saved in `results/clim_outputs/`). 

**_Note_**: 

These scripts will train 4 U-Nets, 16 ViTs, 16 STUNs, and 12 CNNs due to different combinations of hyperparameters (n_replicates, positional embeddings, etc.). This may be quite computationally intensive. We highly recommend modifying the lists in the scripts (that determine which hyperparams to loop through) to reflect the networks that you are interested in training and evaluating.  


### Climate Application

- The training scripts above automatically generate the parameter estimates for the climate model fields in `results/clim_outputs/`. 
- Alternatively, one can also load a trained network into `notebooks/cesm_application.ipynb` and uncomment the saving code to create these. 
- After the parameters have been estimated, open `R_scripts/cesm_ensemble_sim.R` and point towards the outputs of interest in order to generate synthetic climate ensembles and reproduce our correlation comparison experiments. 

---

## Citation

Please use the following BibTeX to cite this work: 

```{bibtex}
@article{sikorski2025latticevision,
  title={LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data},
  author={Sikorski, Antony and Ivanitskiy, Michael and Lenssen, Nathan and Nychka, Douglas and McKenzie, Daniel},
  journal={arXiv preprint arXiv:2505.09803},
  year={2025}
}
```


``````{ end_of_file="README.md" }

``````{ path="makefile" processed_with="makefile_recipes" }
# first/default target is help
.PHONY: default
default: help
	...

# this recipe is weird. we need it because:
# - a one liner for getting the version with toml is unwieldy, and using regex is fragile
# - using $$SCRIPT_GET_VERSION within $(shell ...) doesn't work because of escaping issues
# - trying to write to the file inside the `gen-version-info` recipe doesn't work, 
# 	shell eval happens before our `python -c ...` gets run and `cat` doesn't see the new file
.PHONY: write-proj-version
write-proj-version:
	...

# gets version info from $(PYPROJECT), last version from $(LAST_VERSION_FILE), and python version
# uses just `python` for everything except getting the python version. no echo here, because this is "private"
.PHONY: gen-version-info
gen-version-info: write-proj-version
	...

# getting commit log since the tag specified in $(LAST_VERSION_FILE)
# will write to $(COMMIT_LOG_FILE)
# when publishing, the contents of $(COMMIT_LOG_FILE) will be used as the tag description (but can be edited during the process)
# no echo here, because this is "private"
.PHONY: gen-commit-log
gen-commit-log: gen-version-info
	...

# force the version info to be read, printing it out
# also force the commit log to be generated, and cat it out
.PHONY: version
version: gen-commit-log
	@echo "Current version is $(PROJ_VERSION), last auto-uploaded version is $(LAST_VERSION)"
	...

.PHONY: setup
setup: dep-check
	@echo "install and update via uv"
	...

.PHONY: dep-check-torch
dep-check-torch:
	@echo "see if torch is installed, and which CUDA version and devices it sees"
	...

.PHONY: dep
dep:
	@echo "Exporting dependencies as per $(PYPROJECT) section 'tool.uv-exports.exports'"
	...

.PHONY: dep-check
dep-check:
	@echo "Checking that exported requirements are up to date"
	...

.PHONY: dep-clean
dep-clean:
	@echo "clean up lock files, .venv, and requirements files"
	...

# runs ruff and pycln to format the code
.PHONY: format
format:
	@echo "format the source code"
	...

# runs ruff and pycln to check if the code is formatted correctly
.PHONY: format-check
format-check:
	@echo "check if the source code is formatted correctly"
	...

# runs type checks with mypy
.PHONY: typing
typing: clean
	@echo "running type checks"
	...

# generates a report of the mypy output
.PHONY: typing-report
typing-report:
	@echo "generate a report of the type check output -- errors per file"
	...

.PHONY: test
test: clean
	@echo "running tests"
	...

.PHONY: check
check: clean format-check test typing
	@echo "run format checks, tests, and typing checks"
	...

# generates a whole tree of documentation in html format.
# see `$(MAKE_DOCS_SCRIPT_PATH)` and the templates in `$(DOCS_RESOURCES_DIR)/templates/html/` for more info
.PHONY: docs-html
docs-html:
	@echo "generate html docs"
	...

# instead of a whole website, generates a single markdown file with all docs using the templates in `$(DOCS_RESOURCES_DIR)/templates/markdown/`.
# this is useful if you want to have a copy that you can grep/search, but those docs are much messier.
# docs-combined will use pandoc to convert them to other formats.
.PHONY: docs-md
docs-md:
	@echo "generate combined (single-file) docs in markdown"
	...

# after running docs-md, this will convert the combined markdown file to other formats:
# gfm (github-flavored markdown), plain text, and html
# requires pandoc in path, pointed to by $(PANDOC)
# pdf output would be nice but requires other deps
.PHONY: docs-combined
docs-combined: docs-md
	@echo "generate combined (single-file) docs in markdown and convert to other formats"
	...

# generates coverage reports as html and text with `pytest-cov`, and a badge with `coverage-badge`
# if `.coverage` is not found, will run tests first
# also removes the `.gitignore` file that `coverage html` creates, since we count that as part of the docs
.PHONY: cov
cov:
	@echo "generate coverage reports"
	...

# runs the coverage report, then the docs, then the combined docs
.PHONY: docs
docs: cov docs-html docs-combined todo lmcat
	@echo "generate all documentation and coverage reports"
	...

# removed all generated documentation files, but leaves everything in `$DOCS_RESOURCES_DIR`
# and leaves things defined in `pyproject.toml:tool.makefile.docs.no_clean`
# (templates, svg, css, make_docs.py script)
# distinct from `make clean`
.PHONY: docs-clean
docs-clean:
	@echo "remove generated docs except resources"
	...

.PHONY: todo
todo:
	@echo "get all TODO's from the code"
	...

.PHONY: lmcat-tree
lmcat-tree:
	@echo "show in console the lmcat tree view"
	...

.PHONY: lmcat
lmcat:
	@echo "write the lmcat full output to pyproject.toml:[tool.lmcat.output]"
	...

# verifies that the current branch is $(PUBLISH_BRANCH) and that git is clean
# used before publishing
.PHONY: verify-git
verify-git: 
	@echo "checking git status"
	...

.PHONY: build
build: 
	@echo "build the package"
	...

# gets the commit log, checks everything, builds, and then publishes with twine
# will ask the user to confirm the new version number (and this allows for editing the tag info)
# will also print the contents of $(PYPI_TOKEN_FILE) to the console for the user to copy and paste in when prompted by twine
.PHONY: publish
publish: gen-commit-log check build verify-git version gen-version-info
	@echo "run all checks, build, and then publish"
	...

# cleans up temp files from formatter, type checking, tests, coverage
# removes all built files
# removes $(TESTS_TEMP_DIR) to remove temporary test files
# recursively removes all `__pycache__` directories and `*.pyc` or `*.pyo` files
# distinct from `make docs-clean`, which only removes generated documentation files
.PHONY: clean
clean:
	@echo "clean up temporary files"
	...

.PHONY: clean-all
clean-all: clean docs-clean dep-clean
	@echo "clean up all temporary files, dep files, venv, and generated docs"
	...

.PHONY: info
info: gen-version-info
	@echo "# makefile variables"
	...

.PHONY: info-long
info-long: info
	@echo "# other variables"
	...

# immediately print out the help targets, and then local variables (but those take a bit longer)
.PHONY: help
help: help-targets info
	@echo -n ""
	...

``````{ end_of_file="makefile" }

``````{ path="pyproject.toml"  }
[project]
name = "latticevision"
version = "0.1.0"
description = "Image to image networks for modeling big non-stationary spatial data"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    # basics
    "numpy>=2.1.1",
    "matplotlib>=3.9.2",
    "pandas>=2.2.3",
    # notebook
    "ipykernel>=6.29.5",
    "jupyter>=1.1.1",
    # scikit
    "scikit-image>=0.24.0",
    "scikit-learn>=1.5.2",
    # torch
    "torch>=2.5.1",
    "torchsummary>=1.5.1",
    "torchvision>=0.19.1",
    "torchtune>=0.5.0", # for RoPE and compatible MHA
    "torchao", # required by torchtune?
    # other
    "h5py>=3.12.1",
    "pysr>=0.19.4",
    "netcdf4>=1.5.8",
    "gdown>=5.2.0",
]

# default groups to install
[tool.uv]
default-groups = ["dev", "lint"]

[dependency-groups]
# dev group -- testing, type checking, etc
dev = [
	# test
	"pytest>=8.2.2",
	# coverage
	"pytest-cov>=4.1.0",
	"coverage-badge>=1.1.0",
	# type checking
	"mypy>=1.0.1",
	# docs
	'pdoc>=14.6.0',
	# tomli since no tomlib in python < 3.11
	"tomli>=2.1.0; python_version < '3.11'",
    # lmcat -- a custom library. not exactly docs, but lets an LLM see all the code
    "lmcat>=0.2.0; python_version >= '3.11'",
]
# lint group -- just formatting stuff   
lint = [
	# lint
	"pycln>=2.1.3",
	"ruff>=0.4.8",
]

# project links -- used in doc gen
[project.urls]
Homepage = "https://github.com/antonyxsik/LatticeVision"
Documentation = "https://github.com/antonyxsik/LatticeVision"
Repository = "https://github.com/antonyxsik/LatticeVision"
Issues = "https://github.com/antonyxsik/LatticeVision/issues"

# how to build the package
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# ruff config
[tool.ruff]
exclude = ["__pycache__", "notebooks/old", "latticevision/stun/old"]

[tool.ruff.format]
indent-style = "tab"
skip-magic-trailing-comma = false


[tool.ruff.lint]
# Allow equality comparisons to False 
ignore = ["E712"]

# Custom export configurations
[tool.uv-exports]
args = [
	"--no-hashes"
]
exports = [
	# no groups, no extras, just the base dependencies
    { name = "base", groups = false, extras = false },
	# all groups and extras
    { name = "all", filename="requirements-all.txt", groups = true, extras=true },
]

# `make lmcat` depends on the lmcat and can be configured here
[tool.lmcat]
	output = ".meta/lmcat.txt" # changing this might mean it wont be accessible from the docs
	ignore_patterns = [
		".venv/**",
		".git/**",
		".meta/**",
		"uv.lock",
		"LICENSE",
        ".RData",
        "sample_data/**",
        "results/**",
        ".pytest_cache/**",
        ".ruff_cache/**",
	]
    [tool.lmcat.glob_process]
        "[mM]akefile" = "makefile_recipes"
        "*.ipynb" = "ipynb_to_md"

``````{ end_of_file="pyproject.toml" }

``````{ path="requirements.txt"  }
# This file was autogenerated by uv via the following command:
#    uv export --no-hashes
-e .
aiohappyeyeballs==2.6.1
    # via aiohttp
aiohttp==3.11.13
    # via
    #   datasets
    #   fsspec
aiosignal==1.3.2
    # via aiohttp
antlr4-python3-runtime==4.9.3
    # via omegaconf
anyio==4.8.0
    # via
    #   httpx
    #   jupyter-server
appnope==0.1.4 ; sys_platform == 'darwin'
    # via ipykernel
argon2-cffi==23.1.0
    # via jupyter-server
argon2-cffi-bindings==21.2.0
    # via argon2-cffi
arrow==1.3.0
    # via isoduration
asttokens==3.0.0
    # via stack-data
async-lru==2.0.4
    # via jupyterlab
async-timeout==5.0.1 ; python_full_version < '3.11'
    # via aiohttp
attrs==25.1.0
    # via
    #   aiohttp
    #   igittigitt
    #   jsonschema
    #   referencing
babel==2.17.0
    # via jupyterlab-server
beautifulsoup4==4.13.3
    # via
    #   gdown
    #   nbconvert
bleach==6.2.0
    # via nbconvert
blobfile==3.0.0
    # via torchtune
bracex==2.5.post1 ; python_full_version >= '3.11'
    # via wcmatch
certifi==2025.1.31
    # via
    #   httpcore
    #   httpx
    #   netcdf4
    #   requests
cffi==1.17.1
    # via
    #   argon2-cffi-bindings
    #   pyzmq
cftime==1.6.4.post1
    # via netcdf4
charset-normalizer==3.4.1
    # via requests
cli-exit-tools==1.2.7 ; python_full_version >= '3.11'
    # via igittigitt
click==8.1.8
    # via
    #   cli-exit-tools
    #   igittigitt
    #   pysr
    #   typer
colorama==0.4.6 ; sys_platform == 'win32'
    # via
    #   click
    #   ipython
    #   pytest
    #   tqdm
comm==0.2.2
    # via
    #   ipykernel
    #   ipywidgets
contourpy==1.3.1
    # via matplotlib
coverage==7.6.12
    # via
    #   coverage-badge
    #   pytest-cov
coverage-badge==1.1.2
cycler==0.12.1
    # via matplotlib
datasets==3.3.2
    # via torchtune
debugpy==1.8.12
    # via ipykernel
decorator==5.2.1
    # via ipython
defusedxml==0.7.1
    # via nbconvert
dill==0.3.8
    # via
    #   datasets
    #   multiprocess
exceptiongroup==1.2.2 ; python_full_version < '3.11'
    # via
    #   anyio
    #   ipython
    #   pytest
executing==2.2.0
    # via stack-data
fastjsonschema==2.21.1
    # via nbformat
filelock==3.17.0
    # via
    #   blobfile
    #   datasets
    #   gdown
    #   huggingface-hub
    #   juliapkg
    #   torch
fonttools==4.56.0
    # via matplotlib
fqdn==1.5.1
    # via jsonschema
frozenlist==1.5.0
    # via
    #   aiohttp
    #   aiosignal
fsspec==2024.12.0
    # via
    #   datasets
    #   huggingface-hub
    #   torch
gdown==5.2.0
    # via latticevision
h11==0.14.0
    # via httpcore
h5py==3.13.0
    # via latticevision
hf-transfer==0.1.9
    # via huggingface-hub
httpcore==1.0.7
    # via httpx
httpx==0.28.1
    # via jupyterlab
huggingface-hub==0.29.3
    # via
    #   datasets
    #   torchtune
idna==3.10
    # via
    #   anyio
    #   httpx
    #   jsonschema
    #   requests
    #   yarl
igittigitt==2.1.5 ; python_full_version >= '3.11'
    # via lmcat
imageio==2.37.0
    # via scikit-image
iniconfig==2.0.0
    # via pytest
ipykernel==6.29.5
    # via
    #   jupyter
    #   jupyter-console
    #   jupyterlab
    #   latticevision
ipython==8.33.0 ; python_full_version < '3.11'
    # via
    #   ipykernel
    #   ipywidgets
    #   jupyter-console
ipython==9.0.0 ; python_full_version >= '3.11'
    # via
    #   ipykernel
    #   ipywidgets
    #   jupyter-console
ipython-pygments-lexers==1.1.1 ; python_full_version >= '3.11'
    # via ipython
ipywidgets==8.1.5
    # via jupyter
isoduration==20.11.0
    # via jsonschema
jedi==0.19.2
    # via ipython
jinja2==3.1.5
    # via
    #   jupyter-server
    #   jupyterlab
    #   jupyterlab-server
    #   nbconvert
    #   pdoc
    #   torch
joblib==1.4.2
    # via scikit-learn
json5==0.10.0
    # via jupyterlab-server
jsonpointer==3.0.0
    # via jsonschema
jsonschema==4.23.0
    # via
    #   jupyter-events
    #   jupyterlab-server
    #   nbformat
jsonschema-specifications==2024.10.1
    # via jsonschema
juliacall==0.9.24
    # via pysr
juliapkg==0.1.16
    # via juliacall
jupyter==1.1.1
    # via latticevision
jupyter-client==8.6.3
    # via
    #   ipykernel
    #   jupyter-console
    #   jupyter-server
    #   nbclient
jupyter-console==6.6.3
    # via jupyter
jupyter-core==5.7.2
    # via
    #   ipykernel
    #   jupyter-client
    #   jupyter-console
    #   jupyter-server
    #   jupyterlab
    #   nbclient
    #   nbconvert
    #   nbformat
jupyter-events==0.12.0
    # via jupyter-server
jupyter-lsp==2.2.5
    # via jupyterlab
jupyter-server==2.15.0
    # via
    #   jupyter-lsp
    #   jupyterlab
    #   jupyterlab-server
    #   notebook
    #   notebook-shim
jupyter-server-terminals==0.5.3
    # via jupyter-server
jupyterlab==4.3.5
    # via
    #   jupyter
    #   notebook
jupyterlab-pygments==0.3.0
    # via nbconvert
jupyterlab-server==2.27.3
    # via
    #   jupyterlab
    #   notebook
jupyterlab-widgets==3.0.13
    # via ipywidgets
kagglehub==0.3.10
    # via torchtune
kiwisolver==1.4.8
    # via matplotlib
lazy-loader==0.4
    # via scikit-image
lib-detect-testenv==2.0.8 ; python_full_version >= '3.11'
    # via
    #   cli-exit-tools
    #   igittigitt
libcst==1.6.0
    # via pycln
lmcat==0.2.0 ; python_full_version >= '3.11'
lxml==5.3.1
    # via blobfile
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via
    #   jinja2
    #   nbconvert
    #   pdoc
matplotlib==3.10.1
    # via latticevision
matplotlib-inline==0.1.7
    # via
    #   ipykernel
    #   ipython
mdurl==0.1.2
    # via markdown-it-py
mistune==3.1.2
    # via nbconvert
mpmath==1.3.0
    # via sympy
multidict==6.1.0
    # via
    #   aiohttp
    #   yarl
multiprocess==0.70.16
    # via datasets
muutils==0.8.3 ; python_full_version >= '3.11'
    # via lmcat
mypy==1.15.0
mypy-extensions==1.0.0
    # via mypy
nbclient==0.10.2
    # via nbconvert
nbconvert==7.16.6
    # via
    #   jupyter
    #   jupyter-server
nbformat==5.10.4
    # via
    #   jupyter-server
    #   nbclient
    #   nbconvert
nest-asyncio==1.6.0
    # via ipykernel
netcdf4==1.7.2
    # via latticevision
networkx==3.4.2
    # via
    #   scikit-image
    #   torch
notebook==7.3.2
    # via jupyter
notebook-shim==0.2.4
    # via
    #   jupyterlab
    #   notebook
numpy==2.2.3
    # via
    #   cftime
    #   contourpy
    #   datasets
    #   h5py
    #   imageio
    #   latticevision
    #   matplotlib
    #   netcdf4
    #   pandas
    #   pysr
    #   scikit-image
    #   scikit-learn
    #   scipy
    #   tifffile
    #   torchtune
    #   torchvision
nvidia-cublas-cu12==12.4.5.8 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via
    #   nvidia-cudnn-cu12
    #   nvidia-cusolver-cu12
    #   torch
nvidia-cuda-cupti-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cuda-nvrtc-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cuda-runtime-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cudnn-cu12==9.1.0.70 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cufft-cu12==11.2.1.3 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-curand-cu12==10.3.5.147 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cusolver-cu12==11.6.1.9 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cusparse-cu12==12.3.1.170 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via
    #   nvidia-cusolver-cu12
    #   torch
nvidia-cusparselt-cu12==0.6.2 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-nccl-cu12==2.21.5 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-nvjitlink-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via
    #   nvidia-cufft-cu12
    #   nvidia-cusolver-cu12
    #   nvidia-cusparse-cu12
    #   torch
nvidia-nvtx-cu12==12.4.127 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
omegaconf==2.3.0
    # via torchtune
overrides==7.7.0
    # via jupyter-server
packaging==24.2
    # via
    #   datasets
    #   huggingface-hub
    #   ipykernel
    #   jupyter-events
    #   jupyter-server
    #   jupyterlab
    #   jupyterlab-server
    #   kagglehub
    #   lazy-loader
    #   matplotlib
    #   nbconvert
    #   pytest
    #   scikit-image
pandas==2.2.3
    # via
    #   datasets
    #   latticevision
    #   pysr
pandocfilters==1.5.1
    # via nbconvert
parso==0.8.4
    # via jedi
pathspec==0.12.1
    # via pycln
pdoc==15.0.1
pexpect==4.9.0 ; sys_platform != 'emscripten' and sys_platform != 'win32'
    # via ipython
pillow==11.1.0
    # via
    #   imageio
    #   matplotlib
    #   scikit-image
    #   torchtune
    #   torchvision
platformdirs==4.3.6
    # via jupyter-core
pluggy==1.5.0
    # via pytest
prometheus-client==0.21.1
    # via jupyter-server
prompt-toolkit==3.0.50
    # via
    #   ipython
    #   jupyter-console
propcache==0.3.0
    # via
    #   aiohttp
    #   yarl
psutil==7.0.0
    # via
    #   ipykernel
    #   torchtune
ptyprocess==0.7.0 ; os_name != 'nt' or (sys_platform != 'emscripten' and sys_platform != 'win32')
    # via
    #   pexpect
    #   terminado
pure-eval==0.2.3
    # via stack-data
pyarrow==19.0.1
    # via datasets
pycln==2.5.0
pycparser==2.22
    # via cffi
pycryptodomex==3.21.0
    # via blobfile
pygments==2.19.1
    # via
    #   ipython
    #   ipython-pygments-lexers
    #   jupyter-console
    #   nbconvert
    #   pdoc
    #   rich
pyparsing==3.2.1
    # via matplotlib
pysocks==1.7.1
    # via requests
pysr==1.5.1
    # via latticevision
pytest==8.3.5
    # via pytest-cov
pytest-cov==6.0.0
python-dateutil==2.9.0.post0
    # via
    #   arrow
    #   jupyter-client
    #   matplotlib
    #   pandas
python-json-logger==3.2.1
    # via jupyter-events
pytz==2025.1
    # via pandas
pywin32==308 ; platform_python_implementation != 'PyPy' and sys_platform == 'win32'
    # via jupyter-core
pywinpty==2.0.15 ; os_name == 'nt'
    # via
    #   jupyter-server
    #   jupyter-server-terminals
    #   terminado
pyyaml==6.0.2
    # via
    #   datasets
    #   huggingface-hub
    #   jupyter-events
    #   kagglehub
    #   libcst
    #   omegaconf
    #   pycln
pyzmq==26.2.1
    # via
    #   ipykernel
    #   jupyter-client
    #   jupyter-console
    #   jupyter-server
referencing==0.36.2
    # via
    #   jsonschema
    #   jsonschema-specifications
    #   jupyter-events
regex==2024.11.6
    # via tiktoken
requests==2.32.3
    # via
    #   datasets
    #   gdown
    #   huggingface-hub
    #   jupyterlab-server
    #   kagglehub
    #   tiktoken
rfc3339-validator==0.1.4
    # via
    #   jsonschema
    #   jupyter-events
rfc3986-validator==0.1.1
    # via
    #   jsonschema
    #   jupyter-events
rich==13.9.4
    # via typer
rpds-py==0.23.1
    # via
    #   jsonschema
    #   referencing
ruff==0.9.9
safetensors==0.5.3
    # via torchtune
scikit-image==0.25.2
    # via latticevision
scikit-learn==1.6.1
    # via
    #   latticevision
    #   pysr
scipy==1.15.2
    # via
    #   scikit-image
    #   scikit-learn
semver==3.0.4
    # via juliapkg
send2trash==1.8.3
    # via jupyter-server
sentencepiece==0.2.0
    # via torchtune
setuptools==75.8.2
    # via
    #   coverage-badge
    #   jupyterlab
    #   pysr
    #   torch
shellingham==1.5.4
    # via typer
six==1.17.0
    # via
    #   python-dateutil
    #   rfc3339-validator
sniffio==1.3.1
    # via anyio
soupsieve==2.6
    # via beautifulsoup4
stack-data==0.6.3
    # via ipython
sympy==1.13.1
    # via
    #   pysr
    #   torch
terminado==0.18.1
    # via
    #   jupyter-server
    #   jupyter-server-terminals
threadpoolctl==3.5.0
    # via scikit-learn
tifffile==2025.2.18
    # via scikit-image
tiktoken==0.9.0
    # via torchtune
tinycss2==1.4.0
    # via bleach
tomli==2.2.1 ; python_full_version <= '3.11'
    # via
    #   coverage
    #   jupyterlab
    #   mypy
    #   pytest
tomlkit==0.13.2
    # via pycln
torch==2.6.0
    # via
    #   latticevision
    #   torchvision
torchao==0.9.0
    # via latticevision
torchsummary==1.5.1
    # via latticevision
torchtune==0.5.0
    # via latticevision
torchvision==0.21.0
    # via latticevision
tornado==6.4.2
    # via
    #   ipykernel
    #   jupyter-client
    #   jupyter-server
    #   jupyterlab
    #   notebook
    #   terminado
tqdm==4.67.1
    # via
    #   datasets
    #   gdown
    #   huggingface-hub
    #   kagglehub
    #   torchtune
traitlets==5.14.3
    # via
    #   comm
    #   ipykernel
    #   ipython
    #   ipywidgets
    #   jupyter-client
    #   jupyter-console
    #   jupyter-core
    #   jupyter-events
    #   jupyter-server
    #   jupyterlab
    #   matplotlib-inline
    #   nbclient
    #   nbconvert
    #   nbformat
triton==3.2.0 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
typer==0.15.2
    # via pycln
types-python-dateutil==2.9.0.20241206
    # via arrow
typing-extensions==4.12.2
    # via
    #   anyio
    #   async-lru
    #   beautifulsoup4
    #   huggingface-hub
    #   ipython
    #   mistune
    #   multidict
    #   mypy
    #   referencing
    #   rich
    #   torch
    #   typer
tzdata==2025.1
    # via pandas
uri-template==1.3.0
    # via jsonschema
urllib3==2.3.0
    # via
    #   blobfile
    #   requests
wcmatch==10.0 ; python_full_version >= '3.11'
    # via igittigitt
wcwidth==0.2.13
    # via prompt-toolkit
webcolors==24.11.1
    # via jsonschema
webencodings==0.5.1
    # via
    #   bleach
    #   tinycss2
websocket-client==1.8.0
    # via jupyter-server
widgetsnbextension==4.0.13
    # via ipywidgets
xxhash==3.5.0
    # via datasets
yarl==1.18.3
    # via aiohttp

``````{ end_of_file="requirements.txt" }